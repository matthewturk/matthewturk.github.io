<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academic 4.8.0"><meta name=author content="Matthew Turk"><meta name=description content="SIDE NOTE: I intended for this blog post to be a bit shorter than it turned out, and for it to cover some things it &mldr; didn&rsquo;t! So it looks like there&rsquo;ll be a part three in the series."><link rel=alternate hreflang=en-us href=https://matthewturk.github.io/post/refactoring-yt-frontends-part2/><meta name=theme-color content="#EF525B"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin=anonymous><script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap"><link rel=stylesheet href=/css/academic.css><script async src="https://www.googletagmanager.com/gtag/js?id=UA-141183895-1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments);}
function trackOutboundLink(url,target){gtag('event','click',{'event_category':'outbound','event_label':url,'transport_type':'beacon','event_callback':function(){if(target!=='_blank'){document.location=url;}}});console.debug("Outbound link clicked: "+url);}
function onClickCallback(event){if((event.target.tagName!=='A')||(event.target.host===window.location.host)){return;}
trackOutboundLink(event.target,event.target.getAttribute('target'));}
gtag('js',new Date());gtag('config','UA-141183895-1',{'anonymize_ip':true});document.addEventListener('click',onClickCallback,false);</script><link rel=manifest href=/index.webmanifest><link rel=icon type=image/png href=/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_32x32_fill_lanczos_center_2.png><link rel=apple-touch-icon type=image/png href=/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png><link rel=canonical href=https://matthewturk.github.io/post/refactoring-yt-frontends-part2/><meta property="twitter:card" content="summary"><meta property="twitter:site" content="@powersoffour"><meta property="twitter:creator" content="@powersoffour"><meta property="og:site_name" content="mjt"><meta property="og:url" content="https://matthewturk.github.io/post/refactoring-yt-frontends-part2/"><meta property="og:title" content="Refactoring yt Frontends - Part 2 | mjt"><meta property="og:description" content="SIDE NOTE: I intended for this blog post to be a bit shorter than it turned out, and for it to cover some things it &mldr; didn&rsquo;t! So it looks like there&rsquo;ll be a part three in the series."><meta property="og:image" content="https://matthewturk.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://matthewturk.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2019-06-10T12:59:33-05:00"><meta property="article:modified_time" content="2019-06-10T13:04:50-05:00"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://matthewturk.github.io/post/refactoring-yt-frontends-part2/"},"headline":"Refactoring yt Frontends - Part 2","datePublished":"2019-06-10T12:59:33-05:00","dateModified":"2019-06-10T13:04:50-05:00","author":{"@type":"Person","name":"Matthew Turk"},"publisher":{"@type":"Organization","name":"mjt","logo":{"@type":"ImageObject","url":"https://matthewturk.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_192x192_fill_lanczos_center_2.png"}},"description":"SIDE NOTE: I intended for this blog post to be a bit shorter than it turned out, and for it to cover some things it \u0026hellip; didn\u0026rsquo;t! So it looks like there\u0026rsquo;ll be a part three in the series."}</script><script src=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.js integrity="sha256-5VhCqFam2Cn+yjw61zbBNrbHVJ6SRydPeKopYlngbiQ=" crossorigin=anonymous></script><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/cookieconsent2/3.1.1/cookieconsent.min.css integrity="sha256-zQ0LblD/Af8vOppw18+2anxsuaz3pWYyVWi+bTvTH8Q=" crossorigin=anonymous><script>window.addEventListener("load",function(){window.cookieconsent.initialise({"palette":{"popup":{"background":"#EF525B","text":"#EAE7D6"},"button":{"background":"#EAE7D6","text":"#EF525B"}},"theme":"classic","content":{"message":"This website uses cookies to ensure you get the best experience on our website.","dismiss":"Got it!","link":"Learn more","href":"https://www.cookiesandyou.com"}})});</script><title>Refactoring yt Frontends - Part 2 | mjt</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container><div class="d-none d-lg-inline-flex"><a class=navbar-brand href=/>mjt</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href=/>mjt</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#courses><span>Courses</span></a></li><li class=nav-item><a class=nav-link href=/#talks><span>Talks</span></a></li><li class=nav-item><a class=nav-link href=/#publications><span>Publications</span></a></li><li class=nav-item><a class=nav-link href=/#contact><span>Contact</span></a></li><li class=nav-item><a class=nav-link href=https://data-exp-lab.github.io/ target=_blank rel=noopener><span>Research Group</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href=# aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li></ul></div></nav><article class=article><div class="article-container pt-3"><h1>Refactoring yt Frontends - Part 2</h1><div class=article-metadata><span class=article-date>Last updated on
Jun 10, 2019</span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=/project/yt/>Project</a></div></div><div class=article-container><div class=article-style><p><strong>SIDE NOTE</strong>: I intended for this blog post to be a bit shorter than it turned out, and for it to cover some things it &mldr; didn&rsquo;t! So it looks like there&rsquo;ll be a part three in the series.</p><h2 id=operations-on-data-objects>Operations on Data Objects</h2><p>In my previous post, I walked through a few aspects of how the chunking system in yt works, mostly focusing on the <code>"io"</code> style of chunking, where the order in which data arrives is not important. This style of chunking lends itself very easily to parallelism, as well as dynamic chunk-sizing; we can see this in how operations such as <code>.max()</code> operate on a data object in yt.</p><pre><code class=language-python>import yt
ds = yt.load(&quot;data/IsolatedGalaxy/galaxy0030/galaxy0030&quot;)
dd = ds.r[:,:,:]
</code></pre><pre><code>yt : [INFO     ] 2019-06-10 12:59:13,433 Parameters: current_time              = 0.0060000200028298
yt : [INFO     ] 2019-06-10 12:59:13,434 Parameters: domain_dimensions         = [32 32 32]
yt : [INFO     ] 2019-06-10 12:59:13,435 Parameters: domain_left_edge          = [0. 0. 0.]
yt : [INFO     ] 2019-06-10 12:59:13,437 Parameters: domain_right_edge         = [1. 1. 1.]
yt : [INFO     ] 2019-06-10 12:59:13,439 Parameters: cosmological_simulation   = 0.0
Parsing Hierarchy : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [00:00&lt;00:00, 5931.42it/s]
yt : [INFO     ] 2019-06-10 12:59:13,487 Gathering a field list (this may take a moment.)
</code></pre><pre><code class=language-python>max_vals = dd.max([&quot;density&quot;, &quot;temperature&quot;, &quot;velocity_magnitude&quot;])
print(max_vals)
</code></pre><pre><code>(7.73426503924e-24 g/cm**3, 24826104.0 K, 86290042.8768639 cm/s)
</code></pre><p>I want to highlight something here which sometimes slips by &ndash; if you were to access the array hanging off an object like <code>dd</code>, like this:</p><pre><code class=language-python>dd[&quot;density&quot;]
</code></pre><pre><code>YTArray([4.92775113e-31, 4.94005233e-31, 4.93824694e-31, ...,
         1.12879234e-25, 1.59561490e-25, 1.09824903e-24]) g/cm**3
</code></pre><p>The entire array is loaded into memory. This is done through a different chunk style, <code>"all"</code>, which pre-allocates an array and then loads the whole thing into memory in a single go. I should note that there are a few reasons that this needs to always be provided in the same order as the <code>"io"</code> chunking style, which has presented some fun struggles in refactoring that I may mention later on.</p><p>But above, we aren&rsquo;t accessing <code>dd["density"].max()</code> and instead are accessing <code>dd.max("density")</code> (along with two other fields, temperature and the magnitude of velocity.) This operation, inspired by the numpy / xarray syntax, iterates over chunks and computes the running max.</p><p>There are a few other fun operations that I mentioned last time like <code>argmax</code> and whatnot, but for now let&rsquo;s just look at <code>max</code>. While the syntactic sugar for calling <code>dd.max()</code> is reasonably recent, the underpinning functionality dates back about a decade and has, from the start, been MPI-parallel. It hasn&rsquo;t always been elegant, but it&rsquo;s been parallel and memory-conservative.</p><p>So how does <code>.max()</code> (and, its older form, <code>.quantities["MaximumValue"]()</code>) work? Let&rsquo;s take a look at the source.</p><pre><code class=language-python>dd.max??
</code></pre><pre><code>Signature: dd.max(field, axis=None)
Source:   
    def max(self, field, axis=None):
        r&quot;&quot;&quot;Compute the maximum of a field, optionally along an axis.

        This will, in a parallel-aware fashion, compute the maximum of the
        given field.  Supplying an axis will result in a return value of a
        YTProjection, with method 'mip' for maximum intensity.  If the max has
        already been requested, it will use the cached extrema value.

        Parameters
        ----------
        field : string or tuple field name
            The field to maximize.
        axis : string, optional
            If supplied, the axis to project the maximum along.

        Returns
        -------
        Either a scalar or a YTProjection.

        Examples
        --------

        &gt;&gt;&gt; max_temp = reg.max(&quot;temperature&quot;)
        &gt;&gt;&gt; max_temp_proj = reg.max(&quot;temperature&quot;, axis=&quot;x&quot;)
        &quot;&quot;&quot;
        if axis is None:
            rv = ()
            fields = ensure_list(field)
            for f in fields:
                rv += (self._compute_extrema(f)[[0;36m1],)
            if len(fields) == [0;36m1:
                return rv[[0;36m0]
            else:
                return rv
        elif axis in self.ds.coordinates.axis_name:
            r = self.ds.proj(field, axis, data_source=self, method=&quot;mip&quot;)
            return r
        else:
            raise NotImplementedError(&quot;Unknown axis %s&quot; % axis)
File:      ~/yt/yt/yt/data_objects/data_containers.py
Type:      method
</code></pre><p>(One fun bit here is that if you supply an axis and it&rsquo;s a spatial axis, this will project along the axis.)</p><p>Looks like it calls <code>._compute_extrema</code> so let&rsquo;s take a look there:</p><pre><code class=language-python>dd._compute_extrema??
</code></pre><pre><code>Signature: dd._compute_extrema(field)
Docstring: &lt;no docstring&gt;
Source:   
    def _compute_extrema(self, field):
        if self._extrema_cache is None:
            self._extrema_cache = {}
        if field not in self._extrema_cache:
            # Note we still need to call extrema for each field, as of right
            # now
            mi, ma = self.quantities.extrema(field)
            self._extrema_cache[field] = (mi, ma)
        return self._extrema_cache[field]
File:      ~/yt/yt/yt/data_objects/data_containers.py
Type:      method
</code></pre><p>(Fun fact: until I saw the source code right now, I was prepared to say that it computed all the extrema in a single go. Glad there&rsquo;s a backspace key. I should probably file an issue.)</p><p>This calls <code>self.quantities.extrema</code> for each field, since it&rsquo;s nearly just as cheap to do both min and max in a single pass, and sometimes folks&rsquo;ll want both.</p><p>So we&rsquo;re starting to see the underpinnings here &ndash; <code>.quantities</code> is where lots of the fun things happen. What is it?</p><pre><code class=language-python>dd.quantities.extrema??
</code></pre><pre><code>Signature:      dd.quantities.extrema(fields, non_zero=False)
Type:           Extrema
String form:    &lt;yt.data_objects.derived_quantities.Extrema object at 0x7db454d7a2e8&gt;
File:           ~/yt/yt/yt/data_objects/derived_quantities.py
Source:        
class Extrema(DerivedQuantity):
    r&quot;&quot;&quot;
    Calculates the min and max value of a field or list of fields.
    Returns a YTArray for each field requested.  If one, a single YTArray
    is returned, if many, a list of YTArrays in order of field list is
    returned.  The first element of each YTArray is the minimum of the
    field and the second is the maximum of the field.

    Parameters
    ----------
    fields
        The field or list of fields over which the extrema are to be
        calculated.
    non_zero : bool
        If True, only positive values are considered in the calculation.
        Default: False

    Examples
    --------

    &gt;&gt;&gt; ds = load(&quot;IsolatedGalaxy/galaxy0030/galaxy0030&quot;)
    &gt;&gt;&gt; ad = ds.all_data()
    &gt;&gt;&gt; print ad.quantities.extrema([(&quot;gas&quot;, &quot;density&quot;),
    ...                              (&quot;gas&quot;, &quot;temperature&quot;)])

    &quot;&quot;&quot;
    def count_values(self, fields, non_zero):
        self.num_vals = len(fields) * [0;36m2

    def __call__(self, fields, non_zero = False):
        fields = ensure_list(fields)
        rv = super(Extrema, self).__call__(fields, non_zero)
        if len(rv) == [0;36m1: rv = rv[[0;36m0]
        return rv

    def process_chunk(self, data, fields, non_zero):
        vals = []
        for field in fields:
            field = data._determine_fields(field)[[0;36m0]
            fd = data[field]
            if non_zero: fd = fd[fd &gt; [0;36m0.0]
            if fd.size &gt; [0;36m0:
                vals += [fd.min(), fd.max()]
            else:
                vals += [array_like_field(data, HUGE, field),
                         array_like_field(data, -HUGE, field)]
        return vals

    def reduce_intermediate(self, values):
        # The values get turned into arrays here.
        return [self.data_source.ds.arr([mis.min(), mas.max()])
                for mis, mas in zip(values[::[0;36m2], values[[0;36m1::[0;36m2])]
Call docstring: Calculate results for the derived quantity
</code></pre><p>Ah, this is starting to make sense!</p><p>All the <code>DerivedQuantity</code> objects</p><p>What all do we have?</p><pre><code class=language-python>dd.quantities.keys()
</code></pre><pre><code>dict_keys(['WeightedAverageQuantity', 'TotalQuantity', 'TotalMass', 'CenterOfMass', 'BulkVelocity', 'WeightedVariance', 'AngularMomentumVector', 'Extrema', 'SampleAtMaxFieldValues', 'MaxLocation', 'SampleAtMinFieldValues', 'MinLocation', 'SpinParameter'])
</code></pre><p>Looking at these, there&rsquo;s likely a common theme that is pretty obvious &ndash; they&rsquo;re all pretty easily parallelizable things! Sure, there might need to be some reductions at the end, but these are all pretty straightforward combinations of fields and parameters.</p><p>The way the base class works is interesting, and we can use that to break down what is going on here in a way that demonstrates how this relies on chunking:</p><pre><code class=language-python>yt.data_objects.derived_quantities.DerivedQuantity??
</code></pre><pre><code>Init signature: yt.data_objects.derived_quantities.DerivedQuantity(data_source)
Docstring:      &lt;no docstring&gt;
Source:        
class DerivedQuantity(ParallelAnalysisInterface):
    num_vals = -[0;36m1

    def __init__(self, data_source):
        self.data_source = data_source

    def count_values(self, *args, **kwargs):
        return

    def __call__(self, *args, **kwargs):
        &quot;&quot;&quot;Calculate results for the derived quantity&quot;&quot;&quot;
        # create the index if it doesn't exist yet
        self.data_source.ds.index
        self.count_values(*args, **kwargs)
        chunks = self.data_source.chunks([], chunking_style=&quot;io&quot;)
        storage = {}
        for sto, ds in parallel_objects(chunks, -[0;36m1, storage = storage):
            sto.result = self.process_chunk(ds, *args, **kwargs)
        # Now storage will have everything, and will be done via pickling, so
        # the units will be preserved.  (Credit to Nathan for this
        # idea/implementation.)
        values = [ [] for i in range(self.num_vals) ]
        for key in sorted(storage):
            for i in range(self.num_vals):
                values[i].append(storage[key][i])
        # These will be YTArrays
        values = [self.data_source.ds.arr(values[i]) for i in range(self.num_vals)]
        values = self.reduce_intermediate(values)
        return values

    def process_chunk(self, data, *args, **kwargs):
        raise NotImplementedError

    def reduce_intermediate(self, values):
        raise NotImplementedError
File:           ~/yt/yt/yt/data_objects/derived_quantities.py
Type:           RegisteredDerivedQuantity
Subclasses:     WeightedAverageQuantity, TotalQuantity, CenterOfMass, BulkVelocity, WeightedVariance, AngularMomentumVector, Extrema, SampleAtMaxFieldValues, SpinParameter
</code></pre><p>The key thing I want to highlight here is that this is rather simple in concept; the chunks are iterated over in parallel (via the <code>parallel_objects</code> routine, which parcels them out to different processors), processed, and then the reduction happens through <code>reduce_intermediate</code>.</p><p>There are a few things to note here &ndash; this is actually units-aware, which means that even if you&rsquo;ve got (for some reason) <code>cm</code> for a quantity on one processor and <code>km</code> on another, it will correctly convert them. The other is that the set up is such that only the <code>process_chunk</code> and <code>reduce_intermediate</code> operations need to be implemented, along with setting some properties.</p><p>But, we&rsquo;re getting a bit far away from the topic at hand, which is why how chunking is set up can cause some issues with exposing data to dask. And so I want to return to the notion of the <code>"io"</code> chunking and how this works for differently indexed datasets.</p><h2 id=fine--and-coarse-grained-indexing>Fine- and Coarse-grained Indexing</h2><p>What yt does during the selection of data is key to how it thinks about the processings of that data. The way that data can be provided to yt takes several forms:</p><ul><li>Regularly structured grids and grid based data, where there may be overlapping regions (typically with one &ldquo;authoritative source of truth&rdquo; as in
<a href=https://en.wikipedia.org/wiki/Adaptive_mesh_refinement target=_blank rel=noopener>adaptive mesh refinement</a>)</li><li>Irregular grids, where the distance between points may vary along each spatial axis</li><li>Unstructured mesh, where the data arrives in hexahedra, tetrahedra, etc, and there is typically a well-defined form for evaluating field values internal to each polyhedra</li><li>Discrete, or particle-based datasets, where each point is sampled at some location that we don&rsquo;t know <em>a priori</em> &ndash; for instance, N-body simulations</li><li>Octree or block-structured data, which can in some cases be thought of as a special-case of grid based data but that follows a more regular form</li></ul><p>Several of these have a common trait that comes in quite handy for yt &ndash; namely, that the <em>index</em> of the data occupies considerably less memory than the data itself.</p><h3 id=grid-indexing>Grid Indexing</h3><p>For instance, when dealing with a grid of data, typically that grid can be defined by a set of properties such as:</p><ul><li>&ldquo;origin&rdquo; corner of the grid (&ldquo;left edge&rdquo;)</li><li>&ldquo;terminal&rdquo; corner of the grid (&ldquo;right edge&rdquo;)</li><li>dimensions along each axis</li><li>if irregular, the cell-spacing along each axis</li></ul><p>There are of course a handful of other attributes that might be useful (and which we can sometimes deduce) but these are the basics. If we imagine that each of these requires 64-bits per axis per value, a standard (regular) grid requires 576 bits, or 72 bytes. If we were storing the actual value locations, each would require 3 64-bit numbers &ndash; which means that as soon as we were storing 3 of them, we would</p><p>(Of course, one probably doesn&rsquo;t need to store dimensions as 64 bits, and there are also probably some other ways to reduce the info necessary, but as
<a href=https://en.wikipedia.org/wiki/Straw_man target=_blank rel=noopener>straw-person arguments</a> go, this isn&rsquo;t so bad.)</p><p>What we can get to with this is that for grid and other regular datasets, it&rsquo;s reasonably <em>cheap</em> to index the data. So when we create a data object, for instance:</p><pre><code class=language-python>sp = ds.sphere(&quot;center&quot;, (100.0, &quot;kpc&quot;))
</code></pre><p>yt can determine <em>without touching the disk</em> how many grid cells intersect it, and thus it can pre-allocate arrays of the correct size and fill them in progressively, in whatever fashion it deems best for IO purposes.</p><p>This isn&rsquo;t without cost &ndash; computing the intersections can be quite costly, and so we do some things to cache those. (The cost/benefit of caching often bites us when we are dealing with large unigrid datasets, though.) This was all designed to prevent having to call a big <code>np.concatenate</code> at some point in the operation when chunking based on <code>"all"</code>, but it&rsquo;s not always obvious to me that the balance was correctly struck here.</p><p>When an object is created, no selection is conducted until a field is requested. At some point in the call stack once a field is asked for, the function <code>index._identify_base_chunk</code> is called. This is where things are different for particles, but we&rsquo;ll get to that later.</p><h3 id=particle-indexing>Particle Indexing</h3><p>When dealing with particles, our indexing requirements are very different. Here, the cost of storing the index values is very high &ndash; but, we also don&rsquo;t want to have to perform too much IO. So we&rsquo;re stuck minimizing how much IO we do, while also minimizing the amount of information we store in-memory once we &ldquo;index&rdquo; a dataset.</p><p>In yt-4.0, we accomplish this through the use of bitmap indices, which I described a little bit in the first post. The basic idea of this is that each &ldquo;file&rdquo; (which can be subsets of a single file, and is better thought of as an IO block of some type) is assigned some unique ID. All the files are iterated over and for each discrete point included in that file, an index into a space-filling curve is generated. We use a resonably coarse space filling curve for the first iteration &ndash; say, a level 2 curve &ndash; and that allows ambiguities. This is essentially a binning operation.</p><p>(Incidentally, we often use
<a href=https://en.wikipedia.org/wiki/Z-order target=_blank rel=noopener>Morton Z-Ordering</a> because it&rsquo;s just easier to explain. We might get better compression if we used
<a href=https://en.wikipedia.org/wiki/Hilbert_curve target=_blank rel=noopener>Hilbert</a> since consecutive values may be more likely to be identical.)</p><p>At the end of the first iteration, we have a key-value store of bitarrays, where the key is the file ID and the value is a set of 1&rsquo;s and 0&rsquo;s, where a 1 indicates that a particle is found in a given region identified by the space-filling curve index corresponding with that 1&rsquo;s index in the array. So, for instance, if we had a level 3 index, we would have a set of bitarrays that looked like:</p><pre><code>001 000 101
010 011 011
...
</code></pre><p>So, if we read from left-to-right, the first file has particles that live in (zero-indexed) indices 2, 6 and 8. The second file has particles in indices 1, 4, 5, 7 and 8.</p><p>If we know that our selector only intersects areas touched by index 2, then we only have to read from the first file.</p><p>This would work great if we had particles that were distributed pretty homogeneously on large scales, but in many cases, we don&rsquo;t. Sometimes when particles are written to disk they are sorted on some high-order index and then written out in that order. What yt does is perform a secondary, as-needed indexing based on where there are &ldquo;collisions&rdquo; &ndash; i.e., ambiguities. A set of logical operations is performed across all the bitarrays to identify where multiple files overlap; following this, a <em>second</em> round of indexing is conducted at a much higher spatial order.</p><p>In doing this, we are able to pinpoint with reasonably high precision the file or files that need to be read to get data from a given selector, and minimize very precisely the amount of over-reading that is done.</p><p>Unfortunately, this doesn&rsquo;t give us the ability to explicitly allocate arrays of the correct size. (And, the memory overhead of regaining that ability would be quite high.) But as we saw above, yt doesn&rsquo;t want to do big concatenation operations! So it does the thing I really wish it didn&rsquo;t, which is &mldr; it reads all the position data in IO chunks, figures out how big it is (which only requires a running tally, not a set of allocated arrays), then allocates and fills that single big array.</p><p>This isn&rsquo;t really that efficient, and it arises from the case where the indexing is comparatively cheap.</p><p>But all of this arises out of the design decision that we need to optimize for the case that we want a single big array, rather than a bunch of small arrays &ndash; i.e., for the case of:</p><pre><code class=language-python>ds.r[:][&quot;density&quot;].max()
</code></pre><p>as opposed to</p><pre><code class=language-python>ds.r[:].max(&quot;density&quot;)
</code></pre><h2 id=didnt-you-say-youd-be-talking-about-dask>&mldr;didn&rsquo;t you say you&rsquo;d be talking about Dask?</h2><p>Well, this is where dask comes in! And, it&rsquo;s also why interfacing to dask is a bit tricky &ndash; because we do a lot of work ahead of time before allocating any arrays, and then we get rid of the information generated during that work.</p><p>In an ideal world, what we would be able to do is to export a <em>data object</em> (such as a sphere or cylinder or rectangular prism) and a <em>field-type</em> (so we knew if it was a vector, or particles, or nodal/zonal data) as a dask array. For instance, if instead of returning an array (specifically, a <code>YTArray</code> or <code>unyt_array</code>) when we accessed <code>sp["density"]</code>, it returned a <code>DaskArray</code>, we would open up a number of new and interesting techniques.</p><p>But to do that, we need to be able to know in advance the chunk sizes, and more to the point, we need to be able to specify a function that returns each chunk <em>uniquely</em>.</p><h2 id=next-entry-iterables-and-io>Next Entry: Iterables and IO</h2><p>Turns out, I thought I&rsquo;d be done with this entry a lot sooner than I was!</p><p>In the next blog post, which hopefully will take less than the 8 days this one did, I&rsquo;ll talk about why this is (currently) hard, how to fix that, and what we&rsquo;re doing to fix it.</p></div><div class="media author-card content-widget-hr"><img class="avatar mr-3 avatar-circle" src="https://s.gravatar.com/avatar/b279d746f54e2cd6062e8279a767c4bc?s=200')" alt="Matthew Turk"><div class=media-body><h5 class=card-title><a href=https://matthewturk.github.io/>Matthew Turk</a></h5><h6 class=card-subtitle>Assistant Professor of Information Sciences</h6><p class=card-text>I am interested in the intersection of data analysis, visualization and open source in the sciences.</p><ul class=network-icon aria-hidden=true><li><a href=mailto:matthewturk@gmail.com><i class="fas fa-envelope"></i></a></li><li><a href=https://twitter.com/powersoffour target=_blank rel=noopener><i class="fab fa-twitter"></i></a></li><li><a href="https://scholar.google.com/citations?user=QTmv2p0AAAAJ&hl=en" target=_blank rel=noopener><i class="ai ai-google-scholar"></i></a></li><li><a href=https://github.com/matthewturk target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/python.min.js></script><script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin=anonymous></script><script>const code_highlighting=true;</script><script>const isSiteThemeDark=false;</script><script>const search_config={"indexURI":"/index.json","minLength":1,"threshold":0.3};const i18n={"no_results":"No results found","placeholder":"Search...","results":"results found"};const content_type={'post':"Posts",'project':"Projects",'publication':"Publications",'talk':"Talks",'slides':"Slides"};</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src=/js/academic.min.37431be2d92d7fb0160054761ab79602.js></script><div class=container><footer class=site-footer><p class=powered-by></p><p class=powered-by>Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true><a href=# class=back-to-top><span class=button_icon><i class="fas fa-chevron-up fa-2x"></i></span></a></span></p></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i>Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i>Download</a><div id=modal-error></div></div></div></div></div></body></html>