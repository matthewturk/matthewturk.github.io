[{"authors":["admin"],"categories":null,"content":"Matthew Turk is an assistant professor in the School of Information Sciences and also holds an appointment with the Department of Astronomy in the College of Liberal Arts and Sciences. His research is focused on how individuals interact with data and how that data is processed and understood.\nAt the University of Illinois, he leads the Data Exploration Lab and teaches in Data Visualization, Data Storytelling, and Computational Astrophysics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1591300197,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://matthewturk.github.io/author/matthew-turk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/matthew-turk/","section":"authors","summary":"Matthew Turk is an assistant professor in the School of Information Sciences and also holds an appointment with the Department of Astronomy in the College of Liberal Arts and Sciences. His research is focused on how individuals interact with data and how that data is processed and understood.","tags":null,"title":"Matthew Turk","type":"authors"},{"authors":[],"categories":[],"content":"Making Slide Presentations in Obsidian I do nearly all of my presentations in RevealJS. I originally picked it up sometime just before the pandemic, but then during 2020 I went all in and decided to make it my primary or exclusive means of giving talks. (Actually, that timeline might not quite work out. But who cares? No one is going to fact check me! It\u0026rsquo;s snowing out and no one has time for that!)\nFor the longest time, I\u0026rsquo;ve wanted the ability to easily reuse slides between presentations, but also to override and modify the order, etc. I\u0026rsquo;ve had a figures repository that I use to store the more complex figures I like to use in talks \u0026ndash; I deploy all my presentations to Github, so anything that lives on the same domain works fine for CORS etc. (Usually.)\nBut what I really want was to have concepts that I can rearrange and put in different orders, and then use that to come up with sets of presentations. This is super handy for classes, too \u0026ndash; the entire process of modularization of presentations and learning outcomes, and then arranging them easily and exporting to a presentation.\nA couple weeks ago, I needed to develop a presentation and it hit me that I could do exactly this, and if I tried really hard, I might be able to make it work with a Kanban-style interface. I didn\u0026rsquo;t quite have time during the development of that talk to implement it fully, but I got a good start on it and completed it a few days later. I ended up building it in Obsidian, out of:\nSlides Extended, a recent fork of the original Advanced Slides plugin Obsidian Kanban, a really slick kanban-style interface backed by plain markdown Templater, one of the most essential plugins for any kind of automation in Obsidian What I wanted was the ability to have one or more collections of slides that I could drag and drop into a presentation, and then to export that to a presentation file. And it turns out, implementing this was not too hard after all!\nI set up a directory in my vault under projects called talks, and under it I put a directory called sequences. Each item in sequences is a .md file that contains the content of the slide or slides that comprise that \u0026lsquo;sequence\u0026rsquo; or module.\n![warning]\nThese markdown files, at least for now, cannot include metadata or frontmatter sections. This interferes with how the inclusions are parsed in slides-extended and it took me a while to figure that out.\nUPDATE: Just as I was writing this, ebullient fixed it!\nSo for example, inside my What is yt.md file, I have:\n## Who Am I + Computational **astrophysicist** by training + Developed simulation platforms and analysis **tools** for astrophysics + Worked in interdisciplinary applications, study **communities** of practice in open source scientific software + Tenure-track at the School of Information Sciences, developing and implementing a **grammar** of volumetric analysis The others can be more complicated, and in many I\u0026rsquo;ve included things like figures and multi-stage builds and the like.\nJust as it is, this would allow creation of talks that are made up just by inclusions/embeddings of slides. For example, by creating a note with this:\n![[Some Slide]] --- ![[Another Slide]] This views (and exports) just fine from slides-extended. This is great! But what I wanted to be able to do was to be a little more flexible and agile \u0026ndash; ideally, I\u0026rsquo;d have a Kanban board where I could drag and reorder \u0026ldquo;Some Slide\u0026rdquo; and \u0026ldquo;Another Slide\u0026rdquo; to be in a different order. And, while I\u0026rsquo;m at it, it would be nice to be able to have a \u0026ldquo;library\u0026rdquo; of slides that I can move into a list and then export.\nWith the obsidian Kanban plugin, though, it turns out this is not overly hard to set up. For instance, here is a Kanban board with a list called \u0026ldquo;Slides\u0026rdquo; and one called \u0026ldquo;Candidate Slides\u0026rdquo;.\nBy using templater, we can set it up to iterate over the open document\u0026rsquo;s list \u0026ldquo;Slides,\u0026rdquo; convert all \u0026ldquo;just-a-link\u0026rdquo; entries to embeddings, and export to a new file. For an added bonus, if the currently open document isn\u0026rsquo;t a Kanban board, it won\u0026rsquo;t do anything.\nHere\u0026rsquo;s my templater template:\n--- aliases: [] tags: - talks/mine created: 2023-09-13 modified: 2023-09-13 theme: mturk-slides transition: none center: false fragments: true hash: true width: 1024 height: 768 margin: 0.02 defaultTemplate: \u0026quot;[[blankslide]]\u0026quot; backgroundTransition: none reveald3: - runLastState: false enableTitleSlide: false title: TITLE GOES HERE presenterName: Matthew Turk venueName: Unknown talkDate: Unknown --- # Title of the Talk Here ## Matthew Turk \u0026lt;p data-markdown=true\u0026gt;School of Information Sciences\u0026lt;br/\u0026gt; University of Illinois at Urbana-Champaign\u0026lt;br/\u0026gt; \u0026lt;tt\u0026gt;mjturk@illinois.edu\u0026lt;/tt\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;tt\u0026gt;matthewturk.github.io\u0026lt;/tt\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;!-- .slide: class=\u0026quot;titleslide\u0026quot; --\u0026gt; --- \u0026lt;% const filePath = tp.config.active_file; const converted = []; if (filePath) { const p = dv.page(filePath.path) const f = p.file; if (p['kanban-plugin'] == 'board') { const items = f.tasks.filter(t =\u0026gt; t.section.subpath == \u0026quot;Slides\u0026quot;); items.forEach( i =\u0026gt; { if (i.text.startsWith(\u0026quot;[[\u0026quot;) \u0026amp;\u0026amp; i.text.endsWith(\u0026quot;]]\u0026quot;)) { converted.push(`!${i.text}`); } else { converted.push(i.text); } }); } } -%\u0026gt; \u0026lt;% converted.join(\u0026quot;\\n\\n---\\n\\n\u0026quot;) -%\u0026gt; And just like that, I can rearrange slides, and then when I\u0026rsquo;m ready I press alt-n to create a new talk-template instance which takes my kanban board and converts to a full markdown note suitable for exporting as a full-on RevealJS presentation.\n","date":1734813143,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1734813343,"objectID":"82f5782201e8f1da88f9addbd39656db","permalink":"https://matthewturk.github.io/post/kanban-slides-in-obsidian/","publishdate":"2024-12-21T14:32:23-06:00","relpermalink":"/post/kanban-slides-in-obsidian/","section":"post","summary":"How to make slides from a Kanban Board in Obsidian","tags":[],"title":"Kanban Slides in Obsidian","type":"post"},{"authors":[],"categories":[],"content":"As I\u0026rsquo;ve mentioned on here before, I use Obsidian a lot. One thing that I\u0026rsquo;ve really come to like as of late is the meta-bind plugin. This let\u0026rsquo;s you add items to your notes that edit the frontmatter metadata, as well as buttons to execute actions, and special-purpose \u0026ldquo;views\u0026rdquo; of the data in your notes.\nOne of the most common types of notes I use is a meeting-notes template. (This is a link to the current version, but I try to keep that repo sorta up to date.) I have places for attendees as well as TODOs. What I was finding was that I often wanted to update the attendees from within the page, but my notes got really long and I had to scroll around etc. Another thing that I\u0026rsquo;ve been doing is adding \u0026ldquo;person\u0026rdquo; entries, so that the meeting attendees link to pages of notes I keep on those people.\nWhat I realized I needed was a way to keep the meta-bind inputs and actions visible at all times, even when I\u0026rsquo;m down in the page. So I decided to write a plugin that would find a contextually-appropriate sidecar file and keep it open in the panel to the right. After a couple weeks of iterating on the specifics, it\u0026rsquo;s now in the Obsidian Community Plugin list, and it\u0026rsquo;s available in source code at matthewturk/obsidian-sidecar-panel.\nThis lets me add a panel that is queried from a sidecar-panel entry in the frontmatter, a set of tag mappings, or a default sidecar file. For my meetings, I now have a button to spin up a modal-form to make a new person entry, a button for a new todo, a list of the attendees (which I can add to right there!) and a view of all the previous TODOs for the people currently attending the meeting. You can see that panel here.\nGive it a shot, if you like!\n","date":1709485200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1709496981,"objectID":"382caa9600a299bf559aaa13a9898ab3","permalink":"https://matthewturk.github.io/post/contextual-sidecar-in-obsidian/","publishdate":"2024-03-03T12:00:00-05:00","relpermalink":"/post/contextual-sidecar-in-obsidian/","section":"post","summary":"A plugin to keep helpful stuff available in a sidecar panel.","tags":[],"title":"Contextual Sidecar in Obsidian","type":"post"},{"authors":[],"categories":[],"content":"A project I\u0026rsquo;ve been noodling with for the last few years (okay a lot of years) has been reverse engineering the file formats for the game The Summoning. This has led to things like learning kaitai.io, digging into DOSbox, Ghidra (and writing some scripts), and exploring lots of things in a parsing library. I even live streamed some reversing work and a walkthrough of my efforts a while back.\nI played this game as a kid. It came out in 1992, and my grandfather bought it for us. It\u0026rsquo;s \u0026hellip; long. Like, really, really, really long. And there\u0026rsquo;s a plot! With NPCs! (In fact, the person who designed the plot now runs a rather awesome Science Fiction Magazine) But it\u0026rsquo;s also kind of \u0026hellip; grindy?\nAnyway, it has its fans online, although the spiritual sequel Veil of Darkness seems to be an awful lot more popular; David L. Craddock even did an oral history of it which he included in the book Game Dev Stories 5, which seems to be rather tricky to get ahold of lately. (I snagged it in a Storybundle).\nBoth The Summoning and Veil of Darkness have recently been released on GOG and Steam.\nThere are still lots of mysteries for the code to give up, but I find it absolutely fascinating to dig in and learn about the different ways that the developers did things. Honestly, it\u0026rsquo;s just incredible to be able to appreciate the craft of their work \u0026ndash; and seeing how they saved space in some ways, how they used swap files, and learn about how they operated within the confines of the MS-DOS era. I\u0026rsquo;ve been tempted a few times to try to reach out to one or more of them, but have always been a bit too afraid to.\nOne of the biggest mysteries \u0026ndash; which I am certain has been discovered by others, but which I did not \u0026ndash; was how the game\u0026rsquo;s copy protection worked. When you start the game, it shows a screen like this:\n(In this case though I\u0026rsquo;ve put the first four faces in already.) My assumption had always been that the correct ordering of the faces was stored somewhere in either the executable or in one of the affiliated data files, likely in an obfuscated fashion. But finding out precisely where they were \u0026hellip; that had eluded me.\nUNTIL NOW!\n(Using that phrasing is a joke that only hard-core TheSummoning-heads will get. Haha, right?)\nI recently took the time to implement a script that converted references in Ghidra to segmented memory references, defaulting to pointing at the local DS section. (Of course I don\u0026rsquo;t automate all of it \u0026ndash; I hardcode the DS!) This really helped me understand where strings were used. And shortly after this, I was able to finally understand how the string formatting function worked, and so I found myself knowing where the string \u0026ldquo;Please turn to page\u0026rdquo; was being fed in, and the number that arrived just after it \u0026ndash; the page number. With a bit of work, I dug deeper and found a function that Ghidra decompiled into:\niVar1 = page_number + 1; iVar2 = iVar1; for (iVar4 = 1; iVar4 \u0026lt; 6; iVar4 = iVar4 + 1) { iVar2 = (iVar4 + page_number) % 5; iVar3 = (iVar1 * iVar2 * iVar2 * iVar2 + iVar2 * 0x45 + iVar1 * 0x2f) % 0x8b; iVar2 = iVar3 / 0xc; *(int *)((int)\u0026amp;DAT_235b_44ec + (iVar4 + -1) * 2) = iVar3 % 0xc; } return iVar2; Huh. This was where things got interesting. I had assumed that if things had been procedurally generated, at least one of the numbers that showed up would be the number of pages in the manual or something like that. But this \u0026hellip; was not featuring any of those.\nAnd yet.\nOne of the difficult things for me is understanding how the graphics are done. I\u0026rsquo;m still learning, but I believe the software uses software interrupt 60 to signal that sprites or text or other things are ready to be written to the screen. (Maybe.) But where they were stored in memory was tricky, and so I was not entirely sure that I understood precisely where to look for memory accesses to see where the drawing would take place.\nWhat grabbed my attention here was the 0xc. I\u0026rsquo;m not the kind of person who can effortlessly convert back and forth to hex, but I recognized that as 12 \u0026ndash; the same as the number of faces we have to choose from. And also, it only has one parameter \u0026ndash; the page number.\nI coded this up in Python:\nvals = {} for page_i in range(15): vals[page_i] = [] iVar1 = page_i + 1 ivar2 = iVar1 for iVar4 in range(1, 6): iVar2 = (iVar4 + page_i) % 5 iVar3 = (iVar1 * iVar2 * iVar2 * iVar2 + iVar2 * 0x45 + iVar1 * 0x2f) % 0x8b iVar2 = iVar3 / 0xc vals[page_i].append(iVar3 % 0xc) print(f\u0026quot;{page_i: 2d}: {vals[page_i]}\u0026quot;) (I want to note that I left almost everything exactly as-is, rather than potentially introduce any bugs from accidental errors like reducing a factor incorrectly.) Here is the output:\n0: [9, 6, 3, 1, 11] 1: [1, 5, 9, 10, 2] 2: [0, 5, 2, 2, 1] 3: [1, 1, 2, 8, 2] 4: [0, 7, 3, 9, 4] 5: [7, 3, 11, 0, 4] Even though I didn\u0026rsquo;t have a mapping from the integer to the face, comparing it to the first six pages from the manual made it immediately clear \u0026ndash; this was it.\nI know this is not a huge deal, in the grand scheme of things. I\u0026rsquo;m sure many people have figured this out before me. But to tell the truth, this was a really fun and gratifying thing to figure out, and I am pretty glad I was able to ring in the new year with it.\n","date":1704304800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1704321218,"objectID":"3d164dfa7edb23158fb23d77edb5ab63","permalink":"https://matthewturk.github.io/post/how-i-rang-in-2024/","publishdate":"2024-01-03T13:00:00-05:00","relpermalink":"/post/how-i-rang-in-2024/","section":"post","summary":"A project I\u0026rsquo;ve been noodling with for the last few years (okay a lot of years) has been reverse engineering the file formats for the game The Summoning. This has led to things like learning kaitai.","tags":[],"title":"How I Rang In 2024","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;ve started keeping notes on papers in Obsidian and I thought I\u0026rsquo;d share the workflow, which so far is pretty straightforward and easy to set up. This uses the binary file manager plugin and the annotator plugin. Annotator uses hypothes.is with local storage (in markdown) of the annotations.\nI set up a folder, attachments/papers, which is where I\u0026rsquo;ll be putting all my PDFs. I have this template, templates/pdf-annotation:\n--- aliases: [\u0026quot;{{NAME}} - Annotation\u0026quot;] type: pdf fileClass: pdf-annotation tags: - pdf - notes - paper created: {{NOW:YYYY-MM-DD}} modified: {{NOW:YYYY-MM-DD}} annotation-target: \u0026quot;{{PATH}}\u0026quot; --- # {{FULLNAME}} In the binary file manager plugin, I turn off all the file extensions except PDF, and I have the \u0026ldquo;New file location\u0026rdquo; set to reviews/papers. That\u0026rsquo;s where the metadata markdown files will go. I point at templates/pdf-annotation.md for the template file location, and I turn on \u0026lsquo;watch.\u0026rsquo; (Note that there\u0026rsquo;s an open issue about having it only watch one particular directory, which I am going to enable if it gets implemented. Not sure I will try implementing it myself.) Whenever I have a PDF I want to store a local copy of, I drag it into the attachments/papers directory, and the metadata file gets created. Then I open up the newly created file, and I have a side-by-side annotation view!\nI think, but I\u0026rsquo;m not totally sure, that with a little bit of work it should be possible to make this work without much change (using the same template, location, etc) for online annotations too, which I\u0026rsquo;m going to have a shot at. Annotator supports remote URLs, so the work would just be in making sure I can use the same template both ways.\n","date":1686317533,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686318157,"objectID":"1161316195b94c6986d362b0713b5d6d","permalink":"https://matthewturk.github.io/post/paper-annotation-in-obsidian/","publishdate":"2023-06-09T08:32:13-05:00","relpermalink":"/post/paper-annotation-in-obsidian/","section":"post","summary":"I\u0026rsquo;ve started keeping notes on papers in Obsidian and I thought I\u0026rsquo;d share the workflow, which so far is pretty straightforward and easy to set up. This uses the binary file manager plugin and the annotator plugin.","tags":[],"title":"Paper Annotation in Obsidian","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;ve recently been having fun with yt again. Really, though, I\u0026rsquo;ve been having fun exploring the ways I can use it from different perspectives, and using it to teach myself the library textual.\nA long, long time ago, when we were much younger (around the time Streetlight Manifesto put out their first album, \u0026ldquo;Everything Goes Numb\u0026rdquo;) I started trying to learn and use Enzo. Enzo outputs data in patch-structured refinement, so you have grids that overlap and whatnot and aren\u0026rsquo;t all the same size. The way it outputs it to disk is with a .hierarchy file, which is essentially a serialization of the simulation code \u0026lsquo;walking\u0026rsquo; the hierarchy. So, it outputs grid information, like so:\nGrid = 1 Task = 0 GridRank = 3 GridDimension = 38 38 38 GridStartIndex = 3 3 3 GridEndIndex = 34 34 34 GridLeftEdge = 0 0 0 GridRightEdge = 1 1 1 Time = 0.0060000200028298 SubgridsAreStatic = 0 NumberOfBaryonFields = 24 FieldType = 0 4 5 6 1 49 50 51 52 53 7 8 9 10 11 12 23 24 25 27 19 61 62 83 BaryonFileName = ./DD0030/galaxy0030.cpu0000 CourantSafetyNumber = 0.300000 PPMFlatteningParameter = 0 PPMDiffusionParameter = 0 PPMSteepeningParameter = 0 NumberOfParticles = 0 GravityBoundaryType = 1 Pointer: Grid[1]-\u0026gt;NextGridThisLevel = 0 Pointer: Grid[1]-\u0026gt;NextGridNextLevel = 2 This is the first grid in my old favorite, IsolatedGalaxy. The various parameters include things like the position of the grid in space and so forth, and then at the end, we have these two pointers to the next grid on this level and the next grid on the next level. These two lines are the way that Enzo communicates parentage relationships (and note that Level is not in the block \u0026ndash; you track which level things are on by walking the hierarchy.) The first line here indicates that there are no additional grids on this level, and the second line indicates that the next level starts with grid index 2. The grids are always ordered in this way \u0026ndash; siblings first, then children. That\u0026rsquo;s how you end up with blocks like this:\nGrid = 19 Task = 5 GridRank = 3 GridDimension = 14 10 14 GridStartIndex = 3 3 3 GridEndIndex = 10 6 10 GridLeftEdge = 0.52734375 0.53515625 0.505859375 GridRightEdge = 0.53515625 0.5390625 0.513671875 Time = 0.0060000200028298 SubgridsAreStatic = 0 NumberOfBaryonFields = 24 FieldType = 0 4 5 6 1 49 50 51 52 53 7 8 9 10 11 12 23 24 25 27 19 61 62 83 BaryonFileName = ./DD0030/galaxy0030.cpu0005 CourantSafetyNumber = 0.300000 PPMFlatteningParameter = 0 PPMDiffusionParameter = 0 PPMSteepeningParameter = 0 NumberOfParticles = 50 ParticleFileName = ./DD0030/galaxy0030.cpu0005 GravityBoundaryType = 2 Pointer: Grid[19]-\u0026gt;NextGridThisLevel = 0 Pointer: Grid[19]-\u0026gt;NextGridNextLevel = 0 Pointer: Grid[18]-\u0026gt;NextGridNextLevel = 0 Pointer: Grid[17]-\u0026gt;NextGridNextLevel = 0 Pointer: Grid[16]-\u0026gt;NextGridNextLevel = 0 Pointer: Grid[15]-\u0026gt;NextGridNextLevel = 20 Here, we are in grid 19, but it doesn\u0026rsquo;t have any additional siblings, and it also doesn\u0026rsquo;t have any children. Grids 18, 17, and 16 also do not have any child grids, but 15 does.\nAs you might imagine, while this makes sense and is also straightforward to de-serialize into an in-memory data structure, it can be somewhat tricky to navigate if you just want info about a particular grid and its relationship to other grids.\nEnter, Britton Smith! Britton, who I had the pleasure of attending graduate school with (and, also, continuing to work with up to and including the present day), was a really slick perl script writer.\n(His perl implementation of battleship, including socket communication across LAN machines and a high score table, was a fixture among the PSU astro grads.)\nBritton wrote up a tool to parse the hierarchy and, if I am remembering correctly, present a simple REPL to query for information about the individual grids. Years later he helped me port this concept to Python where it forms the initial indexing stage of the yt Enzo frontend.\nA little while ago, I saw a toot by Glyph Lefkowitz that made me realize I really ought to give a look at textual again.\nSo I built out a very simple Enzo hierarchy explorer that used yt, to try to evoke the old perl script. And it turns out, it was \u0026hellip; incredibly easy!\nI ended up putting it on GitHub at matthewturk/yt_hierarchy_browser. I\u0026rsquo;ve started exploring using things like rich-pixels to display grid slices, but I started to get lost in the \u0026ldquo;messages flow up and properties flow down\u0026rdquo; side of things, and making sure that I had the grid images properly stored in the right places. I will probably come back to that.\nFor now, though, the basics of the app are reasonably small \u0026ndash; three basic components. The first is the App subclass, which sets up the tree viewer on the left and the grid info viewer on the right:\nclass DatasetBrowser(App): CSS_PATH = \u0026quot;yt_hierarchy_browser.css\u0026quot; def compose(self) -\u0026gt; ComposeResult: yield Header() yield Footer() yield GridHierarchyBrowser() yield GridViewer() def on_mount(self) -\u0026gt; None: ghv = self.query_one(GridHierarchyBrowser) ds = yt.load_sample(\u0026quot;IsolatedGalaxy\u0026quot;) assert ds is not None ghv.dataset = ds def on_tree_node_highlighted(self, event: Tree.NodeHighlighted) -\u0026gt; None: if event.node.data is None: return gv = self.query_one(GridViewer) gv.grid = event.node.data['grid'] (I\u0026rsquo;m not sure I\u0026rsquo;ve put my tree node highlighted event handler in the right level of the class structure, but like I said, I\u0026rsquo;m still learning!) This then creates a GridViewer which handles the display whenever a grid is highlighted:\nclass GridViewer(Static): grid: reactive[AMRGridPatch | None] = reactive(None) def compose(self): yield Static(id = \u0026quot;grid_info\u0026quot;) yield Static(id = \u0026quot;grid_view\u0026quot;) def watch_grid(self, grid: AMRGridPatch) -\u0026gt; None: if grid is None: return grid_info = self.query_one(\u0026quot;#grid_info\u0026quot;) grid_info.update(f\u0026quot;Left Edge: {grid.LeftEdge}\\n\u0026quot; f\u0026quot;Right Edge: {grid.RightEdge}\\n\u0026quot; f\u0026quot;Active Dimensions: {grid.ActiveDimensions}\\n\u0026quot; f\u0026quot;Level: {grid.Level}\\n\u0026quot;) Note here that it\u0026rsquo;s getting a grid patch, which gets updated \u0026ndash; I could also do this with the tree node data, but here I\u0026rsquo;m passing the grids so that when I do image display it\u0026rsquo;s a bit easier. And finally, the tree viewer itself:\nclass GridHierarchyBrowser(Static): dataset: reactive[Dataset | None] = reactive(None) def compose(self) -\u0026gt; ComposeResult: yield Tree(label = \u0026quot;Grid Hierarchy\u0026quot;, id = \u0026quot;grid_hierarchy\u0026quot;) def watch_dataset(self, dataset: Dataset) -\u0026gt; None: if dataset is None: return def dictify(g): return {'ActiveDimensions': g.ActiveDimensions, 'LeftEdge': g.LeftEdge, 'RightEdge': g.RightEdge, 'Level': g.Level, 'grid': g} tree: Tree[dict] = self.query_one(Tree) tree.root.expand() def add_children(node, g): data = dictify(g) if len(g.Children) == 0: node.add_leaf(str(g), data = data) else: n = node.add(str(g), data = data) for c in g.Children: add_children(n, c) for grid in dataset.index.select_grids(0): # These are all the root grids node = tree.root.add(str(grid), data = dictify(grid), expand = True) for c in grid.Children: add_children(node, c) Again, the creation of the dict is a bit redundant since I\u0026rsquo;m also grabbing the grid info, but later on I hope that will come in useful. This simply watches the dataset and then fills up the nodes. And, this is what it looks like:\nThis was a pretty fun experiment, and I\u0026rsquo;m pretty excited about exploring more with it.\n","date":1686070448,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686071784,"objectID":"d018b82c51346538871ff462c9ae88b6","permalink":"https://matthewturk.github.io/post/recent-fun-with-yt/","publishdate":"2023-06-06T11:54:08-05:00","relpermalink":"/post/recent-fun-with-yt/","section":"post","summary":"I\u0026rsquo;ve recently been having fun with yt again. Really, though, I\u0026rsquo;ve been having fun exploring the ways I can use it from different perspectives, and using it to teach myself the library textual.","tags":[],"title":"Recent Fun With yt","type":"post"},{"authors":[],"categories":[],"content":"As I noted in a previous post, I\u0026rsquo;ve been using Obsidian for personal note taking and whatnot. I swear by using the Templater plugin, and I have setups for note taking for meetings and check-ins with students.\nI recently formalized something that I do in all of them, which makes it a lot easier to set up. For instance, my meeting-notes template looks like this:\n\u0026lt;%* let projectName = await tp.user.helpfulPrompt(tp, \u0026quot;project\u0026quot;, \u0026quot;#projects\u0026quot;, \u0026quot;Project Name?\u0026quot;); let slugName = tp.user.slugger(projectName); -%\u0026gt; --- fileClass: meeting-notes aliases: [\u0026quot;\u0026lt;% projectName %\u0026gt;-meeting-\u0026lt;% tp.date.now() %\u0026gt;\u0026quot;] tags: - notes - meeting - projects/\u0026lt;% projectName %\u0026gt; project: \u0026lt;% projectName %\u0026gt; created: \u0026lt;% tp.date.now() %\u0026gt; modified: \u0026lt;% tp.date.now() %\u0026gt; date: \u0026lt;% tp.date.now() %\u0026gt; attendees: - Matt Turk - ... title: Notes - \u0026lt;% projectName %\u0026gt; - \u0026lt;% tp.date.now() %\u0026gt; --- # Notes - \u0026lt;% projectName %\u0026gt; - \u0026lt;% tp.date.now() %\u0026gt; ## Agenda and Notes 1. start 2. ... ## Action Items Use the metadata `assigned` like `(assigned:: Matt Turk)` in these. - [ ] ## Other \u0026lt;% await tp.user.safeRenamer(tp, \u0026quot;projects/\u0026quot; + projectName, slugName + \u0026quot; - notes - \u0026quot; + tp.date.now()) %\u0026gt; and my check-in template looks like this:\n\u0026lt;%* let personName = await tp.user.helpfulPrompt(tp, \u0026quot;student\u0026quot;, \u0026quot;#student\u0026quot;, \u0026quot;Student Name?\u0026quot;); let slugName = tp.user.slugger(personName); -%\u0026gt; --- aliases: [] student: \u0026lt;% personName %\u0026gt; tags: - student/\u0026lt;% slugName %\u0026gt; - students/checkin modified: \u0026lt;% tp.date.now() %\u0026gt; created: \u0026lt;% tp.date.now() %\u0026gt; title: \u0026lt;% personName %\u0026gt; - Checkin - \u0026lt;% tp.date.now() %\u0026gt; --- # \u0026lt;% personName %\u0026gt; - Checkin - \u0026lt;% tp.date.now() %\u0026gt; ## Discussion ## Notes ## Action Items \u0026lt;% await tp.file.move(\u0026quot;projects/students/\u0026quot; + slugName + \u0026quot;/\u0026quot; + personName + \u0026quot; - Checkin - \u0026quot; + tp.date.now()) %\u0026gt; I\u0026rsquo;ve set up a couple functions in user scripts that make things a lot easier for me. (I just now noticed I don\u0026rsquo;t use safeRenamer in my check-in template, though, and I\u0026rsquo;ll have to fix that.)\nBecause I have metadata tags for project: and student: in the frontmatter of these templates, I can do some searching to provide an autocomplete for existing values, and also allow for new values to be added.\nHere\u0026rsquo;s my safeRenamer script, which honestly I\u0026rsquo;m not sure is exactly the right one to use, but which is working for me:\nasync function safeRenamer(tp, newFolder, newFilename) { if (!(await app.vault.exists(newFolder))) { await app.vault.createFolder(newFolder); } var newFullpath = `${newFolder}/${newFilename}.md`; var exists = await tp.file.exists(newFullpath); if (exists) { for (let i =1; i \u0026lt;= 100; i++) { newFullpath = `${newFolder}/${newFilename} - ${i}.md`; exists = await tp.file.exists(newFullpath); if (!exists) break; } } // We now have a newFullpath that probably includes the .md extension. // But, we don't want obsidian to use that. if (newFullpath.endsWith(\u0026quot;.md\u0026quot;)) { newFullpath = newFullpath.slice(0, -3); } return tp.file.move(newFullpath); } module.exports = safeRenamer; And, this set of functions helps me figure out what already exists to provide a set of options, as well as allowing for a new value to be added. Here is frontmatterKeyValues.js:\n// https://stackoverflow.com/questions/1960473/get-all-unique-values-in-a-javascript-array-remove-duplicates function onlyUnique(value, index, array) { return array.indexOf(value) === index; } async function frontmatterKeyValues(key, selectionTag) { const dv = app.plugins.plugins[\u0026quot;dataview\u0026quot;].api; if (!selectionTag.startsWith(\u0026quot;#\u0026quot;)) selectionTag = `#${selectionTag}`; keyValues = dv.pages(selectionTag).filter( p =\u0026gt; p[key] ).map( p =\u0026gt; p[key] ).filter(onlyUnique); return keyValues.array(); } module.exports = frontmatterKeyValues; and, finally, helpfulPrompt.js:\nconst OTHER = \u0026quot;Other\u0026quot;; async function helpfulPrompt(tp, key, tag, header) { let knownValues = await tp.user.frontmatterKeyValues(key, tag); knownValues.push(OTHER); let selectedValue = await tp.system.suggester(knownValues, knownValues, true); if (selectedValue === OTHER) { selectedValue = await tp.system.prompt(\u0026quot;Use which value?\u0026quot;, \u0026quot;\u0026quot;, true); } return selectedValue; } module.exports = helpfulPrompt; I also wrote this up (tagLeafFinder.js), but have not yet used it, for identifying stem values in keys:\nasync function tagLeafFinder(stemTag) { if (!stemTag.startsWith(\u0026quot;#\u0026quot;)) stemTag = `#${stemTag}`; if (!stemTag.endsWith(\u0026quot;/\u0026quot;)) stemTag = `${stemTag}/`; const tags = Object.keys( app.metadataCache.getTags() ).filter( t =\u0026gt; t.startsWith(stemTag) ); return tags; } module.exports = tagLeafFinder Now, whenever I start a project meeting, I hit Alt-n and choose the project from the (on-the-fly-generated) list, and I get my meeting notes. And, dataview query in my overview page tells me all the todo items assigned to me across all meeting notes.\n","date":1684861436,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684862689,"objectID":"3ec87a7b0cf2f7480e57b55e77000d6e","permalink":"https://matthewturk.github.io/post/project-notes-in-obsidian/","publishdate":"2023-05-23T12:03:56-05:00","relpermalink":"/post/project-notes-in-obsidian/","section":"post","summary":"A quick setup for project notes in Obsidian","tags":[],"title":"Project Notes in Obsidian","type":"post"},{"authors":[],"categories":[],"content":"I\u0026rsquo;ve been using Obsidian for almost two years now, and I\u0026rsquo;ve been finding it more and more integrated into my daily life. I take notes (using a variety of templates) in it, track my daily goals and some personal statistics, and I love the plugin ecosystem. I\u0026rsquo;ve also really enjoyed reading Eleanor Konik\u0026rsquo;s newsletter about it. Obsidian hasn\u0026rsquo;t really made me \u0026ldquo;10x better\u0026rdquo; or whatever, and I think there\u0026rsquo;s a lot in the \u0026ldquo;Personal Knowledge Management\u0026rdquo; space that just isn\u0026rsquo;t for me, but I have found it to be a very, very useful way to track what I\u0026rsquo;m doing.\nThe place where it wasn\u0026rsquo;t totally working for me was with tracking my runs, though. I\u0026rsquo;ve been running on and off for about fifteen years, but during the pandemic it really escalated; when I got injured back in February 2022, I was averaging about 50 miles a week, and I felt just amazing. (After about six months of rest and recuperation, I\u0026rsquo;m at a much more manageable and reasonable weekly mileage, but I have still set myself a goal of an ultra after tenure.) I really wanted to be able to control this information, though, and see it alongside my other daily info.\nSo I started looking at possibilities, and here\u0026rsquo;s what I\u0026rsquo;ve come up with. I spent a little time trying to figure out if I could build out a plugin, and while I\u0026rsquo;m oh-kay at typescript I\u0026rsquo;m not really that great at the \u0026ldquo;last 10%\u0026rdquo; problem, so instead I\u0026rsquo;m building it out of a couple other plugins. Here\u0026rsquo;s what I\u0026rsquo;m using:\nstrava-offline for downloading Strava data in both GPX and data record (via sqlite) forms Obsidian Execute Code for easily running my update scripts within my vault Obsidian Dataview for viewing the data in table form and querying embedding-page metadata Obsidian Leaflet for map views of the GPX files It sounds like a lot! But I promise, it\u0026rsquo;s not so bad.\nGetting the Data I use the Python package Strava Offline to download my data. It\u0026rsquo;s pip-installable, and has a very stable and straightforward command-line interface. The trickiest part is getting the _strava4_session cookie, but the website explains exactly how to do that.\nI use Obsidian Execute Code to run code snippets in my notes. I don\u0026rsquo;t do a lot in there, as I\u0026rsquo;m really quite happy to use Jupyterlab, but I have a tendency to put maintenance scripts and whatnot in my vault and run them with regularity. Often I keep Python stuff in here (and I\u0026rsquo;ve used py-obsidianmd in the past, but not here) but for this I use just straight up shell commands:\nstrava-offline sqlite strava-offline gpx --strava4-session MY_SESSION_COOKIE_HERE sqlite3 -header -csv ~/.local/share/strava_offline/strava.sqlite \\ \u0026quot;select id,name,start_date,moving_time,elapsed_time,distance,total_elevation_gain,type from activity\u0026quot; \\ \u0026gt; ~/Documents/Journal/projects/running/activities.csv cp -vn ~/.local/share/strava_offline/activities/*.gz \\ ~/Documents/Journal/projects/running/gpx-files/ This executes the strava-offline package (note that if you\u0026rsquo;re in a conda environment or something you\u0026rsquo;ll need to activate it) to update the sqlite file and get the new GPX files. Then it dumps from the sqlite file into a .csv file stored under my projects/running/ directory. Then I copy (with verbose mode on, and with no-clobber on, too) all the gpx.gz files to projects/running/gpx-files/. Note! You can use macros like @vault and whatnot in execute-code to make this nicer looking. I\u0026rsquo;d probably recommend that.\nSo now, the GPX files and CSV file are in your Vault! If you use sync, and you have it syncing non-MD files, they should be picked up just fine.\nDisplaying the Data I have two different views set up for my activities. I use Obsidian Dataview for lots and lots of queries and stuff, and it makes sense to use it here as well for an overview display. For detail, I use Obsidian Leaflet and embedded dataview views of the data.\nI should note that as of the writing of this blog entry, my pull request to dataview hasn\u0026rsquo;t been accepted, but it looks like it will be. This enables the CSV parser to regard things as dates \u0026ndash; otherwise the dates just show up as fp_incr. I also want to thank Jeremy Valentine for helping me out to get the .gpx parser in obsidian-leaflet to handle .gz-compressed files, as the file size really does make a big different for GPX files.\nAnyway! I have a top-level file called Running Activities.md which includes this dataview query:\ntable without id id, start_date, name, moving_time, elapsed_time, type, total_elevation_gain from csv(\u0026quot;activities.csv\u0026quot;) sort id desc This is pretty basic, and would probably work a bit better if I were more careful about it. But, it\u0026rsquo;s a good start.\nFor individual runs, my setup is:\nA simple template for every run, and each run gets its own note. An embedded dataview \u0026ldquo;view\u0026rdquo; that generates the map for each template. Links to the daily notes from each of the runs, and daily notes have queries for backlinks. My run \u0026ldquo;view\u0026rdquo; template looks like this, and I call it run-activity.md:\n## `$= dv.current().name` ```leaflet id: ACTIVITY_NAME zoomFeatures: true maxZoom: 20 gpx: - [[gpx-files/ACTIVITY_ID.gpx.gz]] This is the view that is embedded; in the main run.md template, which is instantiated for each run, I have:\n--- date: a activityId: Whatever aliases: [] tags: - running - running/route - running/strava --- ## Run \\```dataviewjs dv.paragraph((await dv.io.load(\u0026quot;templates/run-activity.md\u0026quot;)).replace(\u0026quot;ACTIVITY_ID\u0026quot;, dv.current().activityId)); \\``` ## Notes But, as I\u0026rsquo;ll note below, there are some issues with this! I want to make it easier to do.\nFuture Ideas In reality, there is a pretty straightforward way of making this much more streamlined. The Obsidian plugin template is really nice, the Strava Javascript API is really nice, and I think it could be all brought in-house to a single plugin. I just haven\u0026rsquo;t figured out the best way to do that, or to convince myself to do it.\nThe other thing is that the actual creation of the instances of run.md is a little clunky. I want to be able to instantiate templates in Templater from within the Obsidian API, but I can\u0026rsquo;t see how to do that exactly. It would be nice to be able to supply variables, a template input filename, and an output filename to a function, but it isn\u0026rsquo;t quite available as-is. I think there might be a way to make QuickAdd do it, but I haven\u0026rsquo;t figured it out quite yet. This remains the most irritating part \u0026ndash; my template will prompt me for the activity id, but that\u0026rsquo;s really not the most efficient way to do it. I\u0026rsquo;d instead much rather be able to parse the CSV file on demand (maybe with a button) and any new entries get a new instance of the template.\nSo that\u0026rsquo;s that! I think that this can be smoother, and it probably will be pretty soon, but here\u0026rsquo;s how I\u0026rsquo;m doing it right now, rough edges and all. Drop me a line if you have any suggestions or questions!\n","date":1682370784,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682373961,"objectID":"3e9987511b9c82d514e05a707f9d09f1","permalink":"https://matthewturk.github.io/post/strava-in-obsidian/","publishdate":"2023-04-24T16:13:04-05:00","relpermalink":"/post/strava-in-obsidian/","section":"post","summary":"How I import my Strava runs into Obsidian","tags":[],"title":"Strava in Obsidian","type":"post"},{"authors":null,"categories":null,"content":"Advanced data visualization topics, with an increased focus on connecting different platforms.\n","date":1641013200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649871598,"objectID":"4f745f68e2d3a4a5700029532d698d0e","permalink":"https://matthewturk.github.io/courses/is545-spr2022/","publishdate":"2022-01-01T00:00:00-05:00","relpermalink":"/courses/is545-spr2022/","section":"courses","summary":"Advanced Data Viz from Spring 2022","tags":[],"title":"IS545 - Spring 2022","type":"courses"},{"authors":[],"categories":null,"content":"","date":1635224400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649870017,"objectID":"c9a5cb5d4bf56d30bff63925eacbe4d0","permalink":"https://matthewturk.github.io/talk/2021-10-26-ieeevis-satellite-yt/","publishdate":"2022-04-13T09:10:44-05:00","relpermalink":"/talk/2021-10-26-ieeevis-satellite-yt/","section":"talk","summary":"","tags":[],"title":"yt: for volumetric analysis and visualization","type":"talk"},{"authors":[],"categories":null,"content":"","date":1634187600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649870017,"objectID":"91322f610166bbc94dad3ddadc2caecd","permalink":"https://matthewturk.github.io/talk/2021-10-14-cross-data-across-boundaries/","publishdate":"2022-04-13T09:15:27-05:00","relpermalink":"/talk/2021-10-14-cross-data-across-boundaries/","section":"talk","summary":"Analyzing complex, multi-source, multi-format and multi-modal data from astrophysical simulations, observations and theory requires methods for transforming raw numbers into manipulable quantities, and the application of high-level semantic models on top of those quantities. In this talk I will present methods for defining and applying a grammar of analysis to volumetric astrophysical data, and describe the implications this has for visualization, analysis and inference in astrophysics.","tags":[],"title":"Data Storytelling and the Grammar of Analysis","type":"talk"},{"authors":null,"categories":null,"content":"This semester built on the previous round, but added more detail on vega-lite and building dashboards with vega-lite.\n","date":1627794e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649871598,"objectID":"4dea327e4fe17622a447134f73182af3","permalink":"https://matthewturk.github.io/courses/is445-fall2021/","publishdate":"2021-08-01T00:00:00-05:00","relpermalink":"/courses/is445-fall2021/","section":"courses","summary":"Data Viz from Fall 2021","tags":[],"title":"IS445 - Fall 2021","type":"courses"},{"authors":[],"categories":null,"content":"","date":1623042e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649870017,"objectID":"46be3f3be7638dd10fe9b1d210333a81","permalink":"https://matthewturk.github.io/talk/2021-06-07-aas-yt-viz/","publishdate":"2022-04-13T09:15:14-05:00","relpermalink":"/talk/2021-06-07-aas-yt-viz/","section":"talk","summary":"","tags":[],"title":"Visualization with yt","type":"talk"},{"authors":[],"categories":null,"content":"","date":1622005200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649870017,"objectID":"321841fcffcaa54a3ebe9c896ad81874","permalink":"https://matthewturk.github.io/talk/2021-05-26-ivoa-yt/","publishdate":"2022-04-13T09:10:32-05:00","relpermalink":"/talk/2021-05-26-ivoa-yt/","section":"talk","summary":"In this talk, we will present work to make accessible large-scale (and small-scale) simulations through the usage of web technologies, APIs, access to python libraries, Jupyter, and webassembly. We’ll describe our utilization of tools such as yt as well as implementations of tools on top of this, such as widgyts and ytree.","tags":[],"title":"Cosmological Simulation Infrastructure","type":"talk"},{"authors":null,"categories":null,"content":"This version of Advanced Data Viz included more server-side rendering and a deeper exploration of prototyping of visualizations.\n","date":1609477200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649871598,"objectID":"124ee270500a9a3a02e22fa4bdeaceae","permalink":"https://matthewturk.github.io/courses/is545-spr2021/","publishdate":"2021-01-01T00:00:00-05:00","relpermalink":"/courses/is545-spr2021/","section":"courses","summary":"Advanced Data Viz from Spring 2021","tags":[],"title":"IS545 - Spring 2021","type":"courses"},{"authors":[],"categories":null,"content":"","date":160749e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649870017,"objectID":"ddf46a576a8c322c541cfec2a86bd79d","permalink":"https://matthewturk.github.io/talk/2020-12-09-rhythm-yt-visualization/","publishdate":"2022-04-13T09:10:24-05:00","relpermalink":"/talk/2020-12-09-rhythm-yt-visualization/","section":"talk","summary":"","tags":[],"title":"state of viz in yt","type":"talk"},{"authors":[],"categories":null,"content":"","date":1603735200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649870017,"objectID":"d2ccb1127befc50b4ba8f447e517aeb0","permalink":"https://matthewturk.github.io/talk/2020-10-26-visastro-grammar-of-analysis/","publishdate":"2022-04-13T09:10:07-05:00","relpermalink":"/talk/2020-10-26-visastro-grammar-of-analysis/","section":"talk","summary":"Analyzing complex, multi-source, multi-format and multi-modal data from astrophysical simulations, observations and theory requires methods for transforming raw numbers into manipulable quantities, and the application of high-level semantic models on top of those quantities. In this talk I will present methods for defining and applying a grammar of analysis to volumetric astrophysical data, and describe the implications this has for visualization, analysis and inference in astrophysics.","tags":[],"title":"A Grammar of Analysis for Volumetric Astrophysical Data","type":"talk"},{"authors":null,"categories":null,"content":"This semester included a redesigned coursework page, as well as more detail on javascript, frameworks, and jupyter widgets.\n","date":1596258e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649871598,"objectID":"53979f337ba76645a7dd5668deb772db","permalink":"https://matthewturk.github.io/courses/is445-fall2020/","publishdate":"2020-08-01T00:00:00-05:00","relpermalink":"/courses/is445-fall2020/","section":"courses","summary":"Data Viz from Fall 2020","tags":[],"title":"IS445 - Fall 2020","type":"courses"},{"authors":["Matthew Turk"],"categories":null,"content":"","date":1587760409,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649870017,"objectID":"fa2a7c8f2c9ce8b754c74693444d3cfe","permalink":"https://matthewturk.github.io/talk/2020-04-24-ai-for-sustainability/","publishdate":"2020-04-24T15:33:29-05:00","relpermalink":"/talk/2020-04-24-ai-for-sustainability/","section":"talk","summary":"This talk explores some ideas about how to interpret volumetric data in an AI-focused way.","tags":[],"title":"Interpretation, Grammar and Visualization","type":"talk"},{"authors":[],"categories":[],"content":"(Confession time: I did not even realize I was mildly-punning in that title.)\nOver the last few months, my hobby-time has mostly been focused on some reverse engineering of old DOS games I played as a kid. The fruits of that will wait for another blog post, but this has led to some very fun investigations using the DOSbox debugger, Ghidra and learning the Ghidra API to decompile MS-DOS system calls (especially int 21h) properly.\nA particularly fun aspect of doing all this, though, has been realizing just how much this has in common with the early stages of developing yt, when we had to reverse engineer file formats like it was going out of style.\nOne of the things that constantly constantly showed up in reverse engineering these formats was making sure our endianness was right. Since we were often dealing with 32-bit values that had uncertain units, and where the values themselves could be quite the extreme and not too far from the limits of representation, it wasn\u0026rsquo;t always immediately obvious if we had it right or not.\nTo be completely honest, the thing that always threw me (and sometimes still does, when I\u0026rsquo;m thinking about ordering of numbers on the stack and in registers) is how numbers get represented as bits in memory, and more specifically, what the difference between \u0026ldquo;big\u0026rdquo; and \u0026ldquo;little\u0026rdquo; endian really is.\nAlso, for a much better description and walkthrough, please feel free to visit the wikipedia article. Its load time is probably comparable to the reading time of this rather brief foray into views.\nimport numpy as np To get started, let\u0026rsquo;s make an array that isn\u0026rsquo;t symmetric at the \u0026ldquo;word\u0026rdquo; level. I\u0026rsquo;ll use my favorite value offset by one to do this. (Remember for later that we\u0026rsquo;re using a number as our base that is greater than can be represented in two bytes, so it truncates at u2 size.)\nWe\u0026rsquo;ll use numpy\u0026rsquo;s ability to encode arrays using different internal representations \u0026ndash; the first array will be in \u0026ldquo;little\u0026rdquo; endian, 2-byte unsigned integers, and the second will be in \u0026ldquo;big\u0026rdquo; endian, 2-byte integers. We toggle between the two with the \u0026lt; and \u0026gt; operators before the data type specification.\narr_little = np.array([0x4d3d3 - 1]).astype(\u0026quot;\u0026lt;u2\u0026quot;) arr_big = arr_little.astype(\u0026quot;\u0026gt;u2\u0026quot;) arr_big, arr_little (array([54226], dtype=uint16), array([54226], dtype=uint16)) So, they\u0026rsquo;re definitely the same values when we print them out! But that\u0026rsquo;s because even though they are encoded differently internally, when we print them, we use a representation that is encoding aware. What happens if we force the issue, though? Let\u0026rsquo;s try, by asking to view them in \u0026ldquo;unsigned bytes\u0026rdquo; format by using the view method with the argument B to indicate the dtype.\nprint(\u0026quot;Little endian: {}\\nBig endian: {}\u0026quot;.format(arr_little.view(\u0026quot;B\u0026quot;), arr_big.view(\u0026quot;B\u0026quot;))) Little endian: [210 211] Big endian: [211 210] Interesting! Note that the individual bytes are the same (and there are two in each one), but the order is different.\nWe can make this even more clear if we look at the binary and hex representations of them:\nprint(\u0026quot;Little endian: {}\\nBig endian: {}\u0026quot;.format(*[[bin(_)[2:] for _ in arr.view(\u0026quot;B\u0026quot;)] for arr in (arr_little, arr_big)])) Little endian: ['11010010', '11010011'] Big endian: ['11010011', '11010010'] print(\u0026quot;Little endian: {}\\nBig endian: {}\u0026quot;.format(*[[hex(_)[2:].upper() for _ in arr.view(\u0026quot;B\u0026quot;)] for arr in (arr_little, arr_big)])) Little endian: ['D2', 'D3'] Big endian: ['D3', 'D2'] What I always found the most difficult about endianness is that the bits aren\u0026rsquo;t actually reversed \u0026ndash; what is reversed is the order of the bytes, but within a given byte the bits remain the same. This gets even more fun with 32 and 64 bit integers.\narr_little32 = np.array([0x4d3d3 - 1]).astype(\u0026quot;\u0026lt;u4\u0026quot;) arr_big32 = arr_little32.astype(\u0026quot;\u0026gt;u4\u0026quot;) arr_big32, arr_little32 (array([316370], dtype=uint32), array([316370], dtype=uint32)) print(\u0026quot;Little endian: {}\\nBig endian: {}\u0026quot;.format(*[[hex(_)[2:].upper() for _ in arr.view(\u0026quot;B\u0026quot;)] for arr in (arr_little32, arr_big32)])) Little endian: ['D2', 'D3', '4', '0'] Big endian: ['0', '4', 'D3', 'D2'] Of course, Wikipedia has an awesome article with a great illustration that demonstrates all of this.\n","date":1579818495,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1579818658,"objectID":"a04e1539e6581c6221bc4521a12004d8","permalink":"https://matthewturk.github.io/post/a-little-bit-of-endianness/","publishdate":"2020-01-23T16:28:15-06:00","relpermalink":"/post/a-little-bit-of-endianness/","section":"post","summary":"(Confession time: I did not even realize I was mildly-punning in that title.)\nOver the last few months, my hobby-time has mostly been focused on some reverse engineering of old DOS games I played as a kid.","tags":[],"title":"A Little Bit of Endianness","type":"post"},{"authors":["Matthew Turk"],"categories":null,"content":"","date":1575923609,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576014432,"objectID":"6293b1f5b627d3b87d7db60ca23205ec","permalink":"https://matthewturk.github.io/talk/2019-12-09-qmc-hamm-research-overview/","publishdate":"2019-12-09T15:33:29-05:00","relpermalink":"/talk/2019-12-09-qmc-hamm-research-overview/","section":"talk","summary":"Brief overview of my interest in the QMC-HAMM project.","tags":[],"title":"QMC-HAMM: Quick Research Overview","type":"talk"},{"authors":["Matthew Turk","Sam Walkow"],"categories":null,"content":"","date":1572467609,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572554291,"objectID":"10104dcdba5685925d881dac0aeb0ce8","permalink":"https://matthewturk.github.io/talk/2019-10-30-ischool-volume-grammar/","publishdate":"2019-10-31T15:33:29-05:00","relpermalink":"/talk/2019-10-30-ischool-volume-grammar/","section":"talk","summary":"Sam and I talked about our work building a grammar for volumetric analysis.  It features some D3js diagrams, which I think I will probably expand on and clean up for future talks.","tags":[],"title":"A Grammar of Volumetric Analysis","type":"talk"},{"authors":null,"categories":null,"content":"This course continued to include javascript and bqplot, but started touching on d3js as well.\n","date":1564635600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649871598,"objectID":"72ca1773dd53db14b7e7dc3d211c020c","permalink":"https://matthewturk.github.io/courses/is590dv-fall2019/","publishdate":"2019-08-01T00:00:00-05:00","relpermalink":"/courses/is590dv-fall2019/","section":"courses","summary":"Data Viz from Fall 2019","tags":[],"title":"IS590DV - Fall 2019","type":"courses"},{"authors":[],"categories":[],"content":"I think I\u0026rsquo;ve talked myself into proposing a big change in yt. I\u0026rsquo;m not the \u0026ldquo;boss\u0026rdquo; of yt, so it might not happen, but I\u0026rsquo;ve kind of worked up my courage to make a serious suggestion.\nThis last week I have been at SciPy 2019 and I had the opportunity to see a lot of talks.\nThere were a few that really stuck with me, but for the purposes of this rather technically-focused blog post, I\u0026rsquo;m going to stick to just one in particular.\nMatt Rocklin gave a talk about refactoring the ecosystem to prepare for heterogeneous computing (you should go watch it!). More specifically, though, what it seemed to me was that it was a talk more about an opportunity to avoid fragmentation and think more carefully about how arrays and APIs are thought of and used. That got me thinking about something I\u0026rsquo;ve kind of touched on in previous posts ( here, here and here) \u0026ndash; basically, that yt is pretty monolithic, and that\u0026rsquo;s not really the best way to evolve with the ecosystem.\nI\u0026rsquo;ll be using findimports for exploring how monolithic it is versus how monolithic it appears to be. Basically, I want to see: is it one repo with lots of interconnections, or is it essentially a couple repos?\n(Also at the end I\u0026rsquo;ll give a pitch for why this is relevant, so if you\u0026rsquo;re even remotely intrigued, at least scroll down to the section labeled \u0026ldquo;OK, the boring stuff is over.\u0026rdquo;)\nimport pickle import findimports yt_imports = pickle.load(open(\u0026quot;yt/yt/import_output.pkl\u0026quot;, \u0026quot;rb\u0026quot;)) The structure of this is a set of keys that are strings of the filename/modulename, with values that are the objects in question. The findimports objects have an attribute imports which is what we\u0026rsquo;re going to look at first, but they also have an imported_names attribute which is the list of names that get imported, in the form of ImportInfo objects. These have name, filename, level and lineno to show where and what they are.\nyt_imports['yt.visualization.plot_window'].imports {'collections', 'distutils.version', 'matplotlib', 'matplotlib.mathtext', 'matplotlib.ticker', 'numbers', 'numpy', 'pyparsing', 'sys', 'types', 'unyt.exceptions', 'yt', 'yt.data_objects.image_array', 'yt.frontends.ytdata.data_structures', 'yt.funcs', 'yt.units.unit_object', 'yt.units.unit_registry', 'yt.units.yt_array', 'yt.utilities.exceptions', 'yt.utilities.math_utils', 'yt.utilities.orientation', 'yt.visualization.base_plot_types', 'yt.visualization.fixed_resolution', 'yt.visualization.geo_plot_utils', 'yt.visualization.plot_container', 'yt.visualization.plot_modifications'} There happen to be a fair number of things in here that are external to yt! So, let\u0026rsquo;s set up a filtering process for those. We\u0026rsquo;ll filter the name that is imported.\nOne thing I should note is that yt does many, but not all, of its imports in absolute form, which maybe isn\u0026rsquo;t \u0026hellip; so great \u0026hellip; but which lets us do this more easily.\nfilter_imports = lambda a: [_ for _ in sorted(a, key=lambda b: b.name) if _.name.startswith(\u0026quot;yt.\u0026quot;)] We\u0026rsquo;ll apply it to the imported_names attribute, since we\u0026rsquo;re interested in characterizing how things are related and interweaved.\nimport_lists = {_ : filter_imports(yt_imports[_].imported_names) for _ in yt_imports} import_lists['yt.visualization.plot_window'] [ImportInfo('yt.data_objects.image_array.ImageArray', 'yt/visualization/plot_window.py', 40, 0), ImportInfo('yt.frontends.ytdata.data_structures.YTSpatialPlotDataset', 'yt/visualization/plot_window.py', 42, 0), ImportInfo('yt.funcs.ensure_list', 'yt/visualization/plot_window.py', 44, 0), ImportInfo('yt.funcs.fix_axis', 'yt/visualization/plot_window.py', 45, 0), ImportInfo('yt.funcs.fix_unitary', 'yt/visualization/plot_window.py', 45, 0), ImportInfo('yt.funcs.iterable', 'yt/visualization/plot_window.py', 44, 0), ImportInfo('yt.funcs.mylog', 'yt/visualization/plot_window.py', 44, 0), ImportInfo('yt.funcs.obj_length', 'yt/visualization/plot_window.py', 45, 0), ImportInfo('yt.load', 'yt/visualization/plot_window.py', 737, 0), ImportInfo('yt.load', 'yt/visualization/plot_window.py', 1373, 0), ImportInfo('yt.load', 'yt/visualization/plot_window.py', 1557, 0), ImportInfo('yt.load', 'yt/visualization/plot_window.py', 2067, 0), ImportInfo('yt.units.unit_object.Unit', 'yt/visualization/plot_window.py', 47, 0), ImportInfo('yt.units.unit_registry.UnitParseError', 'yt/visualization/plot_window.py', 49, 0), ImportInfo('yt.units.yt_array.YTArray', 'yt/visualization/plot_window.py', 51, 0), ImportInfo('yt.units.yt_array.YTQuantity', 'yt/visualization/plot_window.py', 51, 0), ImportInfo('yt.utilities.exceptions.YTCannotParseUnitDisplayName', 'yt/visualization/plot_window.py', 57, 0), ImportInfo('yt.utilities.exceptions.YTDataTypeUnsupported', 'yt/visualization/plot_window.py', 59, 0), ImportInfo('yt.utilities.exceptions.YTInvalidFieldType', 'yt/visualization/plot_window.py', 60, 0), ImportInfo('yt.utilities.exceptions.YTPlotCallbackError', 'yt/visualization/plot_window.py', 58, 0), ImportInfo('yt.utilities.exceptions.YTUnitNotRecognized', 'yt/visualization/plot_window.py', 61, 0), ImportInfo('yt.utilities.math_utils.ortho_find', 'yt/visualization/plot_window.py', 53, 0), ImportInfo('yt.utilities.orientation.Orientation', 'yt/visualization/plot_window.py', 55, 0)] This still isn\u0026rsquo;t incredibly useful, since we kind of want to look at imports at a higher level. For instance, I want to know what yt.visualization.plot_window imports from in the broad cross-section of the code base. So let\u0026rsquo;s write something to collapse the package under yt that we import from. We used startswith(\u0026quot;.yt\u0026quot;) earlier, so it\u0026rsquo;ll be safe to do a split here.\ncollapse_subpackage = lambda a: set(_.name.split(\u0026quot;.\u0026quot;)[1] for _ in a) collapse_subpackage(import_lists['yt.visualization.plot_window']) {'data_objects', 'frontends', 'funcs', 'load', 'units', 'utilities'} Interesting. We import from frontends?! I guess I kind of missed that earlier. Let\u0026rsquo;s see if we can figure out the connections between different modules to see if anything stands out.\nfrom collections import defaultdict subpackage_imports = defaultdict(set) for fn, v in import_lists.items(): if not fn.startswith(\u0026quot;yt.\u0026quot;): continue # Get rid of our tests, etc. subpackage = fn.split(\u0026quot;.\u0026quot;)[1] subpackage_imports[subpackage].update(collapse_subpackage(v)) Let\u0026rsquo;s break this down before we go any further \u0026ndash; for starters, not everything is an absolute import. So that makes things a bit tricky! But we can deal with that later. Let\u0026rsquo;s first see what all we have:\nsubpackage_imports.keys() dict_keys(['__init__', 'api', 'arraytypes', 'config', 'convenience', 'exthook', 'funcs', 'mods', 'pmods', 'startup_tasks', 'testing', 'analysis_modules', 'data_objects', 'extensions', 'extern', 'fields', 'frontends', 'geometry', 'tests', 'units', 'utilities', 'visualization']) A few things stand out right away. Some of these we can immediately get rid of and not consider. For instance, pmods is an MPI-aware importer, mods is a pretty old-school approach to yt importing, and we will just ignore testing, analysis_modules, extensions and extern since they\u0026rsquo;re (in order) testing utilities, gone, a fake hook system, and \u0026ldquo;vendored\u0026rdquo; libraries that we should probably get rid of and just make requirements anyway. units is now part of unyt and some of the others are by-design grabbing lots of stuff.\nblacklist = [\u0026quot;testing\u0026quot;, \u0026quot;analysis_modules\u0026quot;, \u0026quot;extensions\u0026quot;, \u0026quot;extern\u0026quot;, \u0026quot;pmods\u0026quot;, \u0026quot;mods\u0026quot;, \u0026quot;__init__\u0026quot;, \u0026quot;api\u0026quot;, \u0026quot;arraytypes\u0026quot;, \u0026quot;config\u0026quot;, \u0026quot;convenience\u0026quot;, \u0026quot;exthook\u0026quot;, \u0026quot;funcs\u0026quot;, \u0026quot;tests\u0026quot;, \u0026quot;units\u0026quot;, \u0026quot;startup_tasks\u0026quot;] list(subpackage_imports.pop(_, None) for _ in blacklist); We just want to see the interrelationships, so we\u0026rsquo;ll look for N-by-N collisions, where N is just the values that show up as keys.\ncollide_with = set(subpackage_imports.keys()) collisions = {_: collide_with.intersection(subpackage_imports[_]) for _ in subpackage_imports} And here we have it, the moment of truth! What do we see \u0026hellip;\nprint({_:len(__) for _, __ in collisions.items()}) {'data_objects': 6, 'fields': 5, 'frontends': 6, 'geometry': 5, 'utilities': 6, 'visualization': 6} Huh. Well, that was not the dramatic, amazing reveal I\u0026rsquo;d hoped for.\nsubpackage_imports = defaultdict(set) for fn, v in import_lists.items(): if not fn.startswith(\u0026quot;yt.\u0026quot;) or \u0026quot;tests\u0026quot; in fn: continue # Get rid of our tests, etc. subpackage = fn.split(\u0026quot;.\u0026quot;)[1] subpackage_imports[subpackage].update(collapse_subpackage(v)) list(subpackage_imports.pop(_, None) for _ in blacklist); collisions = {_: collide_with.intersection(subpackage_imports[_]) for _ in subpackage_imports} print({_:len(__) for _, __ in collisions.items()}) {'data_objects': 6, 'fields': 4, 'frontends': 6, 'geometry': 4, 'utilities': 5, 'visualization': 6} It gets a little bit better, but honestly, not much. Our most isolated package \u0026ndash; by this (likely flawed) method \u0026ndash; are the geometry and fields packages. So let\u0026rsquo;s break down a bit more what we\u0026rsquo;re seeing, by not filtering quite as much, and by setting up a reverse mapping. And let\u0026rsquo;s do it for both the collapsed name and the non-collapsed name.\nsubpackage_imports = defaultdict(set) imported_by = defaultdict(list) for fn, v in import_lists.items(): if not fn.startswith(\u0026quot;yt.\u0026quot;) or \u0026quot;tests\u0026quot; in fn: continue # Get rid of our tests, etc. subpackage = fn.split(\u0026quot;.\u0026quot;)[1] subpackage_imports[subpackage].update(set(_.name for _ in v)) [imported_by[_.name].append(fn) for _ in v] [imported_by[_].append(fn) for _ in collapse_subpackage(v)] And now we might be getting somewhere. So now we can look up for any given import which files have imported it. Let\u0026rsquo;s see what imports the progress bar:\nimported_by[\u0026quot;yt.funcs.get_pbar\u0026quot;] ['yt.__init__', 'yt.data_objects.particle_trajectories', 'yt.data_objects.level_sets.contour_finder', 'yt.frontends.athena_pp.data_structures', 'yt.frontends.enzo.data_structures', 'yt.frontends.enzo_p.data_structures', 'yt.geometry.particle_geometry_handler', 'yt.utilities.minimal_representation', 'yt.utilities.particle_generator', 'yt.utilities.answer_testing.framework', 'yt.visualization.streamlines', 'yt.visualization.volume_rendering.old_camera'] Nice. Now, let\u0026rsquo;s look at visualization.\nimported_by[\u0026quot;yt.visualization.api.SlicePlot\u0026quot;], imported_by[\u0026quot;yt.visualization.plot_window.SlicePlot\u0026quot;] (['yt.__init__', 'yt.data_objects.analyzer_objects'], ['yt.utilities.command_line']) We\u0026rsquo;re starting to see that things might not be quite as clear-cut as we thought. Let\u0026rsquo;s look at geometry. And I\u0026rsquo;m going to set up a filtering method so that we can avoid lots of redundant pieces of info \u0026ndash; for instance, I don\u0026rsquo;t care about things importing themselves.\nfilter_self_imports = lambda a: [_ for _ in imported_by[a] if not _.startswith(\u0026quot;yt.{}\u0026quot;.format(a))] We\u0026rsquo;ll only look at the first ten, because it\u0026rsquo;s really long\u0026hellip;\nfilter_self_imports(\u0026quot;geometry\u0026quot;)[:10] ['yt.data_objects.construction_data_containers', 'yt.data_objects.data_containers', 'yt.data_objects.grid_patch', 'yt.data_objects.octree_subset', 'yt.data_objects.selection_data_containers', 'yt.data_objects.static_output', 'yt.data_objects.unstructured_mesh', 'yt.frontends._skeleton.data_structures', 'yt.frontends.ahf.data_structures', 'yt.frontends.art.data_structures'] Here things are much clearer. We import geometry once in the visualization subsystem, under plot_modifications. I looked it up, and here\u0026rsquo;s what it is:\nif not issubclass(type(index), UnstructuredIndex): raise RuntimeError(\u0026quot;Mesh line annotations only work for \u0026quot; \u0026quot;unstructured or semi-structured mesh data.\u0026quot;) This is probably an anti-pattern, but even if we wanted to retain this specific behavior, we could remedy it without too much trouble by having an attribute check, or some kind of string-key check.\nAs for all the frontends imports, those are all because they subclass Index! And many of the places importing it in data_objects are just due to a lack of organization in the geometry/utilities/indexing code.\nHistorical Sidenote: As I was doing this, I read the header for grid_patch.py and it reads: \u0026quot;Python-based grid handler, not to be confused with the SWIG-handler\u0026quot;. I am reasonably certain that it has been years since I thought about the proto-SWIG system I\u0026rsquo;d written to wrap around the Enzo C++ code. Kinda supports the point I intend to make when I end this post, I think.\nBack to the task at hand, let\u0026rsquo;s look at some of the other top-level packages and how they related. I\u0026rsquo;m now specifically interested in the visualization and data_objects ones.\nfilter_self_imports(\u0026quot;visualization\u0026quot;) ['yt.__init__', 'yt.data_objects.analyzer_objects', 'yt.data_objects.construction_data_containers', 'yt.data_objects.data_containers', 'yt.data_objects.image_array', 'yt.data_objects.profiles', 'yt.data_objects.region_expression', 'yt.data_objects.selection_data_containers', 'yt.frontends.fits.misc', 'yt.utilities.command_line', 'yt.utilities.fits_image', 'yt.utilities.answer_testing.framework'] Well, that\u0026rsquo;s interesting. A quick skim of the code suggests that analyzer_objects is probably dead code to be removed, construction_data_containers imports viz so that .plot() will work on projections, and there are a handful of other data-object-to-viz things that get done here.\nIn short, I\u0026rsquo;m pretty sure that visualization is a mostly independent subpackage. And the same kind of goes for geometry. The story isn\u0026rsquo;t quite as clear for the others.\nOK, the boring stuff is over. Here\u0026rsquo;s where I wanted to get this whole time: yt is a monolithic package in packaging, but it also has a couple reasonably independent sub-packages within it. It would be a target for breaking up, if those subpackages were independently useful. But as it stands, they probably aren\u0026rsquo;t, since they\u0026rsquo;re all very tightly coupled.\nThey\u0026rsquo;re coupled because they were written that way, without too much thought given to a public, externally-useful API. This is something that\u0026rsquo;s probably not surprising, since yt was a big package that evolved, rather than many packages that interoperated. Lots of stuff we wanted didn\u0026rsquo;t exist, and there was (we thought) really only one obvious way to do things, so why not just do it?\n(Historical sidenote: It was a few years ago that I remember asking at a panel what middleware developers, like I saw myself, were supposed to do in a rapidly evolving ecosystem of the array container. (I probably didn\u0026rsquo;t phrase it very well.) What I took away was: stick with numpy. And we did stick with numpy, and more importantly, we stuck with the Cython interface to numpy.)\nOK, no more rambling, get to the point yt did a lot of stuff on its own. It doesn\u0026rsquo;t need to anymore. It\u0026rsquo;s effectively a medium-ly coupled system, but one that has relied on the ability to change non-public APIs all the time.\nI\u0026rsquo;m starting to convince myself it\u0026rsquo;s time to mature as a project, and to do some combination of jettisoning and liberating things in yt. I\u0026rsquo;ll make it clear that I could be wrong on this, and seeing this through will probably take more thought, time and energy than I can reasonably personally commit, but I think I\u0026rsquo;ve started to see what would be a productive path forward.\nSteps to Integrate Better Here are some things that I think yt could do. These are my thoughts, which I have not presented to the steering council, written up in a YTEP, or even made any efforts toward. In fact, the one member of the steering committee to whom I said, \u0026ldquo;I think I might blog about this\u0026rdquo; explicitly suggested I not do that thing (blog)! But here I am a couple pages deep and I\u0026rsquo;ve always been a bit of a hoarder.\nInventory The first step would be to take a real inventory of what is in yt, and what ought to be in yt. I have some thoughts on things that could be gotten rid of (and I will happily note that I intend to \u0026ldquo;kill my own darlings\u0026rdquo; first, before anyone else\u0026rsquo;s) but that can come later.\nNot everything yt does needs to be done by yt. But before we can figure that out, let\u0026rsquo;s figure out what yt is doing, and where it gets relied on.\nIndexing The thing I think we absolutely need to think about is how tightly coupled the indexing system is with everything else. This will almost certainly be a full blog post at a later time, but the way the indexing is coupled so tightly with the frontends makes me increasingly leery.\nI personally think that the way the grid indexing is set up is too fixed on a pre-parsing step and a finalization step, with lots of little bits in between that need to be handled, and that it could much more easily be set up with a different procedure.\nFor the particle frontends, the particle bitmap indexing has the filenames and the particle types and the coordinates and all of that all wound together. This should be decoupled, so that the notion of the particle locations is held at a higher level than the bitmap indexing.\nConsider Splitting the Package This one \u0026hellip; I want to emphasize that what I think we should do is the process of considering it. I\u0026rsquo;m not sure that it should be split. But I think that evaluating the pros and cons will lead us to think about how our packages interact.\nIf two objects call weird methods on each other, why? Does that need to happen? Is it a micro-optimization that makes no sense other than obfuscation? I\u0026rsquo;m not sure, but we can\u0026rsquo;t figure it out unless we examine.\nBig Picture yt may stay a big repository, but if we reduce the dependence on artisanal objects (why not just subclass xarray.Dataset instead of have a GridPatch object? Well, okay, that one is pretty complicated, but we can talk about it later\u0026hellip;) and we think about the specifics of why we use public APIs versus private APIs, it can lead to a more robust, lightweight package.\nAnd anything that keeps us more lightweight, reduces maintenance burdens, and enables us to take advantage of the astounding advances in the ecosystem is probably going to be better in the long-run.\n","date":1563305316,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563305512,"objectID":"3824d78c8c6b6713387e25eb49fe4add","permalink":"https://matthewturk.github.io/post/yt-internal-and-external-ecosystems/","publishdate":"2019-07-16T14:28:36-05:00","relpermalink":"/post/yt-internal-and-external-ecosystems/","section":"post","summary":"I think I\u0026rsquo;ve talked myself into proposing a big change in yt. I\u0026rsquo;m not the \u0026ldquo;boss\u0026rdquo; of yt, so it might not happen, but I\u0026rsquo;ve kind of worked up my courage to make a serious suggestion.","tags":[],"title":"yt: Internal and External Ecosystems","type":"post"},{"authors":[],"categories":null,"content":"","date":1562871758,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563046685,"objectID":"9b149762383f665c6f591297f2b857f5","permalink":"https://matthewturk.github.io/talk/2019-07-11-getting-lost-in-community-building/","publishdate":"2019-07-11T14:02:38-05:00","relpermalink":"/talk/2019-07-11-getting-lost-in-community-building/","section":"talk","summary":"It's really easy to get lost in community building.","tags":[],"title":"Getting Lost in Community Building","type":"talk"},{"authors":[],"categories":[],"content":"For all the data formats that yt knows about, reading a dataset is supposed to be trivial \u0026ndash; the yt.load command knows how to parse the metadata of something like 30-50 different dialects of data, generate the in-memory representation, and even manage things like the coordinate system (Spherical? Cartesian? etc) and how to apply spatial transformations.\nBut, if you\u0026rsquo;re just experimenting or trying to get a new frontend going, right now it\u0026rsquo;s too hard to load data into yt. (Before you think I\u0026rsquo;m passing the buck, though: this is kind of my fault.)\nLoading some spherical data Last week, I worked with some folks to load in data from a simulation code called CHIMERA. The data specifically was a 2D axially symmetric dataset, with log-spaced bins in the radial direction. The file format was really quite straightforward, too!\nUsually when developing a frontend to make all of this easier, the first step is to load the data into yt using the Stream frontend. This has functions like load_amr_grids, load_uniform_grid, load_unstructured_mesh, load_particles and so on. But, it was pretty annoying to load the data into yt. And it seems like it should be a lot simpler.\nLet\u0026rsquo;s dig in, following along as I had explored it.\nimport h5py import yt import numpy as np f = h5py.File(\u0026quot;chimera_000661800_grid_1_01.h5\u0026quot;, \u0026quot;r\u0026quot;) f.keys() \u0026lt;KeysViewHDF5 ['abundance', 'analysis', 'fluid', 'mesh', 'metadata', 'radiation', 'tracking']\u0026gt; Without any additional information, my first guess is to look at the mesh dataset \u0026ndash; that\u0026rsquo;s likely where I\u0026rsquo;m going to find the information for how the dataset is organized.\nf[\u0026quot;/mesh\u0026quot;].keys() \u0026lt;KeysViewHDF5 ['array_dimensions', 'cycle', 'd_omega', 'dx_cf', 'dy_cf', 'dz_cf', 'i_frame', 'last_frame', 'my_hyperslab_group', 'nz_hyperslabs', 'ongrid_mask', 'phi_index_bound', 'r_comv', 'radial_index_bound', 't_bounce', 'theta_index_bound', 'time', 'time_steps', 'x_cf', 'x_ef', 'y_cf', 'y_ef', 'z_cf', 'z_ef']\u0026gt; Well, here we go! Lots of interesting things to look at. I\u0026rsquo;ll save a bit of time here and note that the things we\u0026rsquo;re interested in are the array_dimensions and the various _ef arrays: x_ef, y_ef, z_ef. These are the dimensions of the simulation and the cell edges, respectively. There are other interesting things here as well, but for our current purposes, this is what we want to look at.\ndims = f[\u0026quot;/mesh/array_dimensions\u0026quot;][()] xgrid, ygrid, zgrid = (f[\u0026quot;/mesh/{}_ef\u0026quot;.format(ax)][()] for ax in 'xyz') xgrid.shape, ygrid.shape, zgrid.shape, dims ((723,), (241,), (2,), array([722, 240, 1], dtype=int32)) We\u0026rsquo;re dealing with cell edges here so we have one more in each dimension than the cell-centered variables. Great! Everything is going according to plan so far.\n(You might be thinking to yourself, \u0026ldquo;Gee, wouldn\u0026rsquo;t it be nice if this 1:1 mapping of values to what yt expects was easier to set up?\u0026rdquo; I certainly was.)\nLet\u0026rsquo;s see what fields we can load up:\nf[\u0026quot;/fluid\u0026quot;].keys() \u0026lt;KeysViewHDF5 ['LScompress', 'agr_c', 'agr_e', 'c_eos_i', 'dimeos', 'dudt_nu', 'dudt_nuc', 'e_int', 'entropy', 'eos_rho', 'grav_x_c', 'grav_y_c', 'grav_z_c', 'press', 'rho_c', 't_c', 'u_c', 'v_c', 'v_csound', 'wBVMD', 'w_c', 'ye_c', 'ylep']\u0026gt; Some might have reduced dimensionality, so we\u0026rsquo;ll take a look at what will be the most obvious to load in by identifying which have the right dimensionality.\nimport collections collections.Counter(f[\u0026quot;/fluid\u0026quot;][v].shape for v in f[\u0026quot;/fluid\u0026quot;]) Counter({(): 1, (722,): 1, (723,): 1, (1,): 2, (2,): 1, (1, 240, 722): 17}) Looks like the fluid fields are stored in reverse order from the dims array, so let\u0026rsquo;s populate our dictionary (all in-memory) with this info.\nWe\u0026rsquo;ll grab the transpose, though, so that we keep the same ijk ordering as specified in the dimensions.\ndata = {n: v[()].T for n, v in f[\u0026quot;/fluid\u0026quot;].items() if v.shape == tuple(dims[::-1])} data.keys() dict_keys(['dudt_nu', 'dudt_nuc', 'e_int', 'entropy', 'grav_x_c', 'grav_y_c', 'grav_z_c', 'press', 'rho_c', 't_c', 'u_c', 'v_c', 'v_csound', 'wBVMD', 'w_c', 'ye_c', 'ylep']) We can now proceed! Let\u0026rsquo;s generate our semi-structured mesh from our coordinate info and load it all in.\nThis next function generates a full set of coordinates and connectivity to make our hexahedral mesh look like a structured system. (This is a place that yt could use some improvement, to speed things up by not requiring this hoop-jumping!)\ncoord, conn = yt.hexahedral_connectivity(xgrid, ygrid, zgrid) Next up will be loading things in using the yt.load_hexahedral_mesh function, so let\u0026rsquo;s look at the docstring for that.\nyt.load_hexahedral_mesh? Signature: yt.load_hexahedral_mesh( data, connectivity, coordinates, length_unit=None, bbox=None, sim_time=0.0, mass_unit=None, time_unit=None, velocity_unit=None, magnetic_unit=None, periodicity=(True, True, True), geometry='cartesian', unit_system='cgs', ) Docstring: Load a hexahedral mesh of data into yt as a :class:`~yt.frontends.stream.data_structures.StreamHandler`. This should allow a semistructured grid of data to be loaded directly into yt and analyzed as would any others. This comes with several caveats: * Units will be incorrect unless the data has already been converted to cgs. * Some functions may behave oddly, and parallelism will be disappointing or non-existent in most cases. * Particles may be difficult to integrate. Particle fields are detected as one-dimensional fields. The number of particles is set by the \u0026quot;number_of_particles\u0026quot; key in data. Parameters ---------- data : dict This is a dict of numpy arrays, where the keys are the field names. There must only be one. Note that the data in the numpy arrays should define the cell-averaged value for of the quantity in in the hexahedral cell. connectivity : array_like This should be of size (N,8) where N is the number of zones. coordinates : array_like This should be of size (M,3) where M is the number of vertices indicated in the connectivity matrix. bbox : array_like (xdim:zdim, LE:RE), optional Size of computational domain in units of the length unit. sim_time : float, optional The simulation time in seconds mass_unit : string Unit to use for masses. Defaults to unitless. time_unit : string Unit to use for times. Defaults to unitless. velocity_unit : string Unit to use for velocities. Defaults to unitless. magnetic_unit : string Unit to use for magnetic fields. Defaults to unitless. periodicity : tuple of booleans Determines whether the data will be treated as periodic along each axis geometry : string or tuple \u0026quot;cartesian\u0026quot;, \u0026quot;cylindrical\u0026quot;, \u0026quot;polar\u0026quot;, \u0026quot;spherical\u0026quot;, \u0026quot;geographic\u0026quot; or \u0026quot;spectral_cube\u0026quot;. Optionally, a tuple can be provided to specify the axis ordering -- for instance, to specify that the axis ordering should be z, x, y, this would be: (\u0026quot;cartesian\u0026quot;, (\u0026quot;z\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)). The same can be done for other coordinates, for instance: (\u0026quot;spherical\u0026quot;, (\u0026quot;theta\u0026quot;, \u0026quot;phi\u0026quot;, \u0026quot;r\u0026quot;)). File: ~/yt/yt/yt/frontends/stream/data_structures.py Type: function Looks like we\u0026rsquo;ve got just about everything except for the bounding box. So let\u0026rsquo;s generate that.\nbbox = np.array([ [xgrid.min(), xgrid.max()], [ygrid.min(), ygrid.max()], [zgrid.min(), zgrid.max()] ]) bbox array([[0.00000000e+00, 7.70275059e+09], [0.00000000e+00, 3.14159265e+00], [0.00000000e+00, 6.28318531e+00]]) And the last thing we need to do is to specify the geometry. This dataset is spherical (but with only one value along the azimuthal direction, in this case) and so we have to specify which coordinate is which. We do that using the geometry keyword argument, where we specify not only the geometry\u0026rsquo;s name, but the axis-ordering.\nHere we have:\nr is the first dimension theta is the declination, which spans an interval of $\\pi$, so it\u0026rsquo;s the second dimension phi is our azimuthal angle, which goes $2\\pi$ (and here is identical at all azimuthal angles) so it\u0026rsquo;s the third. ds = yt.load_hexahedral_mesh(data, conn, coord, bbox = bbox, geometry = (\u0026quot;spherical\u0026quot;, ('r', 'theta', 'phi'))) s = yt.SlicePlot(ds, \u0026quot;phi\u0026quot;, \u0026quot;e_int\u0026quot;) s.show() Looks kinda right! Unfortunately it\u0026rsquo;s a bit tricky to navigate these coordinates in the version of yt I\u0026rsquo;m running, so we have to do a bit of work to get ourselves zoomed in to see things closer up.\ns.set_center( (0.0, 0.0) ) s.zoom(50) s.pan_rel((0.5, 0.0)) We\u0026rsquo;re now at the point that things can be used, but \u0026hellip; it was kind of irritating getting here!\nWhat have we learned? Good question! The first thing I thought as I was going through this was that I did an awful lot of work for what amounted to a one-to-one mapping between the dataset and the arguments that yt expected. It feels like I should just be able to mark all of this up with some metadata to make the loading process much easier, before writing a specific frontend that manages this all for us.\nOver the last few days, a collaborator (Sam Walkow) at UIUC and I started brainstorming what a metadata schema would look like that could map from a file format directly to yt. One of the advantages of having this explicitly typed and validated is that we can check for errors earlier in the process and we can also make logical leaps from one piece of information to another.\nFor instance, note that above we define the bbox independently of the coordinate system. But, for a dataset like this, they will usually be the same! And, since we\u0026rsquo;re dealing with an HDF5 dataset, we should be able to just specify that.\nWe\u0026rsquo;ve started drafting what this might look like, using JSON-Schema via PyDantic and some helper classes. Ideally, we should be able to specify something in yaml that describes how to get everything we want. Imagine, if instead of what we showed above, we had a small schema that yt could read and that could potentially even live as a sidecar file! (And then we could even register a mimetype for display in Jupyterlab\u0026hellip;) We\u0026rsquo;ve been exploring using this for declarative analysis, to reduce the burden of writing out lots of boilerplate code, but it seems like it would be a great match for this, too.\nAnyway, I was speculating that something like this \u0026ndash; but accounting for an explicitly declared list of fields, etc \u0026ndash; might be sufficient.\nmetadata: filetype: hdf5 geometry: dimensions: /mesh/array_dimensions spherical: order: [r, theta, phi] bbox: implicit hexahedral_mesh: edges: [/mesh/x_ef, /mesh/y_ef, /mesh/z_ef] fields: implicit: true root: /fluid validate: true process: transpose This doesn\u0026rsquo;t yet work, but I think it gives a target for how we might think about making it much easier to load in data in formats that have such a nice mapping between yt and the dataset. And it leaves enough room for things like on-demand loading (which the stream frontend does support, but in an opaque way) that it could be quite nice to extend this in the future.\nI\u0026rsquo;ve pretty much talked myself into doing something like this and seeing how it\u0026rsquo;d work. Maybe it could be a SciPy project next week!\nAcknowledgments Thanks to Bronson Messer, Ronan Hix and Samantha Walkow for the data and speculating, and to Nathan Goldbaum for planting this \u0026ldquo;let\u0026rsquo;s just write a yaml declaration\u0026rdquo; idea in my head a million years ago. (It took me a while to catch up to how insightful an idea that was.)\nIt\u0026rsquo;s also probably worth mentioning that these amazing revelations I\u0026rsquo;m having are \u0026hellip; not that new to lots of people using data! I mean, folks working on things like wcsaxes and netCDF metadata and lots of others have trod this ground before!\n","date":1562010461,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562010704,"objectID":"1c52fd1b51a3ba39bf88723b2a21cb53","permalink":"https://matthewturk.github.io/post/loading_data_in_yt/","publishdate":"2019-07-01T14:47:41-05:00","relpermalink":"/post/loading_data_in_yt/","section":"post","summary":"In this blogpost, I walk through the annoying bits about loading unknown data into yt.","tags":[],"title":"Loading data in yt: Can we make it better?","type":"post"},{"authors":[],"categories":[],"content":" tl;dr The Whole Tale leverages neat stuff like Jupyter, repo2docker and RStudio, building out a research environment that gives you access to any data, anywhere \u0026ndash; and when you stop you can pick right up where you left off, with your data, your research, and even any customizations or preferences you\u0026rsquo;ve set.\nYou can even try out the new \u0026ldquo;give me a dataset URL\u0026rdquo; page that I made that isn\u0026rsquo;t quite amazing but is still fun. (It\u0026rsquo;s over here for now but hopefully will move some day.)\nWhole Tale v0.7 Last week at NCSA we had our twice-annual \u0026ldquo;all-hands\u0026rdquo; meeting for the Whole Tale project, which we\u0026rsquo;ve been working on for a few years. It\u0026rsquo;s a collaboration between University of Illinois, University of Chicago, University of Texas at Austin, Notre Dame, and UC Santa Barbara.\nAnd this week we finally, finally, finally decided: we\u0026rsquo;ve been way too cautious in asking people to use the system, and because of that, we haven\u0026rsquo;t really talked about some of the really awesome stuff that\u0026rsquo;s been developed for it. Everything is developed fully in the open on our whole-tale org and wherever we can, we push changes upstream and coordinate with other projects.\nSo, here we go: what is in the hot-off-the-presses Whole Tale v0.7? (We have a release video, too! Also, a much longer one from our previous release, if that interests you.)\nBut first, you might be asking, what is the Whole Tale project, anyway? (Of course there is a paper out there, but it\u0026rsquo;s a lot longer than this blog post.)\n\u0026ldquo;Whole Tale,\u0026rdquo; really? It\u0026rsquo;s a pun! See, it\u0026rsquo;s supposed to both tell the \u0026ldquo;whole story\u0026rdquo; and it\u0026rsquo;s supposed to work for researchers at both ends of the distribution of data sizes \u0026ndash; both the \u0026ldquo;long tail\u0026rdquo; and the other bit.\nThe Whole Tale (hereafter just \u0026lsquo;WT\u0026rsquo;) is designed to be a way to \u0026ndash; first and foremost \u0026ndash; do your research, at any stage in the research lifecycle. That means it should work for exploring data, for refining your processing of data, and all the way up to the point where you want to package it up and publish it. And then, when someone wants to re-explore your work in a different way, or to apply your work to a different dataset, they can do so within this same environment.\nI must confess, writing it like this sounds a bit underwhelming! But there are a couple big hangups with that \u0026ndash; for starters, everybody has a different environment they like using. And, folks usually want to apply their work to some kind of data, which might live somewhere else and might even be lots of bits from lots of places. The final thing that is usually really tricky is that folks usually want to be able to have some stuff that hangs around in between sessions, or in between different research projects.\nWT tries pretty hard to solve these different problems to make a really nice place to work on (and ultimately, publish!) research. And by \u0026ldquo;solve,\u0026rdquo; the idea isn\u0026rsquo;t to just build stuff from scratch, but to find things that exist, glue them together, and make the experience as near-seamless as possible.\nFor these examples, let\u0026rsquo;s imagine that our researcher is \u0026hellip; me, I guess. And let\u0026rsquo;s say that I want to analyze some data from Illinois, where I live. Usually I analyze data in Jupyter, so let\u0026rsquo;s start with that.\nData in there The first pain point that WT aims to address is that of data. Many research workflows \u0026ndash; from the very start all the way up to the very end \u0026ndash; require having access to datasets. Sometimes those datasets are created by the individual researcher, but in many cases, they aren\u0026rsquo;t. And so starting a research project often means answering (even just implicitly!) a bunch of questions about that data.\nWhere is the data?\nHow do I get the data into my working environment?\nCan I grab a bunch of data from lots of other places?\nWT handles data through the notion of \u0026ldquo;registration.\u0026rdquo; When a URL is ingested in the system, it stores some information about that data, and it figures out the best way to get at that data. In the default case, WT will access it via HTTP. It\u0026rsquo;s particularly good at registering and accessing data through Dataverse, DataONE and Globus.\nUnder the hood, WT uses a system called Girder for storing metadata about registered items, where they are located (as a hinting system for where to launch workers), what the access controls are, and what \u0026ldquo;collections\u0026rdquo; they belong to.\nHere\u0026rsquo;s the part that I like about this the most: WT does not just slurp up the data and make a copy locally. Instead, it references the remote data and fetches subsets of it on-demand. And more to the point, it does this by creating a virtual filesystem that makes everything look like they\u0026rsquo;re files there on disk.\nSo I\u0026rsquo;ll use some open data from the State \u0026ndash; I\u0026rsquo;m curious about the distribution of rest areas in the state, so let\u0026rsquo;s use that data set. I grabbed the URL for the CSV by right-clicking on \u0026ldquo;Download\u0026rdquo; and copying the URL. So now when I \u0026ldquo;register\u0026rdquo; this in Whole Tale, I\u0026rsquo;ll see it as a CSV file right there in the filesystem.\n(And I haven\u0026rsquo;t had to download anything to my laptop yet.)\nThere are a handful of special things about this, and a few drawbacks. WT knows how to register data from DataVerse and DataONE, and it can access anything over HTTP as long as the server sends the size back in the HTTP headers. And you can access the file just like it really lived there.\nA Welcome Mat When I open up my laptop, it\u0026rsquo;s basically exactly where I left it the last time I closed it. I\u0026rsquo;ve got tabs open in my browser, in-progress proposals in text editors, and all of my preferences are right there. When I open up vim it\u0026rsquo;s got all my plugins and keybindings still set up.\nWhole Tale wants to feel as much like home as your laptop. So in between sessions, in between different setups, you get to keep a home directory. And when you collaborate with somebody else, the \u0026ldquo;Tale\u0026rdquo; you work on will have a shared workspace directory.\nWhen you use an environment in WT, there\u0026rsquo;ll be three directories that you will see: home, workspace and data. Home is yours, and it\u0026rsquo;ll be there when you quit and come back. If you run this time in RStudio and stick stuff in home, it\u0026rsquo;ll be there when you run again in Jupyterlab. In workspace, stuff that gets saved shows up the next time somebody runs that Tale \u0026ndash; it isn\u0026rsquo;t quite involatile data, but it\u0026rsquo;s also not really personal data. So this is where you might put intermediate results, or helpful scripts.\nEverything under data, of course, is data that exists \u0026hellip; elsewhere. It\u0026rsquo;s all read only, but you can dynamically add new things to the running Tale \u0026ndash; not just things that you\u0026rsquo;ve registered, but things that show up in the WT catalog.\nOne of my favorite parts of WT, though, is the way that the \u0026ldquo;home\u0026rdquo; directory is made available. Not only can you access it through the WT user interface, but you can even mount it as a directory on your laptop or desktop \u0026ndash; it is exposed as a WebDAV filesystem! OSX, Windows and Linux all have native support for this. So if you\u0026rsquo;re working on getting some little scripts or maybe some supplemental data in, you can just drag it over to the WT folder on your desktop.\nSo you\u0026rsquo;ve got data, maybe from lots of places, you\u0026rsquo;ve got an environment, and every time you log in it feels like \u0026ldquo;home\u0026rdquo; \u0026ndash; now let\u0026rsquo;s imagine you\u0026rsquo;ve done some work and you decide, eventually, that you are done. Your work is ready to be published!\nCollaborating and Remixing This part is the most fun \u0026ndash; within a given Tale, we\u0026rsquo;ve got the ability to share files across instances of that Tale (through the \u0026ldquo;workspace\u0026rdquo; folder) and you can also grab somebody else\u0026rsquo;s published tale and mess about with it \u0026ndash; changing the environment, doing different things, and so on and so on.\nTry it out! Give it a shot! There are still some rough edges, but it should mostly work \u0026ndash; and I\u0026rsquo;ve personally been using it already to collaborate with students on projects and papers.\nYou can find out more about the project at the Whole Tale Website and all of the source is in the whole-tale GitHub org. And if you want to try it out on your own resources, all of our development and deployment scripts are in our deploy-dev repository.\nPlus, we\u0026rsquo;d love to work with you to add integrations for new data stores, hear about cool ideas you might have, and if you do something fun with it, we definitely want to hear about that!\n","date":1561735964,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561737867,"objectID":"45d6bb9b5c7bd653853b0da2ffb15ba8","permalink":"https://matthewturk.github.io/post/exploration-whole-tale/","publishdate":"2019-06-28T10:32:44-05:00","relpermalink":"/post/exploration-whole-tale/","section":"post","summary":"What is this Whole Tale thing?","tags":[],"title":"Whole Tale: Exploration, Analysis and Reproducibility","type":"post"},{"authors":[],"categories":[],"content":"tl;dr: kaitai struct is awesome.\nFile formats can be pretty annoying \u0026ndash; especially when you figure them out through weird combinations of reverse-engineering, hand-me-down code and trial-and-error.\nWhat we\u0026rsquo;ve ended up with in yt is a bunch of data formats where the process of conducting the IO is all mixed up with the description of that IO. This means that any attempt to update things (which I\u0026rsquo;ve alluded to in these blog posts) requires a fair bit of care to make sure that the process is not disruptive in any way.\nIn preparation for this, I set out to start writing down some of the file formats in a flat text format by going through my old notes and the code in the various yt io.py files. Before too long, I decided to instead start utilizing kaitai struct instead.\nI\u0026rsquo;ve found Kaitai struct to be useful in other projects \u0026ndash; for instance, I have been using it as a method to reverse engineer the data files from an old DOS game I used to play with my brother back in the early 90s. The combination of the simple (but reasonably flexible) syntax, the ruby-based visualizer and the WebIDE make it really good for exploratory reverse engineering.\nAnd, it generates code! You can run:\n$ ksc -t python somefile.ksy and it\u0026rsquo;ll generate a python reader. It can also generate javascript, java, go, etc, and I think rust is in the works.\nSo the question became: could we use kaitai struct for documenting a data format like, say, the binary format from GADGET-2? After that we can really stretch our legs and try it on more complicated ones, but let\u0026rsquo;s start with this one.\nGadget-2 Data Format The (vanilla) Gadget-2 data format is reasonably straightforward, and it\u0026rsquo;s even described in some detail in the Gadget manual. You have a header, then sets of records. Each of these sets of records has a separating value (sometimes!). According to table 3 in the user guide, the file looks something like:\nHeader Position Velocity ID Mass (only for variable mass particles) Energy (only for gas particles) Density (Only for gas particles) Smoothing length (Only for gas particles) Potential (if enabled in makefile) Acceleration (if enabled in makefile) Endtime (if enabled in makefile) Timestep (if enabled in makefile) Gadget-2 is probably the most widely-used astrophysics simulation code, but one of its defining characteristics is that it is quite readily hackable to include additional parameters, additional particle attributes, and lots more physics. This usually means that when you receive binary gadget data, you need to know what to expect in the file \u0026ndash; the individual file formats are typically not self-describing.\nThe header is often left unchanged, at least in the data I\u0026rsquo;ve seen. In yt we have a simple specification system to allow for modifications to both the fields and the header, but the \u0026ldquo;vanilla\u0026rdquo; header looks something like this (in python struct notation):\ndefault = (('Npart', 6, 'i'), ('Massarr', 6, 'd'), ('Time', 1, 'd'), ('Redshift', 1, 'd'), ('FlagSfr', 1, 'i'), ('FlagFeedback', 1, 'i'), ('Nall', 6, 'i'), ('FlagCooling', 1, 'i'), ('NumFiles', 1, 'i'), ('BoxSize', 1, 'd'), ('Omega0', 1, 'd'), ('OmegaLambda', 1, 'd'), ('HubbleParam', 1, 'd'), ('FlagAge', 1, 'i'), ('FlagMetals', 1, 'i'), ('NallHW', 6, 'i'), ('unused', 16, 'i')) Before we get any further, let\u0026rsquo;s check if we can parse just this with kaitai struct.\nOur First ksy File In its simplest form, Kaitai specifications allow specifying the format of data and the order that different types of data will arrive in. The base data types it has are standard numerical data types, bytes, strings, arrays and so on.\nKaitai has a little bit of metadata we will add at the beginning, so we start with this preamble:\nmeta: id: gadget_format1 endian: le This gives it a name and notes it as little endian.\nIf we wanted to build a parser for the header, the most obvious thing we could do would be to parse the different items in order. The very first item is an array of 6 integers that describes the number of particles in the simulation, separated by each particle type. This comes in a sequence starting at the beginning of our \u0026ldquo;stream,\u0026rdquo; so we can use the seq top-level type to start parsing:\nseq: - id: recsize_0 type: u4 - id: npart1 type: u4 - id: npart2 type: u4 - id: npart3 type: u4 - id: npart4 type: u4 - id: npart5 type: u4 - id: npart6 type: u4 recsize_0 here is because we know that our header will be preceded by a value indicating how many bytes are in it \u0026ndash; this is a common practice, but not universal. And I\u0026rsquo;ve said that each of the variables (npart1 \u0026hellip; npart6) is of type u4, which means \u0026ldquo;unsigned four byte integers.\u0026rdquo;\nThis isn\u0026rsquo;t that efficient, though \u0026ndash; each particle type has its own variable. KS provides us with the option to repeat which can take an expression. Now we can enable grabbing arrays of values, so we can use that, instead. Let\u0026rsquo;s change this to:\nseq: - id: recsize_0 type: u4 - id: npart type: u4 repeat: expr repeat-expr: 6 (You can also do repeat: eos for end of stream and repeat-until.) The result now is that npart is an array \u0026ndash; so later on when we loop, we can look it up by using the special variable _index inside another repeat-expr. We\u0026rsquo;ll look at that a bit later.\nData Types in ksy Parsing the entire header in this way is certainly possible, but it also can be a bit clunky \u0026ndash; and can be harder for maintainability. Instead of writing all of our header values out, let\u0026rsquo;s use a user-defined type.\nThe top-level key types is where user-defined types are described; you can do some pretty fancy things like supply parameters to them, but we\u0026rsquo;ll just use it to hold the data we know.\ntypes: gadget_header: seq: ... Now, in our top-level seq section, we can just reference the header type:\nseq: -id: header type: gadget_header Big Arrays and Lazy-reading The tricky bit comes in when we get to our arrays of values. Kaitai has several different languages it can generate for \u0026ndash; Python, C#, C++, ruby, and others. When reading lots of little items, python can get bogged down. For instance, this is the most obvious way of describing one of the position arrays:\n-id: coordinates_part1 type: f4 repeat: expr repeat-expr: header.npart[0] * 3 The generated python code will read a series of 4-byte objects, one at a time, and append them to a list. The combination of these things results in a lot of overhead from the python language runtime and the type system, so it ends up being rather slow.\nOne way to get around this is to use what\u0026rsquo;s called an instance. This allows parsing to happen only when requested, and it can also exist outside of the rest of the parsing structure.\nImportant note: There are some fiddly issues with making instance objects work and how they relate to substreams of IO which I\u0026rsquo;m going to sidestep here. Let\u0026rsquo;s just assume that it works, instead!\nBy default, we can read in bytes for these individual objects; this lets us read a big block of data (which we can deal with later \u0026ndash; for instance, inside python!) and only parse into individual floats when we request. This might look something like defining a custom type with both a seq and an instances section:\ntypes: f4_array_type: params: - id: count type: u4 seq: - id: buffer size: 4 * count instances: values: pos: 0 size-eos: true id: entries: type: f4 repeat: eos Here we are saying, just read the bytes. But, if anybody asks, this is what the attribute values should look like \u0026ndash; you should parse it starting at 0, go to the end of the stream, and assume it\u0026rsquo;s all f4-typed items. (I am not certain that both size-eos and repeat-eos are necessary.)\nWith these components, we should be able to parse our entire gadget binary file, a\nWorking Implementation I\u0026rsquo;ve gotten this to the point that it mostly works, and I\u0026rsquo;ve been committing to data-exp-lab/astro-data-formats. I ended up splitting the array stuff into its own parameterized type that I store in a separate file, so that we can potentially reuse it in other file formats.\nHere\u0026rsquo;s some of the high-level stuff:\nmeta: id: gadget_format1 endian: le ks-opaque-types: true imports: - array_buffer seq: - id: gadget_header type: header - id: coordinates type: particle_fields('f4', 3) - id: velocities type: particle_fields('f4', 3) - id: particle_ids type: particle_fields('u4', 1) types: header: seq: - id: recsize_0 type: u4 - id: npart type: u4 repeat: expr repeat-expr: 6 - id: massarr type: f8 repeat: expr repeat-expr: 6 [ ... ] particle_fields: params: - id: field_type type: str - id: components type: u1 seq: - id: magic1 type: u4 - id: fields type: field(_index, components, field_type) repeat: expr repeat-expr: 6 - id: magic2 type: u4 field: params: - id: index type: u1 - id: components type: u1 - id: field_type type: str seq: - id: field size: _root.gadget_header.npart[index] * components * 4 type: array_buffer(field_type) And then the array_buffer is an instance-based way of getting the raw bytes. Right now this is reasonably fast, and there\u0026rsquo;s a possibility I could simplify the structure and flatten it out a bit, but it works exactly as I want it to.\nFuture The next thing I\u0026rsquo;m going to do is work on porting the RAMSES frontend to this format. That will enable some more exploration and validation of the different outputs, and hopefully use that as guidance for any future modifications to the IO systems. One of the stickier wickets I\u0026rsquo;ve seen is that it\u0026rsquo;s possible to use memory-mapping, but I think there are some subtle places where data is read into memory and copied \u0026ndash; this kind of gets rid of the purpose of the memory mapping. I hope to explore and dig more deeply into this later.\nUltimately, I believe a combination of ksy files for binary formats and a ksy-like dialect for describing the connection between chunks of data and physical space locations will simplify a considerable amount of data analysis.\nI\u0026rsquo;d also like to thank @GreyCat on gitter, who was super helpful in helping me work out some of the bits I wasn\u0026rsquo;t clear on.\n","date":1561054541,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561054675,"objectID":"151f1bca4c9b3688b837b481cd15c68d","permalink":"https://matthewturk.github.io/post/kaitai-struct-scientific-data/","publishdate":"2019-06-20T13:15:41-05:00","relpermalink":"/post/kaitai-struct-scientific-data/","section":"post","summary":"tl;dr: kaitai struct is awesome.\nFile formats can be pretty annoying \u0026ndash; especially when you figure them out through weird combinations of reverse-engineering, hand-me-down code and trial-and-error.\nWhat we\u0026rsquo;ve ended up with in yt is a bunch of data formats where the process of conducting the IO is all mixed up with the description of that IO.","tags":[],"title":"Kaitai Struct and Scientific Data","type":"post"},{"authors":[],"categories":null,"content":"","date":1560877562,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560877679,"objectID":"ceb7bcdee46f24006f72ca99f8fb9304","permalink":"https://matthewturk.github.io/talk/2019-06-18-oshiw-numfocus/","publishdate":"2019-06-18T13:06:02-04:00","relpermalink":"/talk/2019-06-18-oshiw-numfocus/","section":"talk","summary":"In this talk, I present a retrospective on the origins and goals of the NumFOCUS software sustainability project I was involved in in 2016-2017.","tags":[],"title":"OSHIW: NumFOCUS Sustainability","type":"talk"},{"authors":[],"categories":[],"content":"Welcome to part 3 of a series on how yt deals with data and the ways that helps and hinders things! This time, I am going to describe what \u0026ldquo;chunks\u0026rdquo; of data (YTDataChunk) in yt are, and a few characteristics of them that wouldn\u0026rsquo;t be obvious from the previous blog posts.\nChunks have spatial attributes Data chunks in yt have a set of special attributes that help yt to put them in the context of the coordinate domain.\n(This is as good a time as any to note to myself that I should probably write up a blog post about how coordinate handling works, as opposed to index handling.)\nWhen you receive a chunk of a data object, that chunk will have with it information about the coordinate space of the data contained within it \u0026ndash; specifically, if it is a dataset that is volumetrically discretized in some regular way, it will have information about the centers of the individual grid cells, the sizes of those cells, and some measure of their resolution.\nFor a grid dataset specifically, these attributes will always exist on a data chunk:\nfcoords - the centers of the grid cells, of shape (..., 3) fwidth - the \u0026ldquo;width\u0026rdquo; of the grid cells, of shape (..., 3) icoords - the integer coordinates, with respect to the current resolution, of the grid cells, of shape (..., 3) ires - the \u0026ldquo;level of resolution\u0026rdquo; of a given cell; this mostly makes sense for datasets where there is some universal refinement ratio and a fixed value for the domain dimensions, of shape (...,) There are a couple more that are specific to the individual data selection operation \u0026ndash; for instance, tcoords only makes sense if there is a parameterized vector being pushed through the domain. There\u0026rsquo;s also fcoords_vertex but it is seldom used except in unstructured mesh datasets.\nLet\u0026rsquo;s take a look at these values, and how they correspond to grid attributes:\nimport yt ds = yt.load(\u0026quot;data/IsolatedGalaxy/galaxy0030/galaxy0030\u0026quot;) /home/matthewturk/conda-py3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject return f(*args, **kwds) yt : [INFO ] 2019-06-17 22:23:02,723 Parameters: current_time = 0.0060000200028298 yt : [INFO ] 2019-06-17 22:23:02,724 Parameters: domain_dimensions = [32 32 32] yt : [INFO ] 2019-06-17 22:23:02,726 Parameters: domain_left_edge = [0. 0. 0.] yt : [INFO ] 2019-06-17 22:23:02,728 Parameters: domain_right_edge = [1. 1. 1.] yt : [INFO ] 2019-06-17 22:23:02,734 Parameters: cosmological_simulation = 0.0 What yt is doing here is taking an individual, in-memory sparse index and then expanding that index into a fully-fledged array. Internally, when we ask for any of these attributes, yt iterates over all the objects that belong in a given chunk and then it expands the values associated with those objects.\nWe can see this if we poke a little closer at the Grid objects in this case.\nfor g in ds.index.grids: pass Parsing Hierarchy : 100%|██████████| 173/173 [00:00\u0026lt;00:00, 3064.43it/s] yt : [INFO ] 2019-06-17 22:23:02,831 Gathering a field list (this may take a moment.) This is used internally in yt whenever we want to deal with a geometric selection, or when we apply geometric processing. For instance, the \u0026ldquo;projection\u0026rdquo; operator can use the integer coordinate system to very rapidly insert values into a quadtree. yt can iterate over all of the chunks that belong to a data object and insert them into the quadtree based on their icoords, and it can refine nodes by bit shifting the appropriate amount.\nFor grid data, icoords also gives us some handy things for evaluating relationships between objects. For instance, we might have a root grid with one child grid. We can figure out their relationship by looking at their icoords, ires and the result of get_global_startindex(). In our sample dataset, let\u0026rsquo;s look at some random grid \u0026ndash; I happen to know the first level 8 grid is index 27, so let\u0026rsquo;s use that.\nchild = ds.index.grids[27] parent = child.Parent parent.icoords array([[2048, 2048, 2048], [2048, 2048, 2049], [2048, 2048, 2050], ..., [2087, 2089, 2069], [2087, 2089, 2070], [2087, 2089, 2071]]) child.icoords array([[4096, 4096, 4096], [4096, 4096, 4097], [4096, 4096, 4098], ..., [4133, 4129, 4119], [4133, 4129, 4120], [4133, 4129, 4121]]) parent.LeftEdge, child.LeftEdge (YTArray([0.5, 0.5, 0.5]) code_length, YTArray([0.5, 0.5, 0.5]) code_length) They both start at the same place \u0026ndash; so that would suggest that their minimum icoords should refer to the same location.\ndifference = child.ires.min() - parent.ires.max() left_edge_child = child.icoords.min(axis=0) left_edge_parent = parent.icoords.min(axis=0) (left_edge_child - left_edge_parent) array([2048, 2048, 2048]) This seems odd until we recognize that ires differs between the two, and in this case refers to the bit shifting we need to do to match them up.\n(left_edge_child \u0026gt;\u0026gt; difference) - left_edge_parent array([0, 0, 0]) We can do this with the next grid up, as well.\nleft_edge_gparent = parent.Parent.icoords.min(axis=0) difference_g = child.ires.min() - parent.Parent.ires.max() (left_edge_child \u0026gt;\u0026gt; difference_g) - left_edge_gparent array([0, 0, 0]) There\u0026rsquo;s more to this story \u0026ndash; for instance, non-power-of-two differences, but the idea is consistent; both integer and float positioning can be used between chunks, sub-chunks, and so on.\nWe can also pretty easily get cell positions, and if we access data objects, we receive all the values across all sub-objects.\nsp = ds.sphere(\u0026quot;c\u0026quot;, 0.1) print(sp.fwidth) [[0.00024414 0.00024414 0.00024414] [0.00024414 0.00024414 0.00024414] [0.00024414 0.00024414 0.00024414] ... [0.00012207 0.00012207 0.00012207] [0.00012207 0.00012207 0.00012207] [0.00012207 0.00012207 0.00012207]] code_length Chunks can be sub-chunked The other main advantage of chunks is that they can be sub-chunked. Usually this only means up to one level, but it means that we could in principle do something like this:\ndd = ds.all_data() for i, chunk1 in enumerate(dd.chunks([], \u0026quot;io\u0026quot;)): print(\u0026quot;Chunk \u0026quot;, i, len(chunk1._current_chunk.objs)) print(\u0026quot; \u0026quot;, end = \u0026quot;\u0026quot;) for j, chunk2 in enumerate(dd.chunks([], \u0026quot;spatial\u0026quot;)): print(chunk2._current_chunk.objs, end = \u0026quot; \u0026quot;) print() print() Chunk 0 5 [EnzoGrid_0001] [EnzoGrid_0075] [EnzoGrid_0076] [EnzoGrid_0082] [EnzoGrid_0110] Chunk 1 1 [EnzoGrid_0073] Chunk 2 20 [EnzoGrid_0009] [EnzoGrid_0010] [EnzoGrid_0011] [EnzoGrid_0012] [EnzoGrid_0013] [EnzoGrid_0014] [EnzoGrid_0015] [EnzoGrid_0016] [EnzoGrid_0017] [EnzoGrid_0018] [EnzoGrid_0019] [EnzoGrid_0020] [EnzoGrid_0021] [EnzoGrid_0022] [EnzoGrid_0023] [EnzoGrid_0024] [EnzoGrid_0025] [EnzoGrid_0026] [EnzoGrid_0027] [EnzoGrid_0028] Chunk 3 28 [EnzoGrid_0008] [EnzoGrid_0029] [EnzoGrid_0030] [EnzoGrid_0031] [EnzoGrid_0032] [EnzoGrid_0033] [EnzoGrid_0034] [EnzoGrid_0035] [EnzoGrid_0036] [EnzoGrid_0037] [EnzoGrid_0038] [EnzoGrid_0039] [EnzoGrid_0040] [EnzoGrid_0041] [EnzoGrid_0042] [EnzoGrid_0043] [EnzoGrid_0044] [EnzoGrid_0045] [EnzoGrid_0046] [EnzoGrid_0047] [EnzoGrid_0048] [EnzoGrid_0049] [EnzoGrid_0050] [EnzoGrid_0051] [EnzoGrid_0052] [EnzoGrid_0053] [EnzoGrid_0054] [EnzoGrid_0055] Chunk 4 20 [EnzoGrid_0007] [EnzoGrid_0056] [EnzoGrid_0057] [EnzoGrid_0058] [EnzoGrid_0059] [EnzoGrid_0060] [EnzoGrid_0061] [EnzoGrid_0062] [EnzoGrid_0063] [EnzoGrid_0064] [EnzoGrid_0065] [EnzoGrid_0066] [EnzoGrid_0067] [EnzoGrid_0068] [EnzoGrid_0069] [EnzoGrid_0070] [EnzoGrid_0071] [EnzoGrid_0072] [EnzoGrid_0074] [EnzoGrid_0077] Chunk 5 16 [EnzoGrid_0006] [EnzoGrid_0078] [EnzoGrid_0079] [EnzoGrid_0080] [EnzoGrid_0081] [EnzoGrid_0083] [EnzoGrid_0084] [EnzoGrid_0085] [EnzoGrid_0086] [EnzoGrid_0087] [EnzoGrid_0088] [EnzoGrid_0089] [EnzoGrid_0090] [EnzoGrid_0091] [EnzoGrid_0092] [EnzoGrid_0093] Chunk 6 13 [EnzoGrid_0005] [EnzoGrid_0094] [EnzoGrid_0095] [EnzoGrid_0096] [EnzoGrid_0097] [EnzoGrid_0098] [EnzoGrid_0099] [EnzoGrid_0100] [EnzoGrid_0101] [EnzoGrid_0102] [EnzoGrid_0103] [EnzoGrid_0104] [EnzoGrid_0105] Chunk 7 14 [EnzoGrid_0004] [EnzoGrid_0106] [EnzoGrid_0107] [EnzoGrid_0108] [EnzoGrid_0109] [EnzoGrid_0111] [EnzoGrid_0112] [EnzoGrid_0113] [EnzoGrid_0114] [EnzoGrid_0115] [EnzoGrid_0116] [EnzoGrid_0117] [EnzoGrid_0118] [EnzoGrid_0119] Chunk 8 26 [EnzoGrid_0003] [EnzoGrid_0120] [EnzoGrid_0121] [EnzoGrid_0122] [EnzoGrid_0123] [EnzoGrid_0124] [EnzoGrid_0125] [EnzoGrid_0126] [EnzoGrid_0127] [EnzoGrid_0128] [EnzoGrid_0129] [EnzoGrid_0130] [EnzoGrid_0131] [EnzoGrid_0132] [EnzoGrid_0133] [EnzoGrid_0134] [EnzoGrid_0135] [EnzoGrid_0136] [EnzoGrid_0137] [EnzoGrid_0138] [EnzoGrid_0139] [EnzoGrid_0140] [EnzoGrid_0141] [EnzoGrid_0142] [EnzoGrid_0143] [EnzoGrid_0144] Chunk 9 30 [EnzoGrid_0002] [EnzoGrid_0145] [EnzoGrid_0146] [EnzoGrid_0147] [EnzoGrid_0148] [EnzoGrid_0149] [EnzoGrid_0150] [EnzoGrid_0151] [EnzoGrid_0152] [EnzoGrid_0153] [EnzoGrid_0154] [EnzoGrid_0155] [EnzoGrid_0156] [EnzoGrid_0157] [EnzoGrid_0158] [EnzoGrid_0159] [EnzoGrid_0160] [EnzoGrid_0161] [EnzoGrid_0162] [EnzoGrid_0163] [EnzoGrid_0164] [EnzoGrid_0165] [EnzoGrid_0166] [EnzoGrid_0167] [EnzoGrid_0168] [EnzoGrid_0169] [EnzoGrid_0170] [EnzoGrid_0171] [EnzoGrid_0172] [EnzoGrid_0173] (In general, this is about as deep as it goes, although in principle we could do lots more sub-chunking.) This lets you chunk over IO, then chunk over individual objects, and even request things like the number of ghost zones you need in that sub-chunking.\nChunks can retain state during a long-lived IO task We can also store field parameters and reset them during an individual chunking operation. So for instance, we could have a custom field that requires one field parameter that is then swapped out during the next level of chunking.\nThis feature is probably not used much.\nIteration and Chunks But here\u0026rsquo;s the issue we run into, which actually shows up whenever an error is raised during a chunking operation.\nAll of this is done via generator expressions. This seemed like the right thing to do! Just yield everywhere! It was so hip.\nBut, there\u0026rsquo;s an even more problematic part: often, the generator expressions get unrolled into lists anyway. And, it turns out, I can\u0026rsquo;t blame anybody else for this: this particular core element of yt was something I not only put in, but something I felt rather self-satisfied about.\nLet\u0026rsquo;s take a look at this in the routine that does chunking for the io type in grid index datasets:\nds.index._chunk_io?? Signature: ds.index._chunk_io( dobj, cache=True, local_only=False, preload_fields=None, chunk_sizing='auto', ) Docstring: \u0026lt;no docstring\u0026gt; Source: def _chunk_io(self, dobj, cache=True, local_only=False, preload_fields=None, chunk_sizing=\u0026quot;auto\u0026quot;): # local_only is only useful for inline datasets and requires # implementation by subclasses. if preload_fields is None: preload_fields = [] preload_fields, _ = self._split_fields(preload_fields) gfiles = defaultdict(list) gobjs = getattr(dobj._current_chunk, \u0026quot;objs\u0026quot;, dobj._chunk_info) fast_index = dobj._current_chunk._fast_index for g in gobjs: # Force to be a string because sometimes g.filename is None. gfiles[str(g.filename)].append(g) # We can apply a heuristic here to make sure we aren't loading too # many grids all at once. if chunk_sizing == \u0026quot;auto\u0026quot;: chunk_ngrids = len(gobjs) if chunk_ngrids \u0026gt; 0: nproc = np.float(ytcfg.getint(\u0026quot;yt\u0026quot;, \u0026quot;__global_parallel_size\u0026quot;)) chunking_factor = np.ceil(self._grid_chunksize*nproc/chunk_ngrids).astype(\u0026quot;int\u0026quot;) size = max(self._grid_chunksize//chunking_factor, 1) else: size = self._grid_chunksize elif chunk_sizing == \u0026quot;config_file\u0026quot;: size = ytcfg.getint(\u0026quot;yt\u0026quot;, \u0026quot;chunk_size\u0026quot;) elif chunk_sizing == \u0026quot;just_one\u0026quot;: size = 1 elif chunk_sizing == \u0026quot;old\u0026quot;: size = self._grid_chunksize else: raise RuntimeError(\u0026quot;%s is an invalid value for the 'chunk_sizing' argument.\u0026quot; % chunk_sizing) for fn in sorted(gfiles): gs = gfiles[fn] for grids in (gs[pos:pos + size] for pos in range(0, len(gs), size)): dc = YTDataChunk(dobj, \u0026quot;io\u0026quot;, grids, self._count_selection(dobj, grids), cache = cache, fast_index = fast_index) # We allow four full chunks to be included. with self.io.preload(dc, preload_fields, 4.0 * size): yield dc File: ~/yt/yt/yt/geometry/grid_geometry_handler.py Type: method There\u0026rsquo;s lots going on here, so I\u0026rsquo;ll just grab a few of the most important lines. Specifically, I want to highlight this:\nfor fn in sorted(gfiles): gs = gfiles[fn] for grids in (gs[pos:pos + size] for pos in range(0, len(gs), size)): dc = YTDataChunk(dobj, \u0026quot;io\u0026quot;, grids, self._count_selection(dobj, grids), cache = cache, fast_index = fast_index) # We allow four full chunks to be included. with self.io.preload(dc, preload_fields, 4.0 * size): yield dc The upshot of this is that we first sorted our grids by which file they\u0026rsquo;re in (on the assumption that we probably want to minimize open/close operations), but then in that, we split them up based on the grid counts we want in each chunk, and then we spit out the chunks (with optional preloading of data) to whatever consumes them.\nAs long as we don\u0026rsquo;t do any preloading it\u0026rsquo;s alright for parallel IO, but we\u0026rsquo;re not going to see any of the benefits of this if we ever turn this into a list.\nNow for the Enzo frontend specifically, let\u0026rsquo;s see how the IO routine works:\nds.index.io._read_fluid_selection?? Signature: ds.index.io._read_fluid_selection(chunks, selector, fields, size) Docstring: \u0026lt;no docstring\u0026gt; Source: def _read_fluid_selection(self, chunks, selector, fields, size): # This function has an interesting history. It previously was mandate # to be defined by all of the subclasses. But, to avoid having to # rewrite a whole bunch of IO handlers all at once, and to allow a # better abstraction for grid-based frontends, we're now defining it in # the base class. rv = {} nodal_fields = [] for field in fields: finfo = self.ds.field_info[field] nodal_flag = finfo.nodal_flag if np.any(nodal_flag): num_nodes = 2**sum(nodal_flag) rv[field] = np.empty((size, num_nodes), dtype=\u0026quot;=f8\u0026quot;) nodal_fields.append(field) else: rv[field] = np.empty(size, dtype=\u0026quot;=f8\u0026quot;) ind = {field: 0 for field in fields} for field, obj, data in self.io_iter(chunks, fields): if data is None: continue if isinstance(selector, GridSelector) and field not in nodal_fields: ind[field] += data.size rv[field] = data.copy() else: ind[field] += obj.select(selector, data, rv[field], ind[field]) return rv File: ~/yt/yt/yt/utilities/io_handler.py Type: method This is in the base class for the IO handler; some of the grid-based frontends implement it. In this particular case we aren\u0026rsquo;t unrolling the generator, but you can see some of the issues here anyway: we need to know a fair bit about the IO method (thus the io_iter method, which I will show below) and we need to do a lot of obj.select and whatnot.\nThis isn\u0026rsquo;t terribly efficient, and it also means that since we are yielding a generator expression from within a generator expression, we end up having a nested set of loops that don\u0026rsquo;t know their sizes or allow seeking in their stream of yields.\nThis makes interoperating with something like dask \u0026ndash; which works best when it knows the sizes and shapes and can do its own distribution \u0026ndash; much more challenging. And it also means that we have a few layers of relatively opaque routines that conspire to keep us a ways from the file-based abstraction.\nLet\u0026rsquo;s look at the io_iter function to see how it works for Enzo. You can see that it does do a few fun things; most importantly, it keeps the file handle open if it can. This can save a surprising amount of time on parallel file systems, as it reduces the number of metadata lookups necessary.\nds.index.io.io_iter?? Signature: ds.index.io.io_iter(chunks, fields) Docstring: \u0026lt;no docstring\u0026gt; Source: def io_iter(self, chunks, fields): h5_dtype = self._field_dtype for chunk in chunks: fid = None filename = -1 for obj in chunk.objs: if obj.filename is None: continue if obj.filename != filename: # Note one really important thing here: even if we do # implement LRU caching in the _read_obj_field function, # we'll still be doing file opening and whatnot. This is a # problem, but one we can return to. if fid is not None: fid.close() fid = h5py.h5f.open(b(obj.filename), h5py.h5f.ACC_RDONLY) filename = obj.filename for field in fields: nodal_flag = self.ds.field_info[field].nodal_flag dims = obj.ActiveDimensions[::-1] + nodal_flag[::-1] data = np.empty(dims, dtype=h5_dtype) yield field, obj, self._read_obj_field( obj, field, (fid, data)) if fid is not None: fid.close() File: ~/yt/yt/yt/frontends/enzo/io.py Type: method So to recap, right now: making chunks work nicely with non-yt operations is tricky because of some early design decisions\nNext Up In the next blog post, I\u0026rsquo;m going to present a bit about:\nHow particle IO is handled \u0026ndash; and the differences between grid IO (which has lots of differently-shaped chunks) and particle IO Some efforts to refactor particle IO (before it gets released!) A future for how to make all this stuff work better with dask (yes, really, I promise) ","date":1560825129,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560825481,"objectID":"83bc013dd9d9737134873e743bb020e7","permalink":"https://matthewturk.github.io/post/refactoring-yt-frontends-part3/","publishdate":"2019-06-17T22:32:09-04:00","relpermalink":"/post/refactoring-yt-frontends-part3/","section":"post","summary":"Welcome to part 3 of a series on how yt deals with data and the ways that helps and hinders things! This time, I am going to describe what \u0026ldquo;chunks\u0026rdquo; of data (YTDataChunk) in yt are, and a few characteristics of them that wouldn\u0026rsquo;t be obvious from the previous blog posts.","tags":[],"title":"Refactoring yt Frontends - Part 3","type":"post"},{"authors":[],"categories":[],"content":"SIDE NOTE: I intended for this blog post to be a bit shorter than it turned out, and for it to cover some things it \u0026hellip; didn\u0026rsquo;t! So it looks like there\u0026rsquo;ll be a part three in the series.\nOperations on Data Objects In my previous post, I walked through a few aspects of how the chunking system in yt works, mostly focusing on the \u0026quot;io\u0026quot; style of chunking, where the order in which data arrives is not important. This style of chunking lends itself very easily to parallelism, as well as dynamic chunk-sizing; we can see this in how operations such as .max() operate on a data object in yt.\nimport yt ds = yt.load(\u0026quot;data/IsolatedGalaxy/galaxy0030/galaxy0030\u0026quot;) dd = ds.r[:,:,:] yt : [INFO ] 2019-06-10 12:59:13,433 Parameters: current_time = 0.0060000200028298 yt : [INFO ] 2019-06-10 12:59:13,434 Parameters: domain_dimensions = [32 32 32] yt : [INFO ] 2019-06-10 12:59:13,435 Parameters: domain_left_edge = [0. 0. 0.] yt : [INFO ] 2019-06-10 12:59:13,437 Parameters: domain_right_edge = [1. 1. 1.] yt : [INFO ] 2019-06-10 12:59:13,439 Parameters: cosmological_simulation = 0.0 Parsing Hierarchy : 100%|██████████| 173/173 [00:00\u0026lt;00:00, 5931.42it/s] yt : [INFO ] 2019-06-10 12:59:13,487 Gathering a field list (this may take a moment.) max_vals = dd.max([\u0026quot;density\u0026quot;, \u0026quot;temperature\u0026quot;, \u0026quot;velocity_magnitude\u0026quot;]) print(max_vals) (7.73426503924e-24 g/cm**3, 24826104.0 K, 86290042.8768639 cm/s) I want to highlight something here which sometimes slips by \u0026ndash; if you were to access the array hanging off an object like dd, like this:\ndd[\u0026quot;density\u0026quot;] YTArray([4.92775113e-31, 4.94005233e-31, 4.93824694e-31, ..., 1.12879234e-25, 1.59561490e-25, 1.09824903e-24]) g/cm**3 The entire array is loaded into memory. This is done through a different chunk style, \u0026quot;all\u0026quot;, which pre-allocates an array and then loads the whole thing into memory in a single go. I should note that there are a few reasons that this needs to always be provided in the same order as the \u0026quot;io\u0026quot; chunking style, which has presented some fun struggles in refactoring that I may mention later on.\nBut above, we aren\u0026rsquo;t accessing dd[\u0026quot;density\u0026quot;].max() and instead are accessing dd.max(\u0026quot;density\u0026quot;) (along with two other fields, temperature and the magnitude of velocity.) This operation, inspired by the numpy / xarray syntax, iterates over chunks and computes the running max.\nThere are a few other fun operations that I mentioned last time like argmax and whatnot, but for now let\u0026rsquo;s just look at max. While the syntactic sugar for calling dd.max() is reasonably recent, the underpinning functionality dates back about a decade and has, from the start, been MPI-parallel. It hasn\u0026rsquo;t always been elegant, but it\u0026rsquo;s been parallel and memory-conservative.\nSo how does .max() (and, its older form, .quantities[\u0026quot;MaximumValue\u0026quot;]()) work? Let\u0026rsquo;s take a look at the source.\ndd.max?? Signature: dd.max(field, axis=None) Source: def max(self, field, axis=None): r\u0026quot;\u0026quot;\u0026quot;Compute the maximum of a field, optionally along an axis. This will, in a parallel-aware fashion, compute the maximum of the given field. Supplying an axis will result in a return value of a YTProjection, with method 'mip' for maximum intensity. If the max has already been requested, it will use the cached extrema value. Parameters ---------- field : string or tuple field name The field to maximize. axis : string, optional If supplied, the axis to project the maximum along. Returns ------- Either a scalar or a YTProjection. Examples -------- \u0026gt;\u0026gt;\u0026gt; max_temp = reg.max(\u0026quot;temperature\u0026quot;) \u0026gt;\u0026gt;\u0026gt; max_temp_proj = reg.max(\u0026quot;temperature\u0026quot;, axis=\u0026quot;x\u0026quot;) \u0026quot;\u0026quot;\u0026quot; if axis is None: rv = () fields = ensure_list(field) for f in fields: rv += (self._compute_extrema(f)[\u001b[0;36m1],) if len(fields) == \u001b[0;36m1: return rv[\u001b[0;36m0] else: return rv elif axis in self.ds.coordinates.axis_name: r = self.ds.proj(field, axis, data_source=self, method=\u0026quot;mip\u0026quot;) return r else: raise NotImplementedError(\u0026quot;Unknown axis %s\u0026quot; % axis) File: ~/yt/yt/yt/data_objects/data_containers.py Type: method (One fun bit here is that if you supply an axis and it\u0026rsquo;s a spatial axis, this will project along the axis.)\nLooks like it calls ._compute_extrema so let\u0026rsquo;s take a look there:\ndd._compute_extrema?? Signature: dd._compute_extrema(field) Docstring: \u0026lt;no docstring\u0026gt; Source: def _compute_extrema(self, field): if self._extrema_cache is None: self._extrema_cache = {} if field not in self._extrema_cache: # Note we still need to call extrema for each field, as of right # now mi, ma = self.quantities.extrema(field) self._extrema_cache[field] = (mi, ma) return self._extrema_cache[field] File: ~/yt/yt/yt/data_objects/data_containers.py Type: method (Fun fact: until I saw the source code right now, I was prepared to say that it computed all the extrema in a single go. Glad there\u0026rsquo;s a backspace key. I should probably file an issue.)\nThis calls self.quantities.extrema for each field, since it\u0026rsquo;s nearly just as cheap to do both min and max in a single pass, and sometimes folks\u0026rsquo;ll want both.\nSo we\u0026rsquo;re starting to see the underpinnings here \u0026ndash; .quantities is where lots of the fun things happen. What is it?\ndd.quantities.extrema?? Signature: dd.quantities.extrema(fields, non_zero=False) Type: Extrema String form: \u0026lt;yt.data_objects.derived_quantities.Extrema object at 0x7db454d7a2e8\u0026gt; File: ~/yt/yt/yt/data_objects/derived_quantities.py Source: class Extrema(DerivedQuantity): r\u0026quot;\u0026quot;\u0026quot; Calculates the min and max value of a field or list of fields. Returns a YTArray for each field requested. If one, a single YTArray is returned, if many, a list of YTArrays in order of field list is returned. The first element of each YTArray is the minimum of the field and the second is the maximum of the field. Parameters ---------- fields The field or list of fields over which the extrema are to be calculated. non_zero : bool If True, only positive values are considered in the calculation. Default: False Examples -------- \u0026gt;\u0026gt;\u0026gt; ds = load(\u0026quot;IsolatedGalaxy/galaxy0030/galaxy0030\u0026quot;) \u0026gt;\u0026gt;\u0026gt; ad = ds.all_data() \u0026gt;\u0026gt;\u0026gt; print ad.quantities.extrema([(\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;), ... (\u0026quot;gas\u0026quot;, \u0026quot;temperature\u0026quot;)]) \u0026quot;\u0026quot;\u0026quot; def count_values(self, fields, non_zero): self.num_vals = len(fields) * \u001b[0;36m2 def __call__(self, fields, non_zero = False): fields = ensure_list(fields) rv = super(Extrema, self).__call__(fields, non_zero) if len(rv) == \u001b[0;36m1: rv = rv[\u001b[0;36m0] return rv def process_chunk(self, data, fields, non_zero): vals = [] for field in fields: field = data._determine_fields(field)[\u001b[0;36m0] fd = data[field] if non_zero: fd = fd[fd \u0026gt; \u001b[0;36m0.0] if fd.size \u0026gt; \u001b[0;36m0: vals += [fd.min(), fd.max()] else: vals += [array_like_field(data, HUGE, field), array_like_field(data, -HUGE, field)] return vals def reduce_intermediate(self, values): # The values get turned into arrays here. return [self.data_source.ds.arr([mis.min(), mas.max()]) for mis, mas in zip(values[::\u001b[0;36m2], values[\u001b[0;36m1::\u001b[0;36m2])] Call docstring: Calculate results for the derived quantity Ah, this is starting to make sense!\nAll the DerivedQuantity objects\nWhat all do we have?\ndd.quantities.keys() dict_keys(['WeightedAverageQuantity', 'TotalQuantity', 'TotalMass', 'CenterOfMass', 'BulkVelocity', 'WeightedVariance', 'AngularMomentumVector', 'Extrema', 'SampleAtMaxFieldValues', 'MaxLocation', 'SampleAtMinFieldValues', 'MinLocation', 'SpinParameter']) Looking at these, there\u0026rsquo;s likely a common theme that is pretty obvious \u0026ndash; they\u0026rsquo;re all pretty easily parallelizable things! Sure, there might need to be some reductions at the end, but these are all pretty straightforward combinations of fields and parameters.\nThe way the base class works is interesting, and we can use that to break down what is going on here in a way that demonstrates how this relies on chunking:\nyt.data_objects.derived_quantities.DerivedQuantity?? Init signature: yt.data_objects.derived_quantities.DerivedQuantity(data_source) Docstring: \u0026lt;no docstring\u0026gt; Source: class DerivedQuantity(ParallelAnalysisInterface): num_vals = -\u001b[0;36m1 def __init__(self, data_source): self.data_source = data_source def count_values(self, *args, **kwargs): return def __call__(self, *args, **kwargs): \u0026quot;\u0026quot;\u0026quot;Calculate results for the derived quantity\u0026quot;\u0026quot;\u0026quot; # create the index if it doesn't exist yet self.data_source.ds.index self.count_values(*args, **kwargs) chunks = self.data_source.chunks([], chunking_style=\u0026quot;io\u0026quot;) storage = {} for sto, ds in parallel_objects(chunks, -\u001b[0;36m1, storage = storage): sto.result = self.process_chunk(ds, *args, **kwargs) # Now storage will have everything, and will be done via pickling, so # the units will be preserved. (Credit to Nathan for this # idea/implementation.) values = [ [] for i in range(self.num_vals) ] for key in sorted(storage): for i in range(self.num_vals): values[i].append(storage[key][i]) # These will be YTArrays values = [self.data_source.ds.arr(values[i]) for i in range(self.num_vals)] values = self.reduce_intermediate(values) return values def process_chunk(self, data, *args, **kwargs): raise NotImplementedError def reduce_intermediate(self, values): raise NotImplementedError File: ~/yt/yt/yt/data_objects/derived_quantities.py Type: RegisteredDerivedQuantity Subclasses: WeightedAverageQuantity, TotalQuantity, CenterOfMass, BulkVelocity, WeightedVariance, AngularMomentumVector, Extrema, SampleAtMaxFieldValues, SpinParameter The key thing I want to highlight here is that this is rather simple in concept; the chunks are iterated over in parallel (via the parallel_objects routine, which parcels them out to different processors), processed, and then the reduction happens through reduce_intermediate.\nThere are a few things to note here \u0026ndash; this is actually units-aware, which means that even if you\u0026rsquo;ve got (for some reason) cm for a quantity on one processor and km on another, it will correctly convert them. The other is that the set up is such that only the process_chunk and reduce_intermediate operations need to be implemented, along with setting some properties.\nBut, we\u0026rsquo;re getting a bit far away from the topic at hand, which is why how chunking is set up can cause some issues with exposing data to dask. And so I want to return to the notion of the \u0026quot;io\u0026quot; chunking and how this works for differently indexed datasets.\nFine- and Coarse-grained Indexing What yt does during the selection of data is key to how it thinks about the processings of that data. The way that data can be provided to yt takes several forms:\nRegularly structured grids and grid based data, where there may be overlapping regions (typically with one \u0026ldquo;authoritative source of truth\u0026rdquo; as in adaptive mesh refinement) Irregular grids, where the distance between points may vary along each spatial axis Unstructured mesh, where the data arrives in hexahedra, tetrahedra, etc, and there is typically a well-defined form for evaluating field values internal to each polyhedra Discrete, or particle-based datasets, where each point is sampled at some location that we don\u0026rsquo;t know a priori \u0026ndash; for instance, N-body simulations Octree or block-structured data, which can in some cases be thought of as a special-case of grid based data but that follows a more regular form Several of these have a common trait that comes in quite handy for yt \u0026ndash; namely, that the index of the data occupies considerably less memory than the data itself.\nGrid Indexing For instance, when dealing with a grid of data, typically that grid can be defined by a set of properties such as:\n\u0026ldquo;origin\u0026rdquo; corner of the grid (\u0026ldquo;left edge\u0026rdquo;) \u0026ldquo;terminal\u0026rdquo; corner of the grid (\u0026ldquo;right edge\u0026rdquo;) dimensions along each axis if irregular, the cell-spacing along each axis There are of course a handful of other attributes that might be useful (and which we can sometimes deduce) but these are the basics. If we imagine that each of these requires 64-bits per axis per value, a standard (regular) grid requires 576 bits, or 72 bytes. If we were storing the actual value locations, each would require 3 64-bit numbers \u0026ndash; which means that as soon as we were storing 3 of them, we would\n(Of course, one probably doesn\u0026rsquo;t need to store dimensions as 64 bits, and there are also probably some other ways to reduce the info necessary, but as straw-person arguments go, this isn\u0026rsquo;t so bad.)\nWhat we can get to with this is that for grid and other regular datasets, it\u0026rsquo;s reasonably cheap to index the data. So when we create a data object, for instance:\nsp = ds.sphere(\u0026quot;center\u0026quot;, (100.0, \u0026quot;kpc\u0026quot;)) yt can determine without touching the disk how many grid cells intersect it, and thus it can pre-allocate arrays of the correct size and fill them in progressively, in whatever fashion it deems best for IO purposes.\nThis isn\u0026rsquo;t without cost \u0026ndash; computing the intersections can be quite costly, and so we do some things to cache those. (The cost/benefit of caching often bites us when we are dealing with large unigrid datasets, though.) This was all designed to prevent having to call a big np.concatenate at some point in the operation when chunking based on \u0026quot;all\u0026quot;, but it\u0026rsquo;s not always obvious to me that the balance was correctly struck here.\nWhen an object is created, no selection is conducted until a field is requested. At some point in the call stack once a field is asked for, the function index._identify_base_chunk is called. This is where things are different for particles, but we\u0026rsquo;ll get to that later.\nParticle Indexing When dealing with particles, our indexing requirements are very different. Here, the cost of storing the index values is very high \u0026ndash; but, we also don\u0026rsquo;t want to have to perform too much IO. So we\u0026rsquo;re stuck minimizing how much IO we do, while also minimizing the amount of information we store in-memory once we \u0026ldquo;index\u0026rdquo; a dataset.\nIn yt-4.0, we accomplish this through the use of bitmap indices, which I described a little bit in the first post. The basic idea of this is that each \u0026ldquo;file\u0026rdquo; (which can be subsets of a single file, and is better thought of as an IO block of some type) is assigned some unique ID. All the files are iterated over and for each discrete point included in that file, an index into a space-filling curve is generated. We use a resonably coarse space filling curve for the first iteration \u0026ndash; say, a level 2 curve \u0026ndash; and that allows ambiguities. This is essentially a binning operation.\n(Incidentally, we often use Morton Z-Ordering because it\u0026rsquo;s just easier to explain. We might get better compression if we used Hilbert since consecutive values may be more likely to be identical.)\nAt the end of the first iteration, we have a key-value store of bitarrays, where the key is the file ID and the value is a set of 1\u0026rsquo;s and 0\u0026rsquo;s, where a 1 indicates that a particle is found in a given region identified by the space-filling curve index corresponding with that 1\u0026rsquo;s index in the array. So, for instance, if we had a level 3 index, we would have a set of bitarrays that looked like:\n001 000 101 010 011 011 ... So, if we read from left-to-right, the first file has particles that live in (zero-indexed) indices 2, 6 and 8. The second file has particles in indices 1, 4, 5, 7 and 8.\nIf we know that our selector only intersects areas touched by index 2, then we only have to read from the first file.\nThis would work great if we had particles that were distributed pretty homogeneously on large scales, but in many cases, we don\u0026rsquo;t. Sometimes when particles are written to disk they are sorted on some high-order index and then written out in that order. What yt does is perform a secondary, as-needed indexing based on where there are \u0026ldquo;collisions\u0026rdquo; \u0026ndash; i.e., ambiguities. A set of logical operations is performed across all the bitarrays to identify where multiple files overlap; following this, a second round of indexing is conducted at a much higher spatial order.\nIn doing this, we are able to pinpoint with reasonably high precision the file or files that need to be read to get data from a given selector, and minimize very precisely the amount of over-reading that is done.\nUnfortunately, this doesn\u0026rsquo;t give us the ability to explicitly allocate arrays of the correct size. (And, the memory overhead of regaining that ability would be quite high.) But as we saw above, yt doesn\u0026rsquo;t want to do big concatenation operations! So it does the thing I really wish it didn\u0026rsquo;t, which is \u0026hellip; it reads all the position data in IO chunks, figures out how big it is (which only requires a running tally, not a set of allocated arrays), then allocates and fills that single big array.\nThis isn\u0026rsquo;t really that efficient, and it arises from the case where the indexing is comparatively cheap.\nBut all of this arises out of the design decision that we need to optimize for the case that we want a single big array, rather than a bunch of small arrays \u0026ndash; i.e., for the case of:\nds.r[:][\u0026quot;density\u0026quot;].max() as opposed to\nds.r[:].max(\u0026quot;density\u0026quot;) \u0026hellip;didn\u0026rsquo;t you say you\u0026rsquo;d be talking about Dask? Well, this is where dask comes in! And, it\u0026rsquo;s also why interfacing to dask is a bit tricky \u0026ndash; because we do a lot of work ahead of time before allocating any arrays, and then we get rid of the information generated during that work.\nIn an ideal world, what we would be able to do is to export a data object (such as a sphere or cylinder or rectangular prism) and a field-type (so we knew if it was a vector, or particles, or nodal/zonal data) as a dask array. For instance, if instead of returning an array (specifically, a YTArray or unyt_array) when we accessed sp[\u0026quot;density\u0026quot;], it returned a DaskArray, we would open up a number of new and interesting techniques.\nBut to do that, we need to be able to know in advance the chunk sizes, and more to the point, we need to be able to specify a function that returns each chunk uniquely.\nNext Entry: Iterables and IO Turns out, I thought I\u0026rsquo;d be done with this entry a lot sooner than I was!\nIn the next blog post, which hopefully will take less than the 8 days this one did, I\u0026rsquo;ll talk about why this is (currently) hard, how to fix that, and what we\u0026rsquo;re doing to fix it.\n","date":1560189573,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560189890,"objectID":"3be15376e926cc94641a4a597673f5ed","permalink":"https://matthewturk.github.io/post/refactoring-yt-frontends-part2/","publishdate":"2019-06-10T12:59:33-05:00","relpermalink":"/post/refactoring-yt-frontends-part2/","section":"post","summary":"SIDE NOTE: I intended for this blog post to be a bit shorter than it turned out, and for it to cover some things it \u0026hellip; didn\u0026rsquo;t! So it looks like there\u0026rsquo;ll be a part three in the series.","tags":[],"title":"Refactoring yt Frontends - Part 2","type":"post"},{"authors":[],"categories":null,"content":"","date":1559684783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560367010,"objectID":"211427e115ea8ae0e1122db98bd34924","permalink":"https://matthewturk.github.io/talk/2019-06-04-odia-yt/","publishdate":"2019-06-04T14:46:23-07:00","relpermalink":"/talk/2019-06-04-odia-yt/","section":"talk","summary":"What have your experiences been with digital infrastructure development?","tags":[],"title":"yt: Hard Questions","type":"talk"},{"authors":[],"categories":[],"content":"In the still-in-development version of yt (4.0), the way that particles are handled has been redesigned from the ground up.\nThe current version of yt (3.x) utilizes an octree-based approach for meshing the particles, although not for indexing them \u0026ndash; which presents some problems when doing subsets of particles, as well as when doing visualizations that rely on an implicit meshing. The main result is that, in general, particle visualizations in yt 3.x aren\u0026rsquo;t that great, and are underresolved.\nIn yt 4.0, the particle system has been reimplemented to use EWAH bitmap indices (for more info, see Daniel Lemire\u0026rsquo;s EWAHBoolArray repository) to track which \u0026ldquo;regions\u0026rdquo; of files correspond to particular spatial regions, as designated by indices in a space-filling curve. Things are now orders of magnitude faster to load, to subset, and to visualize \u0026ndash; and the memory overhead is so much lower!\nThis work was led by Nathan Goldbaum and Meagan Lang, with crucial contributions from the rest of the yt community, including feedback and bugfixes from Bili Dong and Cameron Hummels.\nRecently, I\u0026rsquo;ve been exploring using a different array backend in yt, right now focusing on dask. While yt does lots of MPI-parallel operations, much of what we do with these has to be hand-programmed \u0026ndash; so when you implement a new DerivedQuantity (i.e., stuff like calling min on a data object) you have to jump through a few hoops related to intermediate values and the like. Plus, dask seems to be everywhere, and so if we exported to dask arrays or somehow interoperated better with it, we\u0026rsquo;d be able to interoperate with lots of the rest of the ecosystem more easily.\nUnfortunately, there\u0026rsquo;s a bit of an impedance mismatch which \u0026hellip; has made this more difficult than I\u0026rsquo;d like.\nReading Data Before getting too much further, though, I\u0026rsquo;m going to go through a bit about how yt thinks about \u0026ldquo;chunking\u0026rdquo; data.\nThe fundamental thing that yt does is index data. (Well, that, and take a while to compile all the Cython code.) Processing of the data is all layered on top of that \u0026ndash; including some pretty cool semantics-of-data and units, visualization, etc. The main thing is that if you do a subset, it knows where to go to grab that subset of data, and if you want to do something that touches everything, it\u0026rsquo;ll do its best to reduce the number of times data is loaded off disk in service of that.\nWe do this with a \u0026ldquo; chunking\u0026rdquo; system, which is implemented differently if your data is discrete (i.e., particles), mesh-based, and so on.\nSo to show what the problem is, I\u0026rsquo;m going to load up a dataset from the FIRE project.\nimport yt ds = yt.load(\u0026quot;data/FIRE_M12i_ref11/snapshot_600.hdf5\u0026quot;) yt : [INFO ] 2019-06-02 16:02:22,303 Calculating time from 1.000e+00 to be 4.355e+17 seconds yt : [INFO ] 2019-06-02 16:02:22,304 Assuming length units are in kpc/h (comoving) yt : [INFO ] 2019-06-02 16:02:22,337 Parameters: current_time = 4.3545571088051386e+17 s yt : [INFO ] 2019-06-02 16:02:22,338 Parameters: domain_dimensions = [1 1 1] yt : [INFO ] 2019-06-02 16:02:22,339 Parameters: domain_left_edge = [0. 0. 0.] yt : [INFO ] 2019-06-02 16:02:22,341 Parameters: domain_right_edge = [60000. 60000. 60000.] yt : [INFO ] 2019-06-02 16:02:22,342 Parameters: cosmological_simulation = 1 yt : [INFO ] 2019-06-02 16:02:22,343 Parameters: current_redshift = 0.0 yt : [INFO ] 2019-06-02 16:02:22,344 Parameters: omega_lambda = 0.728 yt : [INFO ] 2019-06-02 16:02:22,344 Parameters: omega_matter = 0.272 yt : [INFO ] 2019-06-02 16:02:22,345 Parameters: omega_radiation = 0.0 yt : [INFO ] 2019-06-02 16:02:22,347 Parameters: hubble_constant = 0.702 At this point yt has done a tiny little bit of reading of the data \u0026ndash; just enough to figure out some of the metadata. It hasn\u0026rsquo;t indexed anything yet or read any of the actual data fields off of disk.\nNow let\u0026rsquo;s make a plot of the gas density, integrated over the z axis of the simulation. Keep in mind that in doing this, it will have to read all the gas particles and smooth them onto a buffer. The first time this gets run, an index is generated and then stored to disk. More on that in a moment.\nI\u0026rsquo;m going to use ds.r[:] here for \u0026ldquo;dataset region, but the whole thing\u0026rdquo; and then I call integrate on it and specify the field to integrate. Then, I plot it.\np=ds.r[:].integrate(\u0026quot;density\u0026quot;, axis=\u0026quot;z\u0026quot;).plot((\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;)) yt : [INFO ] 2019-06-02 16:02:22,484 Allocating for 4.787e+06 particles Loading particle index: 100%|██████████| 10/10 [00:00\u0026lt;00:00, 817.25it/s] yt : [INFO ] 2019-06-02 16:02:23,623 xlim = 0.000000 60000.000000 yt : [INFO ] 2019-06-02 16:02:23,623 ylim = 0.000000 60000.000000 yt : [INFO ] 2019-06-02 16:02:23,633 Making a fixed resolution buffer of (('gas', 'density')) 800 by 800 (All that empty space is because there are only gas particles in the middle of the dataset!)\nThe first time any data needs to be read from a particle dataset, yt will construct an in-memory index of the data on disk; by default, it will store this in a sidecar file, so the next time that the dataset is read it does not need to be generated again.\nThe way the bitmap indices work is really fun, but that deserves its own blog post. It suffices to say that the indexing helps to figure out both which files to read, and which subsets of those files to read, since we don\u0026rsquo;t assume that the particles are sorted in any way. (Mostly because each code tends to sort the particles in its own way!)\nNow, for projecting over the whole domain, it\u0026rsquo;s not that big a deal to read everything, since we have to anyway, but if we did a subset it could dramatically reduce the IO necessary, and it also keeps much less data resident in memory than the old implementation.\nContinuing on, let\u0026rsquo;s say that we now want to center at a different location. We\u0026rsquo;d figure out the most dense point, and then set our center.\nc = ds.r[:].argmax((\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;)) (One thing this next set of code highlights is that, in general, how we handle centers in yt is a bit clumsy at times. Writing this blog post led me to filing an issue which may or may not get any traction or support.)\np.set_origin(\u0026quot;center-window\u0026quot;) p.set_center((c[0], c[1])) p.zoom(25) p.set_zlim((\u0026quot;gas\u0026quot;,\u0026quot;density\u0026quot;), 1e-6, 1e-3) yt : [INFO ] 2019-06-02 16:02:25,607 xlim = -713.911179 59286.088821 yt : [INFO ] 2019-06-02 16:02:25,611 ylim = 1049.283652 61049.283652 yt : [INFO ] 2019-06-02 16:02:25,619 Making a fixed resolution buffer of (('gas', 'density')) 800 by 800 So, we can visualize now, and it\u0026rsquo;s faster than it was before, and we also get much better results. Great. So why am I belaboring this point?\nIt\u0026rsquo;s because in the background, yt is queryin a data object to see which items to read off disk, then it is reading those items off disk. In this particular instance, it is doing what we call \u0026ldquo;io\u0026rdquo; chunking \u0026ndash; this means to use whatever type of hinting is best to get the most efficient ordering it knows how. Among other things, yt will try to minimize the number of times it opens a file, it seeks in a file, and it tries to keep the memory allocation count as low as possible.\n(I\u0026rsquo;ll write more on this last point later \u0026ndash; much of what yt does to index in yt-3.x and yt-4.0 is designed to keep the number of allocated arrays in the IO routines as low as possible, and to avoid any expensive concatenation or subselection operations. It turns out, this is \u0026hellip; not as big a deal as thought when this was made a design principle. And in general, it leads to a lot more floating point operations than we would like, and sometimes more stuff in memory, too.)\nAnd, so, uh, \u0026ldquo;chunking\u0026rdquo; is\u0026hellip;? We can figure out how yt chunks this data by, well, asking it to do it manually! Every data object presents a chunks interface which is a generator that modifies its internal state and then yields itself. For instance:\ndd = ds.all_data() for chunk in dd.chunks([], \u0026quot;io\u0026quot;): print(chunk[\u0026quot;particle_ones\u0026quot;].size) 1048576 885527 753678 524288 317696 262144 262144 262144 262144 208609 I mentioned that this generator yields itself; this is true. But the internal state is modified to store where we are in the iteration, along with things like the parameters for derived fields and the like. The source for this looks like this:\nfrom yt.data_objects.data_containers import YTSelectionContainer YTSelectionContainer.chunks?? Signature: YTSelectionContainer.chunks(self, fields, chunking_style, **kwargs) Docstring: \u0026lt;no docstring\u0026gt; Source: def chunks(self, fields, chunking_style, **kwargs): # This is an iterator that will yield the necessary chunks. self.get_data() # Ensure we have built ourselves if fields is None: fields = [] # chunk_ind can be supplied in the keyword arguments. If it's a # scalar, that'll be the only chunk that gets returned; if it's a list, # those are the ones that will be. chunk_ind = kwargs.pop(\u0026quot;chunk_ind\u0026quot;, None) if chunk_ind is not None: chunk_ind = ensure_list(chunk_ind) for ci, chunk in enumerate(self.index._chunk(self, chunking_style, **kwargs)): if chunk_ind is not None and ci not in chunk_ind: continue with self._chunked_read(chunk): self.get_data(fields) # NOTE: we yield before releasing the context yield self File: ~/yt/yt/yt/data_objects/data_containers.py Type: function Note that this relies on the index object providing the _chunk routine, which interprets the type of chunking. Also, _chunked_read is a context manager which looks like this:\nYTSelectionContainer._chunked_read?? Signature: YTSelectionContainer._chunked_read(self, chunk) Docstring: \u0026lt;no docstring\u0026gt; Source: @contextmanager def _chunked_read(self, chunk): # There are several items that need to be swapped out # field_data, size, shape obj_field_data = [] if hasattr(chunk, 'objs'): for obj in chunk.objs: obj_field_data.append(obj.field_data) obj.field_data = YTFieldData() old_field_data, self.field_data = self.field_data, YTFieldData() old_chunk, self._current_chunk = self._current_chunk, chunk old_locked, self._locked = self._locked, False yield self.field_data = old_field_data self._current_chunk = old_chunk self._locked = old_locked if hasattr(chunk, 'objs'): for obj in chunk.objs: obj.field_data = obj_field_data.pop(0) File: ~/yt/yt/yt/data_objects/data_containers.py Type: function This is a bit clunky, but it stores the old state (because, believe it or not, sometimes we have multiple levels of chunking simultaneously, especially for things like spatial derivatives) and then it makes a fresh state, and then it resets it after the context manager concludes.\nSo the end result here is that we have a mechanism that divides the dataset up into the chunks it needs (YTDataChunk objects), and then iterates over them. What does this look like for our particle dataset? Well, we can find out, evidently, by looking at the _current_chunk attribute on the object yielded by chunks.\nI\u0026rsquo;ve changed what we print out here just a little bit, because I want to keep the output a bit more human readable, but this is what it looks like:\ndd = ds.all_data() for chunk in dd.chunks([], \u0026quot;io\u0026quot;): print(\u0026quot;\\nExamining chunk...\u0026quot;) for obj in chunk._current_chunk.objs: print(\u0026quot; Examining obj...\u0026quot;,) for data_file in obj.data_files: print(\u0026quot; {}: {}-{}\u0026quot;.format(data_file.filename, data_file.start, data_file.end)) Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 0-262144 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 262144-524288 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 524288-786432 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 786432-1048576 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1048576-1310720 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1310720-1572864 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1572864-1835008 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1835008-2097152 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 2097152-2359296 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 2359296-2567905 A few notes here. Each chunk is able to have multiple \u0026ldquo;objects\u0026rdquo; associated with it (which in grid frontends usually means multiple grid objects) but here, we have only one entry in the obj list associated with each. Each obj then only has one item in data_files, which is not really a data file, but instead a subset of a data file specified by its start and end indices.\nIf you\u0026rsquo;re thinking this is a bit clumsy, I would agree with you.\nDask Stuff The issue that I wrote about at the start of this blog post shows up when we start looking at how these chunks are generated. In principle, this does not map that badly to how dask expect chunks to be emitted.\n(At this point I need to admit that while I\u0026rsquo;ve worked with dask, it\u0026rsquo;s entirely possible that I am going to misrepresent its capabilities. Any errors are my own, and if I find out I am mistaken about any of this, I will happily update this blog post!)\nIt\u0026rsquo;s possible to create a dask array through the dask.array.Array constructor; this is described in the array design docs. Since yt uses unyt for attaching units we will need to do some additional work, but let\u0026rsquo;s imagine that we are simply happy dealing with unit-less (and, I suppose, unyt-less) arrays for now.\nTo generate these arrays most efficiently, we need to be able to specify their size, how to obtain them, and maybe a couple other things. But for our purposes, those are the two most important things.\nUnfortunately, as you might be able to tell, this is not information that is super easily exposed without iterating over the dataset. Sure, if we iterated and read everything, of course we can show the appropriate info. And, I posted a little bit about how one might do this on issue 1891, but there\u0026rsquo;s a key thing going on in that code \u0026ndash; yt has already read all the data from disk.\nSo, this isn\u0026rsquo;t ideal.\nChunks are not persistent This all comes about because chunks are not persistent, and more specifically, chunks are always create on-demand. Each different data object will have its own set of chunks, and these will map differently. So, for instance, we might end up selecting all the same sets of objects, but they will have different sizes (and even each different field might be a different size).\nsp1 = ds.sphere(c, (1, \u0026quot;Mpc\u0026quot;)) sp2 = ds.r[ (20.0, \u0026quot;Mpc\u0026quot;) : (40.0, \u0026quot;Mpc\u0026quot;), (25.0, \u0026quot;Mpc\u0026quot;) : (45.0, \u0026quot;Mpc\u0026quot;), (55.0, \u0026quot;Mpc\u0026quot;) : (65.0, \u0026quot;Mpc\u0026quot;) ] print(\u0026quot;sp1 len == {}\\nsp2 len == {}\u0026quot;.format( len(list(sp1.chunks([], \u0026quot;io\u0026quot;))), len(list(sp2.chunks([], \u0026quot;io\u0026quot;))) )) print(\u0026quot;sp1 =\u0026gt; \u0026quot;, \u0026quot; \u0026quot;.join(str(chunk[\u0026quot;particle_ones\u0026quot;].size) for chunk in sp1.chunks([], \u0026quot;io\u0026quot;))) print(\u0026quot;sp2 =\u0026gt; \u0026quot;, \u0026quot; \u0026quot;.join(str(chunk[\u0026quot;particle_ones\u0026quot;].size) for chunk in sp2.chunks([], \u0026quot;io\u0026quot;))) sp1 len == 10 sp2 len == 10 sp1 =\u0026gt; 388571 306586 341808 205880 50260 2 1 2 3 0 sp2 =\u0026gt; 12 3673 480 29 146 200 77 419 3697 400 The trickiest part of this is that in these cases, we don\u0026rsquo;t know how big each one is going to be! For other types of indexing, it\u0026rsquo;s slightly different \u0026ndash; the indexing system for grids and octrees and meshes can figure out in advance (without reading data from disk) the precise number of values that will be read. But for particles we don\u0026rsquo;t necessarily know.\nUnfortunately, even if we did, the way that the YTDataChunk objects are the result of creating, then yield-ing, rather than returning a list of objects with known sizes makes it harder to expose this to dask. In particular, because we can\u0026rsquo;t (inexpensively) fast-forward the generator or rewind it or even access it elementwise makes it tricky to interface. One can expose unknown chunk sizes to dask, but it seems like we could do better.\nSo what can be done? Well, let me first note that a lot of this is a result of trying to be clever! Back when the chunking system was being implemented, it seemed like simple generator expressions were the right way to do it. And, a bunch of layers have been added on top of those generator expressions that make it harder to simply strip that component out.\nBut recently, Britton Smith and I have been digging into some of the particle frontends, and we think we might have a solution that would both simplify a lot of this logic and make it a lot easier to expose the arrays to different array backends \u0026ndash; specifically dask.\nFor more on that, wait for part two!\n","date":1559340372,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559509871,"objectID":"e2409a44c1f2fb08fb0b73f5ab5788e4","permalink":"https://matthewturk.github.io/post/refactoring-yt-frontends-part1/","publishdate":"2019-05-31T17:06:12-05:00","relpermalink":"/post/refactoring-yt-frontends-part1/","section":"post","summary":"The first post in a deep dive into yt frontends, chunking, and why and how they might be refactored.","tags":[],"title":"Refactoring yt Frontends - Part 1","type":"post"},{"authors":null,"categories":null,"content":"","date":1557177039,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"2fd5b0f658881a3f851b03da682bc48d","permalink":"https://matthewturk.github.io/project/yt/","publishdate":"2019-05-06T16:10:39-05:00","relpermalink":"/project/yt/","section":"project","summary":"yt is an open-source python package for analyzing and visualizing volumetric data.","tags":[],"title":"yt","type":"project"},{"authors":[],"categories":null,"content":"","date":1556736856,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"5f15fff70af789e7da9d4a78640dfa59","permalink":"https://matthewturk.github.io/talk/2019-05-01-crops-dependencies/","publishdate":"2019-05-01T13:54:16-05:00","relpermalink":"/talk/2019-05-01-crops-dependencies/","section":"talk","summary":"This is a brief overview of how we can think about dependencies in the Crops in Silico project, and how we can use that to organize our work and collaboration.","tags":[],"title":"Crops-in-Silico Collaboration and Dependencies","type":"talk"},{"authors":["Adam Brinckman","Kyle Chard","Niall Gaffney","Mihael Hategan","Matthew B Jones","Kacper Kowalik","Sivakumar Kulasekaran","Bertram Ludäscher","Bryce D Mecum","Jarek Nabrzyski","Victoria Stodden","Ian J Taylor","Matthew J Turk","Kandace Turner"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"4d610cc35353e551e4dd5786294bba6d","permalink":"https://matthewturk.github.io/publication/brinckman-2019-oa/","publishdate":"2019-05-30T20:07:09.390766Z","relpermalink":"/publication/brinckman-2019-oa/","section":"publication","summary":"The act of sharing scientific knowledge is rapidly evolving away from traditional articles and presentations to the delivery of executable objects that integrate the data and computational details (e.g., scripts and workflows) upon which the findings rely. This envisioned coupling of data and process is essential to advancing science but faces technical and institutional barriers. The Whole Tale project aims to address these barriers by connecting computational, data-intensive research efforts with the larger research process---transforming the knowledge discovery and dissemination process into one where data products are united with research articles to create ``living publications'' or tales. The Whole Tale focuses on the full spectrum of science, empowering users in the long tail of science, and power users with demands for access to big data and compute resources. We report here on the design, architecture, and implementation of the Whole Tale environment.","tags":["Living publications; Reproducibility; Provenance; Data sharing; Code sharing;Authorship"],"title":"Computing environments for reproducibility: Capturing the ``Whole Tale''","type":"publication"},{"authors":[],"categories":null,"content":"","date":1556064e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"8e090a510726760f69cb7803cc7fcc0c","permalink":"https://matthewturk.github.io/talk/2019-04-24-ddd-update/","publishdate":"2019-05-01T13:52:48-05:00","relpermalink":"/talk/2019-04-24-ddd-update/","section":"talk","summary":"My 'update' talk at the Moore Data Driven Discovery Investigator Symposium in April, 2019.","tags":[],"title":"DDD Update","type":"talk"},{"authors":[],"categories":null,"content":"","date":1554058436,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"3e44c17136c4c894688a3bcd35473a56","permalink":"https://matthewturk.github.io/talk/2019-03-31-troubleshooting-data-storytelling/","publishdate":"2019-05-01T13:53:56-05:00","relpermalink":"/talk/2019-03-31-troubleshooting-data-storytelling/","section":"talk","summary":"","tags":[],"title":"Troubleshooting Data Storytelling","type":"talk"},{"authors":[],"categories":null,"content":"","date":1553893200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"b4dbdf946084cccc2be51a72048b1513","permalink":"https://matthewturk.github.io/talk/2019-03-29-cirss-open-source-teen-years/","publishdate":"2019-04-30T17:34:14-05:00","relpermalink":"/talk/2019-03-29-cirss-open-source-teen-years/","section":"talk","summary":"In this talk, I will reflect on experiences I have had navigating the landscape of open source scholarly software as projects age, and the way that shapes interaction in an ecosystem.  I will also report some recent developments with the open source project yt, and how they both interact with the shifting needs and desires of community members and how they address the needs and desires of potential future community members.  Many exciting buzzwords such as 'Rust' and 'WebAssembly' will be used.","tags":[],"title":"CIRSS Seminar: Open Source in the Teen Years","type":"talk"},{"authors":null,"categories":null,"content":"","date":1549052179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"4600419641aba970b03879da5587bed8","permalink":"https://matthewturk.github.io/project/crops-in-silico/","publishdate":"2019-02-01T15:16:19-05:00","relpermalink":"/project/crops-in-silico/","section":"project","summary":"Crops in Silico is an integrative and multi-scale modeling platform to combine modeling efforts toward the generation of virtual crops, open and accessible to the global community.","tags":[],"title":"Crops in Silico","type":"project"},{"authors":null,"categories":null,"content":"","date":1548879389,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"670e9b7050687e20c063667daf003870","permalink":"https://matthewturk.github.io/project/whole-tale/","publishdate":"2019-01-30T15:16:29-05:00","relpermalink":"/project/whole-tale/","section":"project","summary":"Whole Tale is an initiative to build a scalable, open source, web-based, multi-user platform for reproducible research.","tags":[],"title":"Whole Tale","type":"project"},{"authors":null,"categories":null,"content":"This course covered topics that could broadly be described as \u0026ldquo;advanced,\u0026rdquo; including new platforms and tools for visualizing data, and how to present data in different, more thoughtful ways. It was structured differently than the other data viz courses, and designed for more interaction with a smaller group of students.\n","date":1546383114,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"81e4db4ee0b05df2af767d7e3ba9a362","permalink":"https://matthewturk.github.io/courses/is590adv-spr2019/","publishdate":"2019-01-01T17:51:54-05:00","relpermalink":"/courses/is590adv-spr2019/","section":"courses","summary":"Seminar on advanced or in-depth topics in data visualization","tags":[],"title":"IS590ADV - Spring 2019","type":"courses"},{"authors":null,"categories":null,"content":"This course, offered in Fall of 2018, included more javascript than previous iterations and also utilized bqplot to a greater extent.\n","date":1533163907,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"edece407444789734693d605192694b5","permalink":"https://matthewturk.github.io/courses/is590dv-fall2018/","publishdate":"2018-08-01T17:51:47-05:00","relpermalink":"/courses/is590dv-fall2018/","section":"courses","summary":"Data Viz from Fall 2018","tags":[],"title":"IS590DV - Fall 2018","type":"courses"},{"authors":["Nathan J Goldbaum","John A ZuHone","Matthew J Turk","Kacper Kowalik","Anna L Rosen"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"abc200f366401cf32068cf880c72c3bb","permalink":"https://matthewturk.github.io/publication/goldbaum-2018-ws/","publishdate":"2019-05-30T20:07:09.372097Z","relpermalink":"/publication/goldbaum-2018-ws/","section":"publication","summary":"Software that processes real-world data or that models a physical system must have some way of managing units. While simple approaches like the understood convention that all data are in a unit system (such as the MKS SI unit system) do work in practice, they are fraught with possible sources of error both by developers and users of the software. In this paper we present unyt, a Python library based on NumPy and SymPy for handling data that has units. It is designed both to aid quick interactive calculations and to be tightly integrated into a larger Python application or library. We compare unyt with two other Python libraries for handling units, Pint and astropy.units, and find that unyt is faster, has higher test coverage, and has fewer lines of code.","tags":["Authorship"],"title":"unyt: Handle, manipulate, and convert data with units in Python","type":"publication"},{"authors":null,"categories":null,"content":"This was an experimental course designed to convey the basics of \u0026ldquo;conversational computation\u0026rdquo; to astronomy undergraduates.\n","date":1514847094,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"a66798ccee3cefb7f8c172b6f85651f8","permalink":"https://matthewturk.github.io/courses/astr496-spr2018/","publishdate":"2018-01-01T17:51:34-05:00","relpermalink":"/courses/astr496-spr2018/","section":"courses","summary":"Introduction to Computational Astrophysics","tags":[],"title":"ASTR496 - Spring 2018","type":"courses"},{"authors":null,"categories":null,"content":"Starting in Spring 2018, I transitioned courses to using Github Pages and RevealJS. The repository includes built slide decks and rendered Jupyter notebooks.\n","date":1514847088,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"106a69791a1c8e84dba3b3f48bdfd470","permalink":"https://matthewturk.github.io/courses/is590dv-spr2018/","publishdate":"2018-01-01T17:51:28-05:00","relpermalink":"/courses/is590dv-spr2018/","section":"courses","summary":"Data Viz from Spring 2018","tags":[],"title":"IS590DV - Spring 2018","type":"courses"},{"authors":["Hsi-Yu Schive","John A ZuHone","Nathan J Goldbaum","Matthew J Turk","Massimo Gaspari","Chin-Yu Cheng"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"3b5789604fd9a3ad84d80b2d4e03c5fe","permalink":"https://matthewturk.github.io/publication/schive-2017-ep/","publishdate":"2019-05-30T20:07:09.39481Z","relpermalink":"/publication/schive-2017-ep/","section":"publication","summary":"We present GAMER-2, a GPU-accelerated adaptive mesh refinement (AMR) code for astrophysics. It provides a rich set of features, including adaptive time-stepping, several hydrodynamic schemes, magnetohydrodynamics, self-gravity, particles, star formation, chemistry and radiative processes with GRACKLE, data analysis with yt, and memory pool for efficient object allocation. GAMER-2 is fully bitwise reproducible. For the performance optimization, it adopts hybrid OpenMP/MPI/GPU parallelization and utilizes overlapping CPU computation, GPU computation, and CPU-GPU communication. Load balancing is achieved using a Hilbert space-filling curve on a level-by-level basis without the need to duplicate the entire AMR hierarchy on each MPI process. To provide convincing demonstrations of the accuracy and performance of GAMER-2, we directly compare with Enzo on isolated disk galaxy simulations and with FLASH on galaxy cluster merger simulations. We show that the physical results obtained by different codes are in very good agreement, and GAMER-2 outperforms Enzo and FLASH by nearly one and two orders of magnitude, respectively, on the Blue Waters supercomputers using $1-256$ nodes. More importantly, GAMER-2 exhibits similar or even better parallel scalability compared to the other two codes. We also demonstrate good weak and strong scaling using up to 4096 GPUs and 65,536 CPU cores, and achieve a uniform resolution as high as $10,240^3$ cells. Furthermore, GAMER-2 can be adopted as an AMR+GPUs framework and has been extensively used for the wave dark matter ($ψ$DM) simulations. GAMER-2 is open source (available at https://github.com/gamer-project/gamer) and new contributions are welcome.","tags":["Authorship;DXL Member Papers"],"title":"GAMER-2: a GPU-accelerated adaptive mesh refinement code -- accuracy, performance, and scalability","type":"publication"},{"authors":null,"categories":null,"content":"This was the second semester that I taught Data Viz, and I was still largely using Google Slides. The link to the repository includes the lecture PDFs and notebooks used.\n","date":1501627863,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"2882197390cd2fb552718a0c7b83241b","permalink":"https://matthewturk.github.io/courses/lis590dv-fall2017/","publishdate":"2017-08-01T17:51:03-05:00","relpermalink":"/courses/lis590dv-fall2017/","section":"courses","summary":"Data Viz from Fall 2017","tags":[],"title":"LIS590DV - Fall 2017","type":"courses"},{"authors":["Amy Marshall-Colon","Stephen P Long","Douglas K Allen","Gabrielle Allen","Daniel A Beard","Bedrich Benes","Susanne von Caemmerer","A J Christensen","Donna J Cox","John C Hart","Peter M Hirst","Kavya Kannan","Daniel S Katz","Jonathan P Lynch","Andrew J Millar","Balaji Panneerselvam","Nathan D Price","Przemyslaw Prusinkiewicz","David Raila","Rachel G Shekar","Stuti Shrivastava","Diwakar Shukla","Venkatraman Srinivasan","Mark Stitt","Matthew J Turk","Eberhard O Voit","Yu Wang","Xinyou Yin","Xin-Guang Zhu"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"bb0989fb716998e413388479a9bd5b8e","permalink":"https://matthewturk.github.io/publication/marshall-colon-2017-fb/","publishdate":"2019-05-30T20:07:09.377714Z","relpermalink":"/publication/marshall-colon-2017-fb/","section":"publication","summary":"Multi-scale models can facilitate whole plant simulations by linking gene networks, protein synthesis, metabolic pathways, physiology, and growth. Whole plant models can be further integrated with ecosystem, weather, and climate models to predict how various interactions respond to environmental perturbations. These models have the potential to fill in missing mechanistic details and generate new hypotheses to prioritize directed engineering efforts. Outcomes will potentially accelerate improvement of crop yield, sustainability, and increase future food security. It is time for a paradigm shift in plant modeling, from largely isolated efforts to a connected community that takes advantage of advances in high performance computing and mechanistic understanding of plant processes. Tools for guiding future crop breeding and engineering, understanding the implications of discoveries at the molecular level for whole plant behavior, and improved prediction of plant and ecosystem responses to the environment are urgently needed. The purpose of this perspective is to introduce Crops in silico (cropsinsilico.org), an integrative and multi-scale modeling platform, as one solution that combines isolated modeling efforts toward the generation of virtual crops, which is open and accessible to the entire plant biology community. The major challenges involved both in the development and deployment of a shared, multi-scale modeling platform, which are summarized in this prospectus, were recently identified during the first Crops in silico Symposium and Workshop.","tags":["computational framework; crop yield; integration; model; multiscale;Authorship"],"title":"Crops In Silico: Generating Virtual Crops Using an Integrative and Multi-scale Modeling Platform","type":"publication"},{"authors":["Britton D Smith","Greg L Bryan","Simon C O Glover","Nathan J Goldbaum","Matthew J Turk","John Regan","John H Wise","Hsi-Yu Schive","Tom Abel","Andrew Emerick","Brian W O'Shea","Peter Anninos","Cameron B Hummels","Sadegh Khochfar"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"01c869b2c02e627571168f1e7ea367d2","permalink":"https://matthewturk.github.io/publication/smith-2017-za/","publishdate":"2019-05-30T20:07:09.393935Z","relpermalink":"/publication/smith-2017-za/","section":"publication","summary":"We present the grackle chemistry and cooling library for astrophysical simulations and models. grackle provides a treatment of non-equilibrium primordial chemistry and cooling for H, D and He species, including H2 formation on dust grains; tabulated primordial and metal cooling; multiple ultraviolet background models; and support for radiation transfer and arbitrary heat sources. The library has an easily implementable interface for simulation codes written in c, c++ and fortran as well as a python interface with added convenience functions for semi-analytical models. As an open-source project, grackle provides a community resource for accessing and disseminating astrochemical data and numerical methods. We present the full details of the core functionality, the simulation and python interfaces, testing infrastructure, performance and range of applicability. grackle is a fully open-source project and new contributions are welcome.","tags":["Authorship;DXL Member Papers"],"title":"grackle: a chemistry and cooling library for astrophysics","type":"publication"},{"authors":null,"categories":null,"content":"This was the first time I taught Data Viz, and the course repository includes the original Google Slides PDFs, links to the presentations, and the notebooks used in the course.\n","date":1483246801,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"a8ae1d547cdc224350917d8d2bd536a0","permalink":"https://matthewturk.github.io/courses/lis590dv-spr2017/","publishdate":"2017-01-01T00:00:01-05:00","relpermalink":"/courses/lis590dv-spr2017/","section":"courses","summary":"Data Viz from Spring 2017","tags":[],"title":"LIS590DV - Spring 2017","type":"courses"},{"authors":["Harshil M Kamdar","Matthew J Turk","Robert J Brunner"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d2de67e520898676ea8c2ae39f8eaf22","permalink":"https://matthewturk.github.io/publication/kamdar-2016-ae/","publishdate":"2019-05-30T20:07:09.37875Z","relpermalink":"/publication/kamdar-2016-ae/","section":"publication","summary":"We present a new exploratory framework to model galaxy formation and evolution in a hierarchical Universe by using machine learning (ML). Our motivations are two-fold: (1) presenting a new, promising technique to study galaxy formation, and (2) quantitatively analysing the extent of the influence of dark matter halo properties on galaxies in the backdrop of semi-analytical models (SAMs). We use the influential Millennium Simulation and the corresponding Munich SAM to train and test various sophisticated ML algorithms (k-Nearest Neighbors, decision trees, random forests, and extremely randomized trees). By using only essential dark matter halo physical properties for haloes of M \u003e 1012 M⊙ and a partial merger tree, our model predicts the hot gas mass, cold gas mass, bulge mass, total stellar mass, black hole mass and cooling radius at z = 0 for each central galaxy in a dark matter halo for the Millennium run. Our results provide a unique and powerful phenomenological framework to explore the galaxy--halo connection that is built upon SAMs and demonstrably place ML as a promising and a computationally efficient tool to study small-scale structure formation.","tags":["Authorship"],"title":"Machine learning and cosmological simulations -- I. Semi-analytical models","type":"publication"},{"authors":["Harshil M Kamdar","Matthew J Turk","Robert J Brunner"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"0e563eede49bd354c26f1fd53f232568","permalink":"https://matthewturk.github.io/publication/kamdar-2016-mu/","publishdate":"2019-05-30T20:07:09.376485Z","relpermalink":"/publication/kamdar-2016-mu/","section":"publication","summary":"We extend a machine learning (ML) framework presented previously to model galaxy formation and evolution in a hierarchical universe using N-body+ hydrodynamical simulations. In this work, we show that ML is a promising technique to study galaxy formation …","tags":["Authorship"],"title":"Machine learning and cosmological simulations--II. Hydrodynamical simulations","type":"publication"},{"authors":["Desika Narayanan","Matthew Turk","Robert Feldmann","Thomas Robitaille","Philip Hopkins","Robert Thompson","Christopher Hayward","David Ball","Claude-André Faucher-Giguère","Dušan Kereš"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"f598bdf0862f455746a898a59f677c1a","permalink":"https://matthewturk.github.io/publication/narayanan-2015-gq/","publishdate":"2019-05-30T20:07:09.392775Z","relpermalink":"/publication/narayanan-2015-gq/","section":"publication","summary":"Submillimetre-bright galaxies at high redshift are the most luminous, heavily star-forming galaxies in the Universe and are characterized by prodigious emission in the far-infrared, with a flux of at least five millijanskys at a wavelength of 850 micrometres. They reside in haloes with masses about 10(13) times that of the Sun, have low gas fractions compared to main-sequence disks at a comparable redshift, trace complex environments and are not easily observable at optical wavelengths. Their physical origin remains unclear. Simulations have been able to form galaxies with the requisite luminosities, but have otherwise been unable to simultaneously match the stellar masses, star formation rates, gas fractions and environments. Here we report a cosmological hydrodynamic galaxy formation simulation that is able to form a submillimetre galaxy that simultaneously satisfies the broad range of observed physical constraints. We find that groups of galaxies residing in massive dark matter haloes have increasing rates of star formation that peak at collective rates of about 500-1,000 solar masses per year at redshifts of two to three, by which time the interstellar medium is sufficiently enriched with metals that the region may be observed as a submillimetre-selected system. The intense star formation rates are fuelled in part by the infall of a reservoir gas supply enabled by stellar feedback at earlier times, not through major mergers. With a lifetime of nearly a billion years, our simulations show that the submillimetre-bright phase of high-redshift galaxies is prolonged and associated with significant mass buildup in early-Universe proto-clusters, and that many submillimetre-bright galaxies are composed of numerous unresolved components (for which there is some observational evidence).","tags":["Authorship;DXL Member Papers"],"title":"The formation of submillimetre-bright galaxies from gas infall over a billion years","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"96c4421b91ab99587be7a001045f86a2","permalink":"https://matthewturk.github.io/publication/turk-2015-yw/","publishdate":"2019-05-30T20:07:09.373042Z","relpermalink":"/publication/turk-2015-yw/","section":"publication","summary":"Gmail. It serves as the center of my digital life. When I receive a new email with a Google Docs document linked in it, there's a preview of that document--- when I send one to someone else, Gmail first checks to see if the person receiving it has permission to view it and lets me know …","tags":["Authorship"],"title":"Vertical Integration","type":"publication"},{"authors":["Samuel W Skillman","Michael S Warren","Matthew J Turk","Risa H Wechsler","Daniel E Holz","P M Sutter"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"c4612415f473e20e2680d86b0e53ec81","permalink":"https://matthewturk.github.io/publication/skillman-2014-vt/","publishdate":"2019-05-30T20:07:09.380327Z","relpermalink":"/publication/skillman-2014-vt/","section":"publication","summary":"The Dark Sky Simulations are an ongoing series of cosmological N-body simulations designed to provide a quantitative and accessible model of the evolution of the large-scale Universe. Such models are essential for many aspects of the study of dark matter and dark energy, since we lack a sufficiently accurate analytic model of non-linear gravitational clustering. In July 2014, we made available to the general community our early data release, consisting of over 55 Terabytes of simulation data products, including our largest simulation to date, which used $1.07 times 10^12~(10240^3)$ particles in a volume $8h^-1mathrmGpc$ across. Our simulations were performed with 2HOT, a purely tree-based adaptive N-body method, running on 200,000 processors of the Titan supercomputer, with data analysis enabled by yt. We provide an overview of the derived halo catalogs, mass function, power spectra and light cone data. We show self-consistency in the mass function and mass power spectrum at the 1% level over a range of more than 1000 in particle mass. We also present a novel method to distribute and access very large datasets, based on an abstraction of the World Wide Web (WWW) as a file system, remote memory-mapped file access semantics, and a space-filling curve index. This method has been implemented for our data release, and provides a means to not only query stored results such as halo catalogs, but also to design and deploy new analysis techniques on large distributed datasets.","tags":["Authorship"],"title":"Dark Sky Simulations: Early Data Release","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"a1f14428b663d7f3b82f91bbfe0a9609","permalink":"https://matthewturk.github.io/publication/turk-2014-ak/","publishdate":"2019-05-30T20:07:09.364588Z","relpermalink":"/publication/turk-2014-ak/","section":"publication","summary":"Designing Software for Collaboration In computational science, collaboration in different scientific communities often takes different forms---examining the results of data generation or acquisition, combining dispa- rate pieces of software in a computational pipeline, and even enhancing …","tags":["Authorship"],"title":"Fostering Collaborative Computational Science","type":"publication"},{"authors":["Benjamin Holtzman","Jason Candler","Matthew Turk","Daniel Peter"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ce1db4fcedcc809c4ff9dcee5ab6f2ff","permalink":"https://matthewturk.github.io/publication/holtzman-2014-wb/","publishdate":"2019-05-30T20:07:09.373678Z","relpermalink":"/publication/holtzman-2014-wb/","section":"publication","summary":"We construct a representation of earthquakes and global seismic waves through sound and animated images. The seismic wave field is the ensemble of elastic waves that propagate through the planet after an earthquake, emanating from the rupture on the fault. The sounds are made by time compression (i.e. speeding up) of seismic data with minimal additional processing. The animated images are renderings of numerical simulations of seismic wave propagation in the globe. Synchronized sounds and images reveal complex patterns and illustrate numerous aspects of the seismic wave field. These movies represent phenomena occurring far from the time and length scales normally accessible to us, creating a profound experience for the observer. The multi-sensory perception of these complex phenomena may also bring new insights to researchers.","tags":["Authorship"],"title":"Seismic Sound Lab: Sights, Sounds and Perception of the Earth as an Acoustic Space","type":"publication"},{"authors":["T Kwasnitschka","K C Yu","M Turk"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"36670fa758f1f2a192e24ec7cefa9258","permalink":"https://matthewturk.github.io/publication/kwasnitschka-2014-ya/","publishdate":"2019-05-30T20:07:09.361875Z","relpermalink":"/publication/kwasnitschka-2014-ya/","section":"publication","summary":"What topics belong inside a planetarium and what is beyond the scope of our mission? Are we limited to astronomy?","tags":["Authorship"],"title":"The Ground beath our Feet: Earth Science in the Planetarium","type":"publication"},{"authors":["Ji-Hoon Kim","Mark R Krumholz","John H Wise","Matthew J Turk","Nathan J Goldbaum","Tom Abel"],"categories":null,"content":"","date":1383264e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"7dc7f1a8d9fa48cd9148edec88aee4a6","permalink":"https://matthewturk.github.io/publication/kim-2013-ac/","publishdate":"2019-05-30T20:07:09.371064Z","relpermalink":"/publication/kim-2013-ac/","section":"publication","summary":"We investigate the spatially resolved star formation relation using a galactic disk formed in a comprehensive high-resolution (3.8 pc) simulation. Our new implementation of stellar feedback includes ionizing radiation as well as supernova explosions, and we handle ionizing radiation by solving the radiative transfer equation rather than by a subgrid model. Photoheating by stellar radiation stabilizes gas against Jeans fragmentation, reducing the star formation rate (SFR). Because we have self-consistently calculated the location of ionized gas, we are able to make simulated, spatially resolved observations of star formation tracers, such as H$α$ emission. We can also observe how stellar feedback manifests itself in the correlation between ionized and molecular gas. Applying our techniques to the disk in a galactic halo of 2.3 $times$ 1011 M ☉, we find that the correlation between SFR density (estimated from mock H$α$ emission) and H2 density shows large scatter, especially at high resolutions of ≲75 pc that are comparable to the size of giant molecular clouds (GMCs). This is because an aperture of GMC size captures only particular stages of GMC evolution and because H$α$ traces hot gas around star-forming regions and is displaced from the H2 peaks themselves. By examining the evolving environment around star clusters, we speculate that the breakdown of the traditional star formation laws of the Kennicutt-Schmidt type at small scales is further aided by a combination of stars drifting from their birthplaces and molecular clouds being dispersed via stellar feedback.","tags":["Authorship"],"title":"DWARF GALAXIES WITH IONIZING RADIATION FEEDBACK. II. SPATIALLY RESOLVED STAR FORMATION RELATION","type":"publication"},{"authors":["Ji-Hoon Kim","Mark R Krumholz","John H Wise","Matthew J Turk","Nathan J Goldbaum","Tom Abel"],"categories":null,"content":"","date":1377993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d89d1130804c0344ac02ec8f41dc7d61","permalink":"https://matthewturk.github.io/publication/kim-2013-jy/","publishdate":"2019-05-30T20:07:09.379476Z","relpermalink":"/publication/kim-2013-jy/","section":"publication","summary":"We describe a new method for simulating ionizing radiation and supernova feedback in the analogs of low-redshift galactic disks. In this method, which we call star-forming molecular cloud (SFMC) particles, we use a ray-tracing technique to solve the radiative transfer equation for ultraviolet photons emitted by thousands of distinct particles on the fly. Joined with high numerical resolution of 3.8 pc, the realistic description of stellar feedback helps to self-regulate star formation. This new feedback scheme also enables us to study the escape of ionizing photons from star-forming clumps and from a galaxy, and to examine the evolving environment of star-forming gas clumps. By simulating a galactic disk in a halo of 2.3 $times$ 1011 M ☉, we find that the average escape fraction from all radiating sources on the spiral arms (excluding the central 2.5 kpc) fluctuates between 0.08% and 5.9% during a ∼20 Myr period with a mean value of 1.1%. The flux of escaped photons from these sources is not strongly beamed, but manifests a large opening angle of more than 60° from the galactic pole. Further, we investigate the escape fraction per SFMC particle, f esc(i), and how it evolves as the particle ages. We discover that the average escape fraction f esc is dominated by a small number of SFMC particles with high f esc(i). On average, the escape fraction from an SFMC particle rises from 0.27% at its birth to 2.1% at the end of a particle lifetime, 6 Myr. This is because SFMC particles drift away from the dense gas clumps in which they were born, and because the gas around the star-forming clumps is dispersed by ionizing radiation and supernova feedback. The framework established in this study brings deeper insight into the physics of photon escape fraction from an individual star-forming clump and from a galactic disk.","tags":["Authorship"],"title":"DWARF GALAXIES WITH IONIZING RADIATION FEEDBACK. I. ESCAPE OF IONIZING PHOTONS","type":"publication"},{"authors":["The Enzo Collaboration","Greg L Bryan","Michael L Norman","Brian W O'Shea","Tom Abel","John H Wise","Matthew J Turk","Daniel R Reynolds","David C Collins","Peng Wang","Samuel W Skillman","Britton Smith","Robert P Harkness","James Bordner","Ji-Hoon Kim","Michael Kuhlen","Hao Xu","Nathan Goldbaum","Cameron Hummels","Alexei G Kritsuk","Elizabeth Tasker","Stephen Skory","Christine M Simpson","Oliver Hahn","Jeffrey S Oishi","Geoffrey C So","Fen Zhao","Renyue Cen","Yuan Li"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"8517285c643a11800825d1dc3fe6aa6b","permalink":"https://matthewturk.github.io/publication/the-enzo-collaboration-2013-es/","publishdate":"2019-05-30T20:07:09.391823Z","relpermalink":"/publication/the-enzo-collaboration-2013-es/","section":"publication","summary":"This paper describes the open-source code Enzo, which uses block-structured adaptive mesh refinement to provide high spatial and temporal resolution for modeling astrophysical fluid flows. The code is Cartesian, can be run in 1, 2, and 3 dimensions, and supports a wide variety of physics including hydrodynamics, ideal and non-ideal magnetohydrodynamics, N-body dynamics (and, more broadly, self-gravity of fluids and particles), primordial gas chemistry, optically-thin radiative cooling of primordial and metal-enriched plasmas (as well as some optically-thick cooling models), radiation transport, cosmological expansion, and models for star formation and feedback in a cosmological context. In addition to explaining the algorithms implemented, we present solutions for a wide range of test problems, demonstrate the code's parallel performance, and discuss the Enzo collaboration's code development methodology.","tags":["Authorship"],"title":"Enzo: An Adaptive Mesh Refinement Code for Astrophysics","type":"publication"},{"authors":["M J Turk"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"c8a33d90212b796c8d03b80111c09aa0","permalink":"https://matthewturk.github.io/publication/turk-2013-ks/","publishdate":"2019-05-30T20:07:09.374781Z","relpermalink":"/publication/turk-2013-ks/","section":"publication","summary":"As scientists' needs for computational techniques and tools grow, they cease to be supportable by software developed in isolation. In many cases, these needs are being met by communities of practice, where software is developed by domain scientists to reach …","tags":["Authorship"],"title":"How to scale a code in the human dimension","type":"publication"},{"authors":["Matthew J Turk"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"2e693ceb78643251252efd3fdb648e1b","permalink":"https://matthewturk.github.io/publication/turk-2013-jj/","publishdate":"2019-05-30T20:07:09.375168Z","relpermalink":"/publication/turk-2013-jj/","section":"publication","summary":"As scientists' needs for computational techniques and tools grow, they cease to be supportable by software developed in isolation. In many cases, these needs are being met by communities of practice, where software is developed by domain scientists to reach …","tags":["XSEDE proceedings","community","open source;Authorship"],"title":"Scaling a Code in the Human Dimension","type":"publication"},{"authors":["John H Wise","Tom Abel","Matthew J Turk","Michael L Norman","Britton D Smith"],"categories":null,"content":"","date":1346457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"52da72f3fa7de9f835f555ba2e7cb3a4","permalink":"https://matthewturk.github.io/publication/wise-2012-kl/","publishdate":"2019-05-30T20:07:09.362758Z","relpermalink":"/publication/wise-2012-kl/","section":"publication","summary":"Here we present three adaptive mesh refinement radiation hydrodynamics simulations that illustrate the impact of momentum transfer from ionising radiation to the absorbing gas on star formation in high-redshift dwarf galaxies. Momentum transfer is calculated by solving the radiative transfer equation with a ray tracing algorithm that is adaptive in spatial and angular coordinates. We find that momentum input partially affects star formation by increasing the turbulent support to a three-dimensional rms velocity equal to the circular velocity of early haloes. Compared to a calculation that neglects radiation pressure, the star formation rate is decreased by a factor of five to 1.8 ? 10?2 M? yr?1 in a dwarf galaxy with a dark matter and stellar mass of 2.0 ? 108 M? and 4.5 ? 105 M?, respectively, when radiation pressure is included. Its mean metallicity of 10?2.1 Z? is consistent with the observed dwarf galaxy luminosity-metallicity relation. In addition to photo-heating in H II regions, radiation pressure further drives dense gas from star forming regions, so supernovae feedback occurs in a warmer and more diffuse medium, launching metal-rich outflows.","tags":["Authorship"],"title":"The imprint of pop III stars on the first galaxies","type":"publication"},{"authors":["Matthew J Turk","Jeffrey S Oishi","Tom Abel","Greg L Bryan"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"3cbbedf0bd49e08ae8ed40e5800104c8","permalink":"https://matthewturk.github.io/publication/turk-2012-ac/","publishdate":"2019-05-30T20:07:09.382091Z","relpermalink":"/publication/turk-2012-ac/","section":"publication","summary":"We study the buildup of magnetic fields during the formation of Population III star-forming regions by conducting cosmological simulations from realistic initial conditions and varying the Jeans resolution. To investigate this in detail, we start simulations from identical initial conditions, mandating 16, 32, and 64 zones per Jeans length, and study the variation in their magnetic field amplification. We find that, while compression results in some amplification, turbulent velocity fluctuations driven by the collapse can further amplify an initially weak seed field via dynamo action, provided there is sufficient numerical resolution to capture vortical motions (we find this requirement to be 64 zones per Jeans length, slightly larger than but consistent with previous work run with more idealized collapse scenarios). We explore saturation of amplification of the magnetic field, which could potentially become dynamically important in subsequent, fully resolved calculations. We have also identified a relatively surprising phenomenon that is purely hydrodynamic: the higher-resolved simulations possess substantially different characteristics, including higher infall velocity, increased temperatures inside 1000 AU, and decreased molecular hydrogen content in the innermost region. Furthermore, we find that disk formation is suppressed in higher-resolution calculations, at least at the times that we can follow the calculation. We discuss the effect this may have on the buildup of disks over the accretion history of the first clump to form as well as the potential for gravitational instabilities to develop and induce fragmentation.","tags":["Authorship"],"title":"MAGNETIC FIELDS IN POPULATION III STAR FORMATION","type":"publication"},{"authors":["John H Wise","Tom Abel","Matthew J Turk","Michael L Norman","Britton D Smith"],"categories":null,"content":"","date":1325376e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"aa120229066e2ea9fc4e86004a779463","permalink":"https://matthewturk.github.io/publication/wise-2012-dn/","publishdate":"2019-05-30T20:07:09.382982Z","relpermalink":"/publication/wise-2012-dn/","section":"publication","summary":"Massive stars provide feedback that shapes the interstellar medium of galaxies at all redshifts and their resulting stellar populations. Here we present three adaptive mesh refinement radiation hydrodynamics simulations that illustrate the impact of momentum …","tags":["Authorship"],"title":"The birth of a galaxy--II. The role of radiation pressure","type":"publication"},{"authors":["John H Wise","Matthew J Turk","Michael L Norman","Tom Abel"],"categories":null,"content":"","date":1322697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"2e513dfacd2ecb326d361c9ba89cffb2","permalink":"https://matthewturk.github.io/publication/wise-2011-ph/","publishdate":"2019-05-30T20:07:09.386569Z","relpermalink":"/publication/wise-2011-ph/","section":"publication","summary":"By definition, Population III stars are metal-free, and their protostellar collapse is driven by molecular hydrogen cooling in the gas phase, leading to large characteristic masses. Population II stars with lower characteristic masses form when the star-forming gas reaches a critical metallicity of 10--6-10--3.5 Z ☉. We present an adaptive mesh refinement radiation hydrodynamics simulation that follows the transition from Population III to Population II star formation. The maximum spatial resolution of 1 comoving parsec allows for individual molecular clouds to be well resolved and their stellar associations to be studied in detail. We model stellar radiative feedback with adaptive ray tracing. A top-heavy initial mass function for the Population III stars is considered, resulting in a plausible distribution of pair-instability supernovae and associated metal enrichment. We find that the gas fraction recovers from 5% to nearly the cosmic fraction in halos with merger histories rich in halos above 107 M ☉. A single pair-instability supernova is sufficient to enrich the host halo to a metallicity floor of 10--3 Z ☉ and to transition to Population II star formation. This provides a natural explanation for the observed floor on damped Ly$α$ systems metallicities reported in the literature, which is of this order. We find that stellar metallicities do not necessarily trace stellar ages, as mergers of halos with established stellar populations can create superpositions of t--Z evolutionary tracks. A bimodal metallicity distribution is created after a starburst occurs when the halo can cool efficiently through atomic line cooling.","tags":["Authorship"],"title":"THE BIRTH OF A GALAXY: PRIMORDIAL METAL ENRICHMENT AND STELLAR POPULATIONS","type":"publication"},{"authors":["M J Turk","B D Smith"],"categories":null,"content":"","date":129384e4,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"b3764bc9a12880080515cba2186c14bc","permalink":"https://matthewturk.github.io/publication/turk-2011-ck/","publishdate":"2019-05-30T20:07:09.366733Z","relpermalink":"/publication/turk-2011-ck/","section":"publication","summary":"The usage of the high-level scripting language Python has enabled new mechanisms for data interrogation, discovery and visualization of scientific data. We present yt, an open source, community-developed astrophysical analysis and visualization toolkit for data …","tags":["Authorship"],"title":"High-Performance Astrophysical Simulations and Analysis with Python","type":"publication"},{"authors":["Matthew J Turk","Paul Clark","S C O Glover","T H Greif","Tom Abel","Ralf Klessen","Volker Bromm"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"972f8cfd264acbc97e71613f0de8f131","permalink":"https://matthewturk.github.io/publication/turk-2010-mg/","publishdate":"2019-05-30T20:07:09.381233Z","relpermalink":"/publication/turk-2010-mg/","section":"publication","summary":"The transformation of atomic hydrogen to molecular hydrogen through three-body reactions is a crucial stage in the collapse of primordial, metal-free halos, where the first generation of stars (Population III stars) in the universe is formed. However, in the published literature, the rate coefficient for this reaction is uncertain by nearly an order of magnitude. We report on the results of both adaptive mesh refinement and smoothed particle hydrodynamics simulations of the collapse of metal-free halos as a function of the value of this rate coefficient. For each simulation method, we have simulated a single halo three times, using three different values of the rate coefficient. We find that while variation between halo realizations may be greater than that caused by the three-body rate coefficient being used, both the accretion physics onto Population III protostars as well as the long-term stability of the disk and any potential fragmentation may depend strongly on this rate coefficient.","tags":["Authorship"],"title":"EFFECTS OF VARYING THE THREE-BODY MOLECULAR HYDROGEN FORMATION RATE IN PRIMORDIAL STAR FORMATION","type":"publication"},{"authors":["Matthew J Turk","Michael L Norman","Tom Abel"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ba4d0d7e4ffe9e250803df6633aca598","permalink":"https://matthewturk.github.io/publication/turk-2010-vh/","publishdate":"2019-05-30T20:07:09.389768Z","relpermalink":"/publication/turk-2010-vh/","section":"publication","summary":"We report on simulations of the formation of the first stars in the","tags":["cosmology: theory; galaxies: formation; H II regions; stars:; formation; Astrophysics - Cosmology and Nongalactic Astrophysics;Authorship"],"title":"High-entropy Polar Regions Around the First Protostars","type":"publication"},{"authors":["Matthew J Turk","Britton D Smith","Jeffrey S Oishi","Stephen Skory","Samuel W Skillman","Tom Abel","Michael L Norman"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"880368f1545659f8c60cd5a61c8a39e5","permalink":"https://matthewturk.github.io/publication/turk-2010-hk/","publishdate":"2019-05-30T20:07:09.388606Z","relpermalink":"/publication/turk-2010-hk/","section":"publication","summary":"The analysis of complex multiphysics astrophysical simulations presents a unique and rapidly growing set of challenges: reproducibility, parallelization, and vast increases in data size and complexity chief among them. In order to meet these challenges, and in order to open up new avenues for collaboration between users of multiple simulation platforms, we present yt (available at http://yt.enzotools.org/) an open source, community-developed astrophysical analysis and visualization toolkit. Analysis and visualization with yt are oriented around physically relevant quantities rather than quantities native to astrophysical simulation codes. While originally designed for handling Enzo's structure adaptive mesh refinement data, yt has been extended to work with several different simulation methods and simulation codes including Orion, RAMSES, and FLASH. We report on its methods for reading, handling, and visualizing data, including projections, multivariate volume rendering, multi-dimensional histograms, halo finding, light cone generation, and topologically connected isocontour identification. Furthermore, we discuss the underlying algorithms yt uses for processing and visualizing data, and its mechanisms for parallelization of analysis tasks.","tags":["Authorship"],"title":"yt: A MULTI-CODE ANALYSIS TOOLKIT FOR ASTROPHYSICAL SIMULATION DATA","type":"publication"},{"authors":["S Skory","M J Turk","M L Norman","A L Coil"],"categories":null,"content":"","date":1262304e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"477b00158b1a9bd5ba4c5e21832541b4","permalink":"https://matthewturk.github.io/publication/skory-2010-il/","publishdate":"2019-05-30T20:07:09.377133Z","relpermalink":"/publication/skory-2010-il/","section":"publication","summary":"Modern N-body cosmological simulations contain billions ($10^ 9$) of dark matter particles. These simulations require hundreds to thousands of gigabytes of memory, and employ hundreds to tens of thousands of processing cores on many compute nodes. In order to …","tags":["Authorship"],"title":"Parallel hop: A scalable halo finder for massive cosmological data sets","type":"publication"},{"authors":["Matthew J Turk","Tom Abel","Brian O'Shea"],"categories":null,"content":"","date":1246406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ecfd80b5ead375d5b6a7dee166994961","permalink":"https://matthewturk.github.io/publication/turk-2009-fl/","publishdate":"2019-05-30T20:07:09.385796Z","relpermalink":"/publication/turk-2009-fl/","section":"publication","summary":"Previous high-resolution cosmological simulations predicted that the first stars to appear in the early universe were very massive and formed in isolation. Here, we discuss a cosmological simulation in which the central 50 M(o) (where M(o) is the mass of the Sun) clump breaks up into two cores having a mass ratio of two to one, with one fragment collapsing to densities of 10(-8) grams per cubic centimeter. The second fragment, at a distance of approximately 800 astronomical units, is also optically thick to its own cooling radiation from molecular hydrogen lines but is still able to cool via collision-induced emission. The two dense peaks will continue to accrete from the surrounding cold gas reservoir over a period of approximately 10(5) years and will likely form a binary star system.","tags":["Authorship"],"title":"The formation of Population III binaries from cosmological initial conditions","type":"publication"},{"authors":["Britton D Smith","Matthew J Turk","Steinn Sigurdsson","Brian W O'Shea","Michael L Norman"],"categories":null,"content":"","date":1230768e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"b1dcadcb3cbb4b53f68bf4f702bd8de8","permalink":"https://matthewturk.github.io/publication/smith-2009-dp/","publishdate":"2019-05-30T20:07:09.384512Z","relpermalink":"/publication/smith-2009-dp/","section":"publication","summary":"Simulations of the formation of Population III (Pop III) stars suggest that they were much more massive than the Pop II and Pop I stars observed today. This is due to the collapse dynamics of metal-free gas, which is regulated by the radiative cooling of molecular hydrogen. We study how the collapse of gas clouds is altered by the addition of metals to the star-forming environment by performing a series of simulations of pre-enriched star formation at various metallicities. To make a clean comparison with metal-free star formation, we use initial conditions identical to a Pop III star formation simulation, with low ionization and no external radiation other than the cosmic microwave background (CMB). For metallicities below the critical metallicity, Z cr, collapse proceeds similar to the metal-free case, and only massive objects form. For metallicities well above Z cr, efficient cooling rapidly lowers the gas temperature to the temperature of the CMB. The gas is unable to radiatively cool below the CMB temperature, and becomes thermally stable. For high metallicities, Z ≳ 10--2.5 Z ☉, this occurs early in the evolution of the gas cloud, when the density is still relatively low. The resulting cloud cores show little or no fragmentation, and would most likely form massive stars. If the metallicity is not vastly above Z cr, the cloud cools efficiently but does not reach the CMB temperature, and fragmentation into multiple objects occurs. We conclude that there were three distinct modes of star formation at high redshift (z ≳ 4): a ``primordial'' mode, producing massive stars (10s to 100s of M ☉) at very low metallicities (Z ≲ 10--3.75 Z ☉); a CMB-regulated mode, producing moderate mass (10s of M ☉) stars at high metallicities (Z ≳ 10--2.5 Z ☉ at redshift z∼ 15-20); and a low-mass (a few M ☉) mode existing between these two metallicities. As the universe ages and the CMB temperature decreases, the range of the low-mass mode extends to higher metallicities, eventually becoming the only mode of star formation.","tags":["Authorship"],"title":"THREE MODES OF METAL-ENRICHED STAR FORMATION IN THE EARLY UNIVERSE","type":"publication"},{"authors":["John H Wise","Matthew J Turk","Tom Abel"],"categories":null,"content":"","date":1228089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"6e687ba97e6cd867bc232f39e56f3687","permalink":"https://matthewturk.github.io/publication/wise-2008-gl/","publishdate":"2019-05-30T20:07:09.383653Z","relpermalink":"/publication/wise-2008-gl/","section":"publication","summary":"Numerous cosmological hydrodynamic studies have addressed the formation of galaxies. Here we choose to study the first stages of galaxy formation, including nonequilibrium atomic primordial gas cooling, gravity, and hydrodynamics. Using initial conditions appropriate for the concordance cosmological model of structure formation, we perform two adaptive mesh refinement simulations of 108 M☉ galaxies at high redshift. The calculations resolve the Jeans length at all times with more than 16 cells and capture over 14 orders of magnitude in length scales. In both cases, the dense, 105 solar mass, one parsec central regions are found to contract rapidly and have turbulent Mach numbers up to 4. Despite the ever decreasing Jeans length of the isothermal gas, we only find one site of fragmentation during the collapse. However, rotational secular bar instabilities transport angular momentum outward in the central parsec as the gas continues to collapse and lead to multiple nested unstable fragments with decreasing masses down to sub-Jupiter mass scales. Although these numerical experiments neglect star formation and feedback, they clearly highlight the physics of turbulence in gravitationally collapsing gas. The angular momentum segregation seen in our calculations plays an important role in theories that form supermassive black holes from gaseous collapse.","tags":["Authorship"],"title":"Resolving the Formation of Protogalaxies. II. Central Gravitational Collapse","type":"publication"},{"authors":["Matthew J Turk","Tom Abel","Brian W O'Shea"],"categories":null,"content":"","date":1204329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"41c188ffd7b199c76187c2e05e873a44","permalink":"https://matthewturk.github.io/publication/turk-2008-nk/","publishdate":"2019-05-30T20:07:09.363676Z","relpermalink":"/publication/turk-2008-nk/","section":"publication","summary":"Modeling the formation of the first stars in the universe is a well?posed problem and ideally suited for computational investigation.We have conducted high?resolution numerical studies of the formation of primordial stars. Beginning with primordial initial conditions appropriate for a ?CDM model, we used the Eulerian adaptive mesh refinement code (Enzo) to achieve unprecedented numerical resolution, resolving cosmological scales as well as sub?stellar scales simultaneously. Building on the work of Abel, Bryan and Norman (2002), we followed the evolution of the first collapsing cloud until molecular hydrogen is optically thick to cooling radiation. In addition, the calculations account for the process of collision?induced emission (CIE) and add approximations to the optical depth in both molecular hydrogen roto?vibrational cooling and CIE. Also considered are the effects of chemical heating/cooling from the formation/destruction of molecular hydrogen. We present the results of these simulations, showing the formation of a 10 Jupiter?mass protostellar core bounded by a strongly aspherical accretion shock. Accretion rates are found to be as high as one solar mass per year.","tags":["Authorship"],"title":"Towards Forming a Primordial Protostar in a Cosmological AMR Simulation","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d444bc751f807abdbc31db0e30dd812f","permalink":"https://matthewturk.github.io/publication/turk-2008-bl/","publishdate":"2019-05-30T20:07:09.374317Z","relpermalink":"/publication/turk-2008-bl/","section":"publication","summary":"The study the origins of cosmic structure requires large-scale computer simulations beginning with well-constrained, observationally-determined, initial conditions. We use Adaptive Mesh Refinement to conduct multi-resolution simulations spanning twelve orders …","tags":["Authorship"],"title":"Analysis and visualization of multi-scale astrophysical simulations using python and numpy","type":"publication"}]