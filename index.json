[{"authors":["admin"],"categories":null,"content":"Matthew Turk is an assistant professor in the School of Information Sciences and also holds an appointment with the Department of Astronomy in the College of Liberal Arts and Sciences. His research is focused on how individuals interact with data and how that data is processed and understood.\nAt the University of Illinois, he leads the Data Exploration Lab and teaches in Data Visualization, Data Storytelling, and Computational Astrophysics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1559268143,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://matthewturk.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"Matthew Turk is an assistant professor in the School of Information Sciences and also holds an appointment with the Department of Astronomy in the College of Liberal Arts and Sciences. His research is focused on how individuals interact with data and how that data is processed and understood.\nAt the University of Illinois, he leads the Data Exploration Lab and teaches in Data Visualization, Data Storytelling, and Computational Astrophysics.","tags":null,"title":"Matthew Turk","type":"author"},{"authors":[],"categories":null,"content":"","date":1562871758,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563046685,"objectID":"9b149762383f665c6f591297f2b857f5","permalink":"https://matthewturk.github.io/talk/2019-07-11-getting-lost-in-community-building/","publishdate":"2019-07-11T14:02:38-05:00","relpermalink":"/talk/2019-07-11-getting-lost-in-community-building/","section":"talk","summary":"It's really easy to get lost in community building.","tags":[],"title":"Getting Lost in Community Building","type":"talk"},{"authors":[],"categories":[],"content":" For all the data formats that yt knows about, reading a dataset is supposed to be trivial \u0026ndash; the yt.load command knows how to parse the metadata of something like 30-50 different dialects of data, generate the in-memory representation, and even manage things like the coordinate system (Spherical? Cartesian? etc) and how to apply spatial transformations.\nBut, if you\u0026rsquo;re just experimenting or trying to get a new frontend going, right now it\u0026rsquo;s too hard to load data into yt. (Before you think I\u0026rsquo;m passing the buck, though: this is kind of my fault.)\nLoading some spherical data Last week, I worked with some folks to load in data from a simulation code called CHIMERA. The data specifically was a 2D axially symmetric dataset, with log-spaced bins in the radial direction. The file format was really quite straightforward, too!\nUsually when developing a frontend to make all of this easier, the first step is to load the data into yt using the Stream frontend. This has functions like load_amr_grids, load_uniform_grid, load_unstructured_mesh, load_particles and so on. But, it was pretty annoying to load the data into yt. And it seems like it should be a lot simpler.\nLet\u0026rsquo;s dig in, following along as I had explored it.\nimport h5py import yt import numpy as np f = h5py.File(\u0026quot;chimera_000661800_grid_1_01.h5\u0026quot;, \u0026quot;r\u0026quot;) f.keys()  \u0026lt;KeysViewHDF5 ['abundance', 'analysis', 'fluid', 'mesh', 'metadata', 'radiation', 'tracking']\u0026gt;  Without any additional information, my first guess is to look at the mesh dataset \u0026ndash; that\u0026rsquo;s likely where I\u0026rsquo;m going to find the information for how the dataset is organized.\nf[\u0026quot;/mesh\u0026quot;].keys()  \u0026lt;KeysViewHDF5 ['array_dimensions', 'cycle', 'd_omega', 'dx_cf', 'dy_cf', 'dz_cf', 'i_frame', 'last_frame', 'my_hyperslab_group', 'nz_hyperslabs', 'ongrid_mask', 'phi_index_bound', 'r_comv', 'radial_index_bound', 't_bounce', 'theta_index_bound', 'time', 'time_steps', 'x_cf', 'x_ef', 'y_cf', 'y_ef', 'z_cf', 'z_ef']\u0026gt;  Well, here we go! Lots of interesting things to look at. I\u0026rsquo;ll save a bit of time here and note that the things we\u0026rsquo;re interested in are the array_dimensions and the various _ef arrays: x_ef, y_ef, z_ef. These are the dimensions of the simulation and the cell edges, respectively. There are other interesting things here as well, but for our current purposes, this is what we want to look at.\ndims = f[\u0026quot;/mesh/array_dimensions\u0026quot;][()] xgrid, ygrid, zgrid = (f[\u0026quot;/mesh/{}_ef\u0026quot;.format(ax)][()] for ax in 'xyz')  xgrid.shape, ygrid.shape, zgrid.shape, dims  ((723,), (241,), (2,), array([722, 240, 1], dtype=int32))  We\u0026rsquo;re dealing with cell edges here so we have one more in each dimension than the cell-centered variables. Great! Everything is going according to plan so far.\n(You might be thinking to yourself, \u0026ldquo;Gee, wouldn\u0026rsquo;t it be nice if this 1:1 mapping of values to what yt expects was easier to set up?\u0026rdquo; I certainly was.)\nLet\u0026rsquo;s see what fields we can load up:\nf[\u0026quot;/fluid\u0026quot;].keys()  \u0026lt;KeysViewHDF5 ['LScompress', 'agr_c', 'agr_e', 'c_eos_i', 'dimeos', 'dudt_nu', 'dudt_nuc', 'e_int', 'entropy', 'eos_rho', 'grav_x_c', 'grav_y_c', 'grav_z_c', 'press', 'rho_c', 't_c', 'u_c', 'v_c', 'v_csound', 'wBVMD', 'w_c', 'ye_c', 'ylep']\u0026gt;  Some might have reduced dimensionality, so we\u0026rsquo;ll take a look at what will be the most obvious to load in by identifying which have the right dimensionality.\nimport collections collections.Counter(f[\u0026quot;/fluid\u0026quot;][v].shape for v in f[\u0026quot;/fluid\u0026quot;])  Counter({(): 1, (722,): 1, (723,): 1, (1,): 2, (2,): 1, (1, 240, 722): 17})  Looks like the fluid fields are stored in reverse order from the dims array, so let\u0026rsquo;s populate our dictionary (all in-memory) with this info.\nWe\u0026rsquo;ll grab the transpose, though, so that we keep the same ijk ordering as specified in the dimensions.\ndata = {n: v[()].T for n, v in f[\u0026quot;/fluid\u0026quot;].items() if v.shape == tuple(dims[::-1])} data.keys()  dict_keys(['dudt_nu', 'dudt_nuc', 'e_int', 'entropy', 'grav_x_c', 'grav_y_c', 'grav_z_c', 'press', 'rho_c', 't_c', 'u_c', 'v_c', 'v_csound', 'wBVMD', 'w_c', 'ye_c', 'ylep'])  We can now proceed! Let\u0026rsquo;s generate our semi-structured mesh from our coordinate info and load it all in.\nThis next function generates a full set of coordinates and connectivity to make our hexahedral mesh look like a structured system. (This is a place that yt could use some improvement, to speed things up by not requiring this hoop-jumping!)\ncoord, conn = yt.hexahedral_connectivity(xgrid, ygrid, zgrid)  Next up will be loading things in using the yt.load_hexahedral_mesh function, so let\u0026rsquo;s look at the docstring for that.\nyt.load_hexahedral_mesh?  Signature: yt.load_hexahedral_mesh( data, connectivity, coordinates, length_unit=None, bbox=None, sim_time=0.0, mass_unit=None, time_unit=None, velocity_unit=None, magnetic_unit=None, periodicity=(True, True, True), geometry='cartesian', unit_system='cgs', ) Docstring: Load a hexahedral mesh of data into yt as a :class:`~yt.frontends.stream.data_structures.StreamHandler`. This should allow a semistructured grid of data to be loaded directly into yt and analyzed as would any others. This comes with several caveats: * Units will be incorrect unless the data has already been converted to cgs. * Some functions may behave oddly, and parallelism will be disappointing or non-existent in most cases. * Particles may be difficult to integrate. Particle fields are detected as one-dimensional fields. The number of particles is set by the \u0026quot;number_of_particles\u0026quot; key in data. Parameters ---------- data : dict This is a dict of numpy arrays, where the keys are the field names. There must only be one. Note that the data in the numpy arrays should define the cell-averaged value for of the quantity in in the hexahedral cell. connectivity : array_like This should be of size (N,8) where N is the number of zones. coordinates : array_like This should be of size (M,3) where M is the number of vertices indicated in the connectivity matrix. bbox : array_like (xdim:zdim, LE:RE), optional Size of computational domain in units of the length unit. sim_time : float, optional The simulation time in seconds mass_unit : string Unit to use for masses. Defaults to unitless. time_unit : string Unit to use for times. Defaults to unitless. velocity_unit : string Unit to use for velocities. Defaults to unitless. magnetic_unit : string Unit to use for magnetic fields. Defaults to unitless. periodicity : tuple of booleans Determines whether the data will be treated as periodic along each axis geometry : string or tuple \u0026quot;cartesian\u0026quot;, \u0026quot;cylindrical\u0026quot;, \u0026quot;polar\u0026quot;, \u0026quot;spherical\u0026quot;, \u0026quot;geographic\u0026quot; or \u0026quot;spectral_cube\u0026quot;. Optionally, a tuple can be provided to specify the axis ordering -- for instance, to specify that the axis ordering should be z, x, y, this would be: (\u0026quot;cartesian\u0026quot;, (\u0026quot;z\u0026quot;, \u0026quot;x\u0026quot;, \u0026quot;y\u0026quot;)). The same can be done for other coordinates, for instance: (\u0026quot;spherical\u0026quot;, (\u0026quot;theta\u0026quot;, \u0026quot;phi\u0026quot;, \u0026quot;r\u0026quot;)). File: ~/yt/yt/yt/frontends/stream/data_structures.py Type: function  Looks like we\u0026rsquo;ve got just about everything except for the bounding box. So let\u0026rsquo;s generate that.\nbbox = np.array([ [xgrid.min(), xgrid.max()], [ygrid.min(), ygrid.max()], [zgrid.min(), zgrid.max()] ]) bbox  array([[0.00000000e+00, 7.70275059e+09], [0.00000000e+00, 3.14159265e+00], [0.00000000e+00, 6.28318531e+00]])  And the last thing we need to do is to specify the geometry. This dataset is spherical (but with only one value along the azimuthal direction, in this case) and so we have to specify which coordinate is which. We do that using the geometry keyword argument, where we specify not only the geometry\u0026rsquo;s name, but the axis-ordering.\nHere we have:\n r is the first dimension theta is the declination, which spans an interval of $\\pi$, so it\u0026rsquo;s the second dimension phi is our azimuthal angle, which goes $2\\pi$ (and here is identical at all azimuthal angles) so it\u0026rsquo;s the third.  ds = yt.load_hexahedral_mesh(data, conn, coord, bbox = bbox, geometry = (\u0026quot;spherical\u0026quot;, ('r', 'theta', 'phi')))  s = yt.SlicePlot(ds, \u0026quot;phi\u0026quot;, \u0026quot;e_int\u0026quot;) s.show()  Looks kinda right! Unfortunately it\u0026rsquo;s a bit tricky to navigate these coordinates in the version of yt I\u0026rsquo;m running, so we have to do a bit of work to get ourselves zoomed in to see things closer up.\ns.set_center( (0.0, 0.0) ) s.zoom(50) s.pan_rel((0.5, 0.0))  We\u0026rsquo;re now at the point that things can be used, but \u0026hellip; it was kind of irritating getting here!\nWhat have we learned? Good question! The first thing I thought as I was going through this was that I did an awful lot of work for what amounted to a one-to-one mapping between the dataset and the arguments that yt expected. It feels like I should just be able to mark all of this up with some metadata to make the loading process much easier, before writing a specific frontend that manages this all for us.\nOver the last few days, a collaborator (Sam Walkow) at UIUC and I started brainstorming what a metadata schema would look like that could map from a file format directly to yt. One of the advantages of having this explicitly typed and validated is that we can check for errors earlier in the process and we can also make logical leaps from one piece of information to another.\nFor instance, note that above we define the bbox independently of the coordinate system. But, for a dataset like this, they will usually be the same! And, since we\u0026rsquo;re dealing with an HDF5 dataset, we should be able to just specify that.\nWe\u0026rsquo;ve started drafting what this might look like, using JSON-Schema via PyDantic and some helper classes. Ideally, we should be able to specify something in yaml that describes how to get everything we want. Imagine, if instead of what we showed above, we had a small schema that yt could read and that could potentially even live as a sidecar file! (And then we could even register a mimetype for display in Jupyterlab\u0026hellip;) We\u0026rsquo;ve been exploring using this for declarative analysis, to reduce the burden of writing out lots of boilerplate code, but it seems like it would be a great match for this, too.\nAnyway, I was speculating that something like this \u0026ndash; but accounting for an explicitly declared list of fields, etc \u0026ndash; might be sufficient.\nmetadata: filetype: hdf5 geometry: dimensions: /mesh/array_dimensions spherical: order: [r, theta, phi] bbox: implicit hexahedral_mesh: edges: [/mesh/x_ef, /mesh/y_ef, /mesh/z_ef] fields: implicit: true root: /fluid validate: true process: transpose  This doesn\u0026rsquo;t yet work, but I think it gives a target for how we might think about making it much easier to load in data in formats that have such a nice mapping between yt and the dataset. And it leaves enough room for things like on-demand loading (which the stream frontend does support, but in an opaque way) that it could be quite nice to extend this in the future.\nI\u0026rsquo;ve pretty much talked myself into doing something like this and seeing how it\u0026rsquo;d work. Maybe it could be a SciPy project next week!\nAcknowledgments Thanks to Bronson Messer, Ronan Hix and Samantha Walkow for the data and speculating, and to Nathan Goldbaum for planting this \u0026ldquo;let\u0026rsquo;s just write a yaml declaration\u0026rdquo; idea in my head a million years ago. (It took me a while to catch up to how insightful an idea that was.)\nIt\u0026rsquo;s also probably worth mentioning that these amazing revelations I\u0026rsquo;m having are \u0026hellip; not that new to lots of people using data! I mean, folks working on things like wcsaxes and netCDF metadata and lots of others have trod this ground before!\n","date":1562010461,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562010704,"objectID":"1c52fd1b51a3ba39bf88723b2a21cb53","permalink":"https://matthewturk.github.io/post/loading_data_in_yt/","publishdate":"2019-07-01T14:47:41-05:00","relpermalink":"/post/loading_data_in_yt/","section":"post","summary":"In this blogpost, I walk through the annoying bits about loading unknown data into yt.","tags":[],"title":"Loading data in yt: Can we make it better?","type":"post"},{"authors":[],"categories":[],"content":"  tl;dr The Whole Tale leverages neat stuff like Jupyter, repo2docker and RStudio, building out a research environment that gives you access to any data, anywhere \u0026ndash; and when you stop you can pick right up where you left off, with your data, your research, and even any customizations or preferences you\u0026rsquo;ve set.\nYou can even try out the new \u0026ldquo;give me a dataset URL\u0026rdquo; page that I made that isn\u0026rsquo;t quite amazing but is still fun. (It\u0026rsquo;s over here for now but hopefully will move some day.)\nWhole Tale v0.7 Last week at NCSA we had our twice-annual \u0026ldquo;all-hands\u0026rdquo; meeting for the Whole Tale project, which we\u0026rsquo;ve been working on for a few years. It\u0026rsquo;s a collaboration between University of Illinois, University of Chicago, University of Texas at Austin, Notre Dame, and UC Santa Barbara.\nAnd this week we finally, finally, finally decided: we\u0026rsquo;ve been way too cautious in asking people to use the system, and because of that, we haven\u0026rsquo;t really talked about some of the really awesome stuff that\u0026rsquo;s been developed for it. Everything is developed fully in the open on our whole-tale org and wherever we can, we push changes upstream and coordinate with other projects.\nSo, here we go: what is in the hot-off-the-presses Whole Tale v0.7? (We have a release video, too! Also, a much longer one from our previous release, if that interests you.)\nBut first, you might be asking, what is the Whole Tale project, anyway? (Of course there is a paper out there, but it\u0026rsquo;s a lot longer than this blog post.)\n\u0026ldquo;Whole Tale,\u0026rdquo; really? It\u0026rsquo;s a pun! See, it\u0026rsquo;s supposed to both tell the \u0026ldquo;whole story\u0026rdquo; and it\u0026rsquo;s supposed to work for researchers at both ends of the distribution of data sizes \u0026ndash; both the \u0026ldquo;long tail\u0026rdquo; and the other bit.\nThe Whole Tale (hereafter just \u0026lsquo;WT\u0026rsquo;) is designed to be a way to \u0026ndash; first and foremost \u0026ndash; do your research, at any stage in the research lifecycle. That means it should work for exploring data, for refining your processing of data, and all the way up to the point where you want to package it up and publish it. And then, when someone wants to re-explore your work in a different way, or to apply your work to a different dataset, they can do so within this same environment.\nI must confess, writing it like this sounds a bit underwhelming! But there are a couple big hangups with that \u0026ndash; for starters, everybody has a different environment they like using. And, folks usually want to apply their work to some kind of data, which might live somewhere else and might even be lots of bits from lots of places. The final thing that is usually really tricky is that folks usually want to be able to have some stuff that hangs around in between sessions, or in between different research projects.\nWT tries pretty hard to solve these different problems to make a really nice place to work on (and ultimately, publish!) research. And by \u0026ldquo;solve,\u0026rdquo; the idea isn\u0026rsquo;t to just build stuff from scratch, but to find things that exist, glue them together, and make the experience as near-seamless as possible.\nFor these examples, let\u0026rsquo;s imagine that our researcher is \u0026hellip; me, I guess. And let\u0026rsquo;s say that I want to analyze some data from Illinois, where I live. Usually I analyze data in Jupyter, so let\u0026rsquo;s start with that.\nData in there The first pain point that WT aims to address is that of data. Many research workflows \u0026ndash; from the very start all the way up to the very end \u0026ndash; require having access to datasets. Sometimes those datasets are created by the individual researcher, but in many cases, they aren\u0026rsquo;t. And so starting a research project often means answering (even just implicitly!) a bunch of questions about that data.\nWhere is the data?\nHow do I get the data into my working environment?\nCan I grab a bunch of data from lots of other places?\nWT handles data through the notion of \u0026ldquo;registration.\u0026rdquo; When a URL is ingested in the system, it stores some information about that data, and it figures out the best way to get at that data. In the default case, WT will access it via HTTP. It\u0026rsquo;s particularly good at registering and accessing data through Dataverse, DataONE and Globus.\nUnder the hood, WT uses a system called Girder for storing metadata about registered items, where they are located (as a hinting system for where to launch workers), what the access controls are, and what \u0026ldquo;collections\u0026rdquo; they belong to.\nHere\u0026rsquo;s the part that I like about this the most: WT does not just slurp up the data and make a copy locally. Instead, it references the remote data and fetches subsets of it on-demand. And more to the point, it does this by creating a virtual filesystem that makes everything look like they\u0026rsquo;re files there on disk.\nSo I\u0026rsquo;ll use some open data from the State \u0026ndash; I\u0026rsquo;m curious about the distribution of rest areas in the state, so let\u0026rsquo;s use that data set. I grabbed the URL for the CSV by right-clicking on \u0026ldquo;Download\u0026rdquo; and copying the URL. So now when I \u0026ldquo;register\u0026rdquo; this in Whole Tale, I\u0026rsquo;ll see it as a CSV file right there in the filesystem.\n(And I haven\u0026rsquo;t had to download anything to my laptop yet.)\nThere are a handful of special things about this, and a few drawbacks. WT knows how to register data from DataVerse and DataONE, and it can access anything over HTTP as long as the server sends the size back in the HTTP headers. And you can access the file just like it really lived there.\nA Welcome Mat When I open up my laptop, it\u0026rsquo;s basically exactly where I left it the last time I closed it. I\u0026rsquo;ve got tabs open in my browser, in-progress proposals in text editors, and all of my preferences are right there. When I open up vim it\u0026rsquo;s got all my plugins and keybindings still set up.\nWhole Tale wants to feel as much like home as your laptop. So in between sessions, in between different setups, you get to keep a home directory. And when you collaborate with somebody else, the \u0026ldquo;Tale\u0026rdquo; you work on will have a shared workspace directory.\nWhen you use an environment in WT, there\u0026rsquo;ll be three directories that you will see: home, workspace and data. Home is yours, and it\u0026rsquo;ll be there when you quit and come back. If you run this time in RStudio and stick stuff in home, it\u0026rsquo;ll be there when you run again in Jupyterlab. In workspace, stuff that gets saved shows up the next time somebody runs that Tale \u0026ndash; it isn\u0026rsquo;t quite involatile data, but it\u0026rsquo;s also not really personal data. So this is where you might put intermediate results, or helpful scripts.\nEverything under data, of course, is data that exists \u0026hellip; elsewhere. It\u0026rsquo;s all read only, but you can dynamically add new things to the running Tale \u0026ndash; not just things that you\u0026rsquo;ve registered, but things that show up in the WT catalog.\nOne of my favorite parts of WT, though, is the way that the \u0026ldquo;home\u0026rdquo; directory is made available. Not only can you access it through the WT user interface, but you can even mount it as a directory on your laptop or desktop \u0026ndash; it is exposed as a WebDAV filesystem! OSX, Windows and Linux all have native support for this. So if you\u0026rsquo;re working on getting some little scripts or maybe some supplemental data in, you can just drag it over to the WT folder on your desktop.\nSo you\u0026rsquo;ve got data, maybe from lots of places, you\u0026rsquo;ve got an environment, and every time you log in it feels like \u0026ldquo;home\u0026rdquo; \u0026ndash; now let\u0026rsquo;s imagine you\u0026rsquo;ve done some work and you decide, eventually, that you are done. Your work is ready to be published!\nCollaborating and Remixing This part is the most fun \u0026ndash; within a given Tale, we\u0026rsquo;ve got the ability to share files across instances of that Tale (through the \u0026ldquo;workspace\u0026rdquo; folder) and you can also grab somebody else\u0026rsquo;s published tale and mess about with it \u0026ndash; changing the environment, doing different things, and so on and so on.\nTry it out! Give it a shot! There are still some rough edges, but it should mostly work \u0026ndash; and I\u0026rsquo;ve personally been using it already to collaborate with students on projects and papers.\nYou can find out more about the project at the Whole Tale Website and all of the source is in the whole-tale GitHub org. And if you want to try it out on your own resources, all of our development and deployment scripts are in our deploy-dev repository.\nPlus, we\u0026rsquo;d love to work with you to add integrations for new data stores, hear about cool ideas you might have, and if you do something fun with it, we definitely want to hear about that!\n","date":1561735964,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561737867,"objectID":"45d6bb9b5c7bd653853b0da2ffb15ba8","permalink":"https://matthewturk.github.io/post/exploration-whole-tale/","publishdate":"2019-06-28T10:32:44-05:00","relpermalink":"/post/exploration-whole-tale/","section":"post","summary":"What is this Whole Tale thing?","tags":[],"title":"Whole Tale: Exploration, Analysis and Reproducibility","type":"post"},{"authors":[],"categories":[],"content":" tl;dr: kaitai struct is awesome.\nFile formats can be pretty annoying \u0026ndash; especially when you figure them out through weird combinations of reverse-engineering, hand-me-down code and trial-and-error.\nWhat we\u0026rsquo;ve ended up with in yt is a bunch of data formats where the process of conducting the IO is all mixed up with the description of that IO. This means that any attempt to update things (which I\u0026rsquo;ve alluded to in these blog posts) requires a fair bit of care to make sure that the process is not disruptive in any way.\nIn preparation for this, I set out to start writing down some of the file formats in a flat text format by going through my old notes and the code in the various yt io.py files. Before too long, I decided to instead start utilizing kaitai struct instead.\nI\u0026rsquo;ve found Kaitai struct to be useful in other projects \u0026ndash; for instance, I have been using it as a method to reverse engineer the data files from an old DOS game I used to play with my brother back in the early 90s. The combination of the simple (but reasonably flexible) syntax, the ruby-based visualizer and the WebIDE make it really good for exploratory reverse engineering.\nAnd, it generates code! You can run:\n$ ksc -t python somefile.ksy  and it\u0026rsquo;ll generate a python reader. It can also generate javascript, java, go, etc, and I think rust is in the works.\nSo the question became: could we use kaitai struct for documenting a data format like, say, the binary format from GADGET-2? After that we can really stretch our legs and try it on more complicated ones, but let\u0026rsquo;s start with this one.\nGadget-2 Data Format The (vanilla) Gadget-2 data format is reasonably straightforward, and it\u0026rsquo;s even described in some detail in the Gadget manual. You have a header, then sets of records. Each of these sets of records has a separating value (sometimes!). According to table 3 in the user guide, the file looks something like:\n Header Position Velocity ID Mass (only for variable mass particles) Energy (only for gas particles) Density (Only for gas particles) Smoothing length (Only for gas particles) Potential (if enabled in makefile) Acceleration (if enabled in makefile) Endtime (if enabled in makefile) Timestep (if enabled in makefile)  Gadget-2 is probably the most widely-used astrophysics simulation code, but one of its defining characteristics is that it is quite readily hackable to include additional parameters, additional particle attributes, and lots more physics. This usually means that when you receive binary gadget data, you need to know what to expect in the file \u0026ndash; the individual file formats are typically not self-describing.\nThe header is often left unchanged, at least in the data I\u0026rsquo;ve seen. In yt we have a simple specification system to allow for modifications to both the fields and the header, but the \u0026ldquo;vanilla\u0026rdquo; header looks something like this (in python struct notation):\ndefault = (('Npart', 6, 'i'), ('Massarr', 6, 'd'), ('Time', 1, 'd'), ('Redshift', 1, 'd'), ('FlagSfr', 1, 'i'), ('FlagFeedback', 1, 'i'), ('Nall', 6, 'i'), ('FlagCooling', 1, 'i'), ('NumFiles', 1, 'i'), ('BoxSize', 1, 'd'), ('Omega0', 1, 'd'), ('OmegaLambda', 1, 'd'), ('HubbleParam', 1, 'd'), ('FlagAge', 1, 'i'), ('FlagMetals', 1, 'i'), ('NallHW', 6, 'i'), ('unused', 16, 'i'))  Before we get any further, let\u0026rsquo;s check if we can parse just this with kaitai struct.\nOur First ksy File In its simplest form, Kaitai specifications allow specifying the format of data and the order that different types of data will arrive in. The base data types it has are standard numerical data types, bytes, strings, arrays and so on.\nKaitai has a little bit of metadata we will add at the beginning, so we start with this preamble:\nmeta: id: gadget_format1 endian: le  This gives it a name and notes it as little endian.\nIf we wanted to build a parser for the header, the most obvious thing we could do would be to parse the different items in order. The very first item is an array of 6 integers that describes the number of particles in the simulation, separated by each particle type. This comes in a sequence starting at the beginning of our \u0026ldquo;stream,\u0026rdquo; so we can use the seq top-level type to start parsing:\nseq: - id: recsize_0 type: u4 - id: npart1 type: u4 - id: npart2 type: u4 - id: npart3 type: u4 - id: npart4 type: u4 - id: npart5 type: u4 - id: npart6 type: u4  recsize_0 here is because we know that our header will be preceded by a value indicating how many bytes are in it \u0026ndash; this is a common practice, but not universal. And I\u0026rsquo;ve said that each of the variables (npart1 \u0026hellip; npart6) is of type u4, which means \u0026ldquo;unsigned four byte integers.\u0026rdquo;\nThis isn\u0026rsquo;t that efficient, though \u0026ndash; each particle type has its own variable. KS provides us with the option to repeat which can take an expression. Now we can enable grabbing arrays of values, so we can use that, instead. Let\u0026rsquo;s change this to:\nseq: - id: recsize_0 type: u4 - id: npart type: u4 repeat: expr repeat-expr: 6  (You can also do repeat: eos for end of stream and repeat-until.) The result now is that npart is an array \u0026ndash; so later on when we loop, we can look it up by using the special variable _index inside another repeat-expr. We\u0026rsquo;ll look at that a bit later.\nData Types in ksy Parsing the entire header in this way is certainly possible, but it also can be a bit clunky \u0026ndash; and can be harder for maintainability. Instead of writing all of our header values out, let\u0026rsquo;s use a user-defined type.\nThe top-level key types is where user-defined types are described; you can do some pretty fancy things like supply parameters to them, but we\u0026rsquo;ll just use it to hold the data we know.\ntypes: gadget_header: seq: ...  Now, in our top-level seq section, we can just reference the header type:\nseq: -id: header type: gadget_header  Big Arrays and Lazy-reading The tricky bit comes in when we get to our arrays of values. Kaitai has several different languages it can generate for \u0026ndash; Python, C#, C++, ruby, and others. When reading lots of little items, python can get bogged down. For instance, this is the most obvious way of describing one of the position arrays:\n-id: coordinates_part1 type: f4 repeat: expr repeat-expr: header.npart[0] * 3  The generated python code will read a series of 4-byte objects, one at a time, and append them to a list. The combination of these things results in a lot of overhead from the python language runtime and the type system, so it ends up being rather slow.\nOne way to get around this is to use what\u0026rsquo;s called an instance. This allows parsing to happen only when requested, and it can also exist outside of the rest of the parsing structure.\nImportant note: There are some fiddly issues with making instance objects work and how they relate to substreams of IO which I\u0026rsquo;m going to sidestep here. Let\u0026rsquo;s just assume that it works, instead!\nBy default, we can read in bytes for these individual objects; this lets us read a big block of data (which we can deal with later \u0026ndash; for instance, inside python!) and only parse into individual floats when we request. This might look something like defining a custom type with both a seq and an instances section:\ntypes: f4_array_type: params: - id: count type: u4 seq: - id: buffer size: 4 * count instances: values: pos: 0 size-eos: true id: entries: type: f4 repeat: eos  Here we are saying, just read the bytes. But, if anybody asks, this is what the attribute values should look like \u0026ndash; you should parse it starting at 0, go to the end of the stream, and assume it\u0026rsquo;s all f4-typed items. (I am not certain that both size-eos and repeat-eos are necessary.)\nWith these components, we should be able to parse our entire gadget binary file, a\nWorking Implementation I\u0026rsquo;ve gotten this to the point that it mostly works, and I\u0026rsquo;ve been committing to data-exp-lab/astro-data-formats. I ended up splitting the array stuff into its own parameterized type that I store in a separate file, so that we can potentially reuse it in other file formats.\nHere\u0026rsquo;s some of the high-level stuff:\nmeta: id: gadget_format1 endian: le ks-opaque-types: true imports: - array_buffer seq: - id: gadget_header type: header - id: coordinates type: particle_fields('f4', 3) - id: velocities type: particle_fields('f4', 3) - id: particle_ids type: particle_fields('u4', 1) types: header: seq: - id: recsize_0 type: u4 - id: npart type: u4 repeat: expr repeat-expr: 6 - id: massarr type: f8 repeat: expr repeat-expr: 6 [ ... ] particle_fields: params: - id: field_type type: str - id: components type: u1 seq: - id: magic1 type: u4 - id: fields type: field(_index, components, field_type) repeat: expr repeat-expr: 6 - id: magic2 type: u4 field: params: - id: index type: u1 - id: components type: u1 - id: field_type type: str seq: - id: field size: _root.gadget_header.npart[index] * components * 4 type: array_buffer(field_type)  And then the array_buffer is an instance-based way of getting the raw bytes. Right now this is reasonably fast, and there\u0026rsquo;s a possibility I could simplify the structure and flatten it out a bit, but it works exactly as I want it to.\nFuture The next thing I\u0026rsquo;m going to do is work on porting the RAMSES frontend to this format. That will enable some more exploration and validation of the different outputs, and hopefully use that as guidance for any future modifications to the IO systems. One of the stickier wickets I\u0026rsquo;ve seen is that it\u0026rsquo;s possible to use memory-mapping, but I think there are some subtle places where data is read into memory and copied \u0026ndash; this kind of gets rid of the purpose of the memory mapping. I hope to explore and dig more deeply into this later.\nUltimately, I believe a combination of ksy files for binary formats and a ksy-like dialect for describing the connection between chunks of data and physical space locations will simplify a considerable amount of data analysis.\nI\u0026rsquo;d also like to thank @GreyCat on gitter, who was super helpful in helping me work out some of the bits I wasn\u0026rsquo;t clear on.\n","date":1561054541,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561054675,"objectID":"151f1bca4c9b3688b837b481cd15c68d","permalink":"https://matthewturk.github.io/post/kaitai-struct-scientific-data/","publishdate":"2019-06-20T13:15:41-05:00","relpermalink":"/post/kaitai-struct-scientific-data/","section":"post","summary":"tl;dr: kaitai struct is awesome.\nFile formats can be pretty annoying \u0026ndash; especially when you figure them out through weird combinations of reverse-engineering, hand-me-down code and trial-and-error.\nWhat we\u0026rsquo;ve ended up with in yt is a bunch of data formats where the process of conducting the IO is all mixed up with the description of that IO. This means that any attempt to update things (which I\u0026rsquo;ve alluded to in these blog posts) requires a fair bit of care to make sure that the process is not disruptive in any way.","tags":[],"title":"Kaitai Struct and Scientific Data","type":"post"},{"authors":[],"categories":null,"content":"","date":1560877562,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560877679,"objectID":"ceb7bcdee46f24006f72ca99f8fb9304","permalink":"https://matthewturk.github.io/talk/2019-06-18-oshiw-numfocus/","publishdate":"2019-06-18T13:06:02-04:00","relpermalink":"/talk/2019-06-18-oshiw-numfocus/","section":"talk","summary":"In this talk, I present a retrospective on the origins and goals of the NumFOCUS software sustainability project I was involved in in 2016-2017.","tags":[],"title":"OSHIW: NumFOCUS Sustainability","type":"talk"},{"authors":[],"categories":[],"content":" Welcome to part 3 of a series on how yt deals with data and the ways that helps and hinders things! This time, I am going to describe what \u0026ldquo;chunks\u0026rdquo; of data (YTDataChunk) in yt are, and a few characteristics of them that wouldn\u0026rsquo;t be obvious from the previous blog posts.\nChunks have spatial attributes Data chunks in yt have a set of special attributes that help yt to put them in the context of the coordinate domain.\n(This is as good a time as any to note to myself that I should probably write up a blog post about how coordinate handling works, as opposed to index handling.)\nWhen you receive a chunk of a data object, that chunk will have with it information about the coordinate space of the data contained within it \u0026ndash; specifically, if it is a dataset that is volumetrically discretized in some regular way, it will have information about the centers of the individual grid cells, the sizes of those cells, and some measure of their resolution.\nFor a grid dataset specifically, these attributes will always exist on a data chunk:\n fcoords - the centers of the grid cells, of shape (..., 3) fwidth - the \u0026ldquo;width\u0026rdquo; of the grid cells, of shape (..., 3) icoords - the integer coordinates, with respect to the current resolution, of the grid cells, of shape (..., 3) ires - the \u0026ldquo;level of resolution\u0026rdquo; of a given cell; this mostly makes sense for datasets where there is some universal refinement ratio and a fixed value for the domain dimensions, of shape (...,)  There are a couple more that are specific to the individual data selection operation \u0026ndash; for instance, tcoords only makes sense if there is a parameterized vector being pushed through the domain. There\u0026rsquo;s also fcoords_vertex but it is seldom used except in unstructured mesh datasets.\nLet\u0026rsquo;s take a look at these values, and how they correspond to grid attributes:\nimport yt ds = yt.load(\u0026quot;data/IsolatedGalaxy/galaxy0030/galaxy0030\u0026quot;)  /home/matthewturk/conda-py3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 88 from C header, got 96 from PyObject return f(*args, **kwds) yt : [INFO ] 2019-06-17 22:23:02,723 Parameters: current_time = 0.0060000200028298 yt : [INFO ] 2019-06-17 22:23:02,724 Parameters: domain_dimensions = [32 32 32] yt : [INFO ] 2019-06-17 22:23:02,726 Parameters: domain_left_edge = [0. 0. 0.] yt : [INFO ] 2019-06-17 22:23:02,728 Parameters: domain_right_edge = [1. 1. 1.] yt : [INFO ] 2019-06-17 22:23:02,734 Parameters: cosmological_simulation = 0.0  What yt is doing here is taking an individual, in-memory sparse index and then expanding that index into a fully-fledged array. Internally, when we ask for any of these attributes, yt iterates over all the objects that belong in a given chunk and then it expands the values associated with those objects.\nWe can see this if we poke a little closer at the Grid objects in this case.\nfor g in ds.index.grids: pass  Parsing Hierarchy : 100%|██████████| 173/173 [00:00\u0026lt;00:00, 3064.43it/s] yt : [INFO ] 2019-06-17 22:23:02,831 Gathering a field list (this may take a moment.)  This is used internally in yt whenever we want to deal with a geometric selection, or when we apply geometric processing. For instance, the \u0026ldquo;projection\u0026rdquo; operator can use the integer coordinate system to very rapidly insert values into a quadtree. yt can iterate over all of the chunks that belong to a data object and insert them into the quadtree based on their icoords, and it can refine nodes by bit shifting the appropriate amount.\nFor grid data, icoords also gives us some handy things for evaluating relationships between objects. For instance, we might have a root grid with one child grid. We can figure out their relationship by looking at their icoords, ires and the result of get_global_startindex(). In our sample dataset, let\u0026rsquo;s look at some random grid \u0026ndash; I happen to know the first level 8 grid is index 27, so let\u0026rsquo;s use that.\nchild = ds.index.grids[27] parent = child.Parent  parent.icoords  array([[2048, 2048, 2048], [2048, 2048, 2049], [2048, 2048, 2050], ..., [2087, 2089, 2069], [2087, 2089, 2070], [2087, 2089, 2071]])  child.icoords  array([[4096, 4096, 4096], [4096, 4096, 4097], [4096, 4096, 4098], ..., [4133, 4129, 4119], [4133, 4129, 4120], [4133, 4129, 4121]])  parent.LeftEdge, child.LeftEdge  (YTArray([0.5, 0.5, 0.5]) code_length, YTArray([0.5, 0.5, 0.5]) code_length)  They both start at the same place \u0026ndash; so that would suggest that their minimum icoords should refer to the same location.\ndifference = child.ires.min() - parent.ires.max() left_edge_child = child.icoords.min(axis=0) left_edge_parent = parent.icoords.min(axis=0) (left_edge_child - left_edge_parent)  array([2048, 2048, 2048])  This seems odd until we recognize that ires differs between the two, and in this case refers to the bit shifting we need to do to match them up.\n(left_edge_child \u0026gt;\u0026gt; difference) - left_edge_parent  array([0, 0, 0])  We can do this with the next grid up, as well.\nleft_edge_gparent = parent.Parent.icoords.min(axis=0) difference_g = child.ires.min() - parent.Parent.ires.max() (left_edge_child \u0026gt;\u0026gt; difference_g) - left_edge_gparent  array([0, 0, 0])  There\u0026rsquo;s more to this story \u0026ndash; for instance, non-power-of-two differences, but the idea is consistent; both integer and float positioning can be used between chunks, sub-chunks, and so on.\nWe can also pretty easily get cell positions, and if we access data objects, we receive all the values across all sub-objects.\nsp = ds.sphere(\u0026quot;c\u0026quot;, 0.1) print(sp.fwidth)  [[0.00024414 0.00024414 0.00024414] [0.00024414 0.00024414 0.00024414] [0.00024414 0.00024414 0.00024414] ... [0.00012207 0.00012207 0.00012207] [0.00012207 0.00012207 0.00012207] [0.00012207 0.00012207 0.00012207]] code_length  Chunks can be sub-chunked The other main advantage of chunks is that they can be sub-chunked. Usually this only means up to one level, but it means that we could in principle do something like this:\ndd = ds.all_data() for i, chunk1 in enumerate(dd.chunks([], \u0026quot;io\u0026quot;)): print(\u0026quot;Chunk \u0026quot;, i, len(chunk1._current_chunk.objs)) print(\u0026quot; \u0026quot;, end = \u0026quot;\u0026quot;) for j, chunk2 in enumerate(dd.chunks([], \u0026quot;spatial\u0026quot;)): print(chunk2._current_chunk.objs, end = \u0026quot; \u0026quot;) print() print()  Chunk 0 5 [EnzoGrid_0001] [EnzoGrid_0075] [EnzoGrid_0076] [EnzoGrid_0082] [EnzoGrid_0110] Chunk 1 1 [EnzoGrid_0073] Chunk 2 20 [EnzoGrid_0009] [EnzoGrid_0010] [EnzoGrid_0011] [EnzoGrid_0012] [EnzoGrid_0013] [EnzoGrid_0014] [EnzoGrid_0015] [EnzoGrid_0016] [EnzoGrid_0017] [EnzoGrid_0018] [EnzoGrid_0019] [EnzoGrid_0020] [EnzoGrid_0021] [EnzoGrid_0022] [EnzoGrid_0023] [EnzoGrid_0024] [EnzoGrid_0025] [EnzoGrid_0026] [EnzoGrid_0027] [EnzoGrid_0028] Chunk 3 28 [EnzoGrid_0008] [EnzoGrid_0029] [EnzoGrid_0030] [EnzoGrid_0031] [EnzoGrid_0032] [EnzoGrid_0033] [EnzoGrid_0034] [EnzoGrid_0035] [EnzoGrid_0036] [EnzoGrid_0037] [EnzoGrid_0038] [EnzoGrid_0039] [EnzoGrid_0040] [EnzoGrid_0041] [EnzoGrid_0042] [EnzoGrid_0043] [EnzoGrid_0044] [EnzoGrid_0045] [EnzoGrid_0046] [EnzoGrid_0047] [EnzoGrid_0048] [EnzoGrid_0049] [EnzoGrid_0050] [EnzoGrid_0051] [EnzoGrid_0052] [EnzoGrid_0053] [EnzoGrid_0054] [EnzoGrid_0055] Chunk 4 20 [EnzoGrid_0007] [EnzoGrid_0056] [EnzoGrid_0057] [EnzoGrid_0058] [EnzoGrid_0059] [EnzoGrid_0060] [EnzoGrid_0061] [EnzoGrid_0062] [EnzoGrid_0063] [EnzoGrid_0064] [EnzoGrid_0065] [EnzoGrid_0066] [EnzoGrid_0067] [EnzoGrid_0068] [EnzoGrid_0069] [EnzoGrid_0070] [EnzoGrid_0071] [EnzoGrid_0072] [EnzoGrid_0074] [EnzoGrid_0077] Chunk 5 16 [EnzoGrid_0006] [EnzoGrid_0078] [EnzoGrid_0079] [EnzoGrid_0080] [EnzoGrid_0081] [EnzoGrid_0083] [EnzoGrid_0084] [EnzoGrid_0085] [EnzoGrid_0086] [EnzoGrid_0087] [EnzoGrid_0088] [EnzoGrid_0089] [EnzoGrid_0090] [EnzoGrid_0091] [EnzoGrid_0092] [EnzoGrid_0093] Chunk 6 13 [EnzoGrid_0005] [EnzoGrid_0094] [EnzoGrid_0095] [EnzoGrid_0096] [EnzoGrid_0097] [EnzoGrid_0098] [EnzoGrid_0099] [EnzoGrid_0100] [EnzoGrid_0101] [EnzoGrid_0102] [EnzoGrid_0103] [EnzoGrid_0104] [EnzoGrid_0105] Chunk 7 14 [EnzoGrid_0004] [EnzoGrid_0106] [EnzoGrid_0107] [EnzoGrid_0108] [EnzoGrid_0109] [EnzoGrid_0111] [EnzoGrid_0112] [EnzoGrid_0113] [EnzoGrid_0114] [EnzoGrid_0115] [EnzoGrid_0116] [EnzoGrid_0117] [EnzoGrid_0118] [EnzoGrid_0119] Chunk 8 26 [EnzoGrid_0003] [EnzoGrid_0120] [EnzoGrid_0121] [EnzoGrid_0122] [EnzoGrid_0123] [EnzoGrid_0124] [EnzoGrid_0125] [EnzoGrid_0126] [EnzoGrid_0127] [EnzoGrid_0128] [EnzoGrid_0129] [EnzoGrid_0130] [EnzoGrid_0131] [EnzoGrid_0132] [EnzoGrid_0133] [EnzoGrid_0134] [EnzoGrid_0135] [EnzoGrid_0136] [EnzoGrid_0137] [EnzoGrid_0138] [EnzoGrid_0139] [EnzoGrid_0140] [EnzoGrid_0141] [EnzoGrid_0142] [EnzoGrid_0143] [EnzoGrid_0144] Chunk 9 30 [EnzoGrid_0002] [EnzoGrid_0145] [EnzoGrid_0146] [EnzoGrid_0147] [EnzoGrid_0148] [EnzoGrid_0149] [EnzoGrid_0150] [EnzoGrid_0151] [EnzoGrid_0152] [EnzoGrid_0153] [EnzoGrid_0154] [EnzoGrid_0155] [EnzoGrid_0156] [EnzoGrid_0157] [EnzoGrid_0158] [EnzoGrid_0159] [EnzoGrid_0160] [EnzoGrid_0161] [EnzoGrid_0162] [EnzoGrid_0163] [EnzoGrid_0164] [EnzoGrid_0165] [EnzoGrid_0166] [EnzoGrid_0167] [EnzoGrid_0168] [EnzoGrid_0169] [EnzoGrid_0170] [EnzoGrid_0171] [EnzoGrid_0172] [EnzoGrid_0173]  (In general, this is about as deep as it goes, although in principle we could do lots more sub-chunking.) This lets you chunk over IO, then chunk over individual objects, and even request things like the number of ghost zones you need in that sub-chunking.\nChunks can retain state during a long-lived IO task We can also store field parameters and reset them during an individual chunking operation. So for instance, we could have a custom field that requires one field parameter that is then swapped out during the next level of chunking.\nThis feature is probably not used much.\nIteration and Chunks But here\u0026rsquo;s the issue we run into, which actually shows up whenever an error is raised during a chunking operation.\nAll of this is done via generator expressions. This seemed like the right thing to do! Just yield everywhere! It was so hip.\nBut, there\u0026rsquo;s an even more problematic part: often, the generator expressions get unrolled into lists anyway. And, it turns out, I can\u0026rsquo;t blame anybody else for this: this particular core element of yt was something I not only put in, but something I felt rather self-satisfied about.\nLet\u0026rsquo;s take a look at this in the routine that does chunking for the io type in grid index datasets:\nds.index._chunk_io??  Signature: ds.index._chunk_io( dobj, cache=True, local_only=False, preload_fields=None, chunk_sizing='auto', ) Docstring: \u0026lt;no docstring\u0026gt; Source: def _chunk_io(self, dobj, cache=True, local_only=False, preload_fields=None, chunk_sizing=\u0026quot;auto\u0026quot;): # local_only is only useful for inline datasets and requires # implementation by subclasses. if preload_fields is None: preload_fields = [] preload_fields, _ = self._split_fields(preload_fields) gfiles = defaultdict(list) gobjs = getattr(dobj._current_chunk, \u0026quot;objs\u0026quot;, dobj._chunk_info) fast_index = dobj._current_chunk._fast_index for g in gobjs: # Force to be a string because sometimes g.filename is None. gfiles[str(g.filename)].append(g) # We can apply a heuristic here to make sure we aren't loading too # many grids all at once. if chunk_sizing == \u0026quot;auto\u0026quot;: chunk_ngrids = len(gobjs) if chunk_ngrids \u0026gt; 0: nproc = np.float(ytcfg.getint(\u0026quot;yt\u0026quot;, \u0026quot;__global_parallel_size\u0026quot;)) chunking_factor = np.ceil(self._grid_chunksize*nproc/chunk_ngrids).astype(\u0026quot;int\u0026quot;) size = max(self._grid_chunksize//chunking_factor, 1) else: size = self._grid_chunksize elif chunk_sizing == \u0026quot;config_file\u0026quot;: size = ytcfg.getint(\u0026quot;yt\u0026quot;, \u0026quot;chunk_size\u0026quot;) elif chunk_sizing == \u0026quot;just_one\u0026quot;: size = 1 elif chunk_sizing == \u0026quot;old\u0026quot;: size = self._grid_chunksize else: raise RuntimeError(\u0026quot;%s is an invalid value for the 'chunk_sizing' argument.\u0026quot; % chunk_sizing) for fn in sorted(gfiles): gs = gfiles[fn] for grids in (gs[pos:pos + size] for pos in range(0, len(gs), size)): dc = YTDataChunk(dobj, \u0026quot;io\u0026quot;, grids, self._count_selection(dobj, grids), cache = cache, fast_index = fast_index) # We allow four full chunks to be included. with self.io.preload(dc, preload_fields, 4.0 * size): yield dc File: ~/yt/yt/yt/geometry/grid_geometry_handler.py Type: method  There\u0026rsquo;s lots going on here, so I\u0026rsquo;ll just grab a few of the most important lines. Specifically, I want to highlight this:\nfor fn in sorted(gfiles): gs = gfiles[fn] for grids in (gs[pos:pos + size] for pos in range(0, len(gs), size)): dc = YTDataChunk(dobj, \u0026quot;io\u0026quot;, grids, self._count_selection(dobj, grids), cache = cache, fast_index = fast_index) # We allow four full chunks to be included. with self.io.preload(dc, preload_fields, 4.0 * size): yield dc  The upshot of this is that we first sorted our grids by which file they\u0026rsquo;re in (on the assumption that we probably want to minimize open/close operations), but then in that, we split them up based on the grid counts we want in each chunk, and then we spit out the chunks (with optional preloading of data) to whatever consumes them.\nAs long as we don\u0026rsquo;t do any preloading it\u0026rsquo;s alright for parallel IO, but we\u0026rsquo;re not going to see any of the benefits of this if we ever turn this into a list.\nNow for the Enzo frontend specifically, let\u0026rsquo;s see how the IO routine works:\nds.index.io._read_fluid_selection??  Signature: ds.index.io._read_fluid_selection(chunks, selector, fields, size) Docstring: \u0026lt;no docstring\u0026gt; Source: def _read_fluid_selection(self, chunks, selector, fields, size): # This function has an interesting history. It previously was mandate # to be defined by all of the subclasses. But, to avoid having to # rewrite a whole bunch of IO handlers all at once, and to allow a # better abstraction for grid-based frontends, we're now defining it in # the base class. rv = {} nodal_fields = [] for field in fields: finfo = self.ds.field_info[field] nodal_flag = finfo.nodal_flag if np.any(nodal_flag): num_nodes = 2**sum(nodal_flag) rv[field] = np.empty((size, num_nodes), dtype=\u0026quot;=f8\u0026quot;) nodal_fields.append(field) else: rv[field] = np.empty(size, dtype=\u0026quot;=f8\u0026quot;) ind = {field: 0 for field in fields} for field, obj, data in self.io_iter(chunks, fields): if data is None: continue if isinstance(selector, GridSelector) and field not in nodal_fields: ind[field] += data.size rv[field] = data.copy() else: ind[field] += obj.select(selector, data, rv[field], ind[field]) return rv File: ~/yt/yt/yt/utilities/io_handler.py Type: method  This is in the base class for the IO handler; some of the grid-based frontends implement it. In this particular case we aren\u0026rsquo;t unrolling the generator, but you can see some of the issues here anyway: we need to know a fair bit about the IO method (thus the io_iter method, which I will show below) and we need to do a lot of obj.select and whatnot.\nThis isn\u0026rsquo;t terribly efficient, and it also means that since we are yielding a generator expression from within a generator expression, we end up having a nested set of loops that don\u0026rsquo;t know their sizes or allow seeking in their stream of yields.\nThis makes interoperating with something like dask \u0026ndash; which works best when it knows the sizes and shapes and can do its own distribution \u0026ndash; much more challenging. And it also means that we have a few layers of relatively opaque routines that conspire to keep us a ways from the file-based abstraction.\nLet\u0026rsquo;s look at the io_iter function to see how it works for Enzo. You can see that it does do a few fun things; most importantly, it keeps the file handle open if it can. This can save a surprising amount of time on parallel file systems, as it reduces the number of metadata lookups necessary.\nds.index.io.io_iter??  Signature: ds.index.io.io_iter(chunks, fields) Docstring: \u0026lt;no docstring\u0026gt; Source: def io_iter(self, chunks, fields): h5_dtype = self._field_dtype for chunk in chunks: fid = None filename = -1 for obj in chunk.objs: if obj.filename is None: continue if obj.filename != filename: # Note one really important thing here: even if we do # implement LRU caching in the _read_obj_field function, # we'll still be doing file opening and whatnot. This is a # problem, but one we can return to. if fid is not None: fid.close() fid = h5py.h5f.open(b(obj.filename), h5py.h5f.ACC_RDONLY) filename = obj.filename for field in fields: nodal_flag = self.ds.field_info[field].nodal_flag dims = obj.ActiveDimensions[::-1] + nodal_flag[::-1] data = np.empty(dims, dtype=h5_dtype) yield field, obj, self._read_obj_field( obj, field, (fid, data)) if fid is not None: fid.close() File: ~/yt/yt/yt/frontends/enzo/io.py Type: method  So to recap, right now: making chunks work nicely with non-yt operations is tricky because of some early design decisions\nNext Up In the next blog post, I\u0026rsquo;m going to present a bit about:\n How particle IO is handled \u0026ndash; and the differences between grid IO (which has lots of differently-shaped chunks) and particle IO Some efforts to refactor particle IO (before it gets released!) A future for how to make all this stuff work better with dask (yes, really, I promise)  ","date":1560825129,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560825481,"objectID":"83bc013dd9d9737134873e743bb020e7","permalink":"https://matthewturk.github.io/post/refactoring-yt-frontends-part3/","publishdate":"2019-06-17T22:32:09-04:00","relpermalink":"/post/refactoring-yt-frontends-part3/","section":"post","summary":"Welcome to part 3 of a series on how yt deals with data and the ways that helps and hinders things! This time, I am going to describe what \u0026ldquo;chunks\u0026rdquo; of data (YTDataChunk) in yt are, and a few characteristics of them that wouldn\u0026rsquo;t be obvious from the previous blog posts.\nChunks have spatial attributes Data chunks in yt have a set of special attributes that help yt to put them in the context of the coordinate domain.","tags":[],"title":"Refactoring yt Frontends - Part 3","type":"post"},{"authors":[],"categories":[],"content":" SIDE NOTE: I intended for this blog post to be a bit shorter than it turned out, and for it to cover some things it \u0026hellip; didn\u0026rsquo;t! So it looks like there\u0026rsquo;ll be a part three in the series.\nOperations on Data Objects In my previous post, I walked through a few aspects of how the chunking system in yt works, mostly focusing on the \u0026quot;io\u0026quot; style of chunking, where the order in which data arrives is not important. This style of chunking lends itself very easily to parallelism, as well as dynamic chunk-sizing; we can see this in how operations such as .max() operate on a data object in yt.\nimport yt ds = yt.load(\u0026quot;data/IsolatedGalaxy/galaxy0030/galaxy0030\u0026quot;) dd = ds.r[:,:,:]  yt : [INFO ] 2019-06-10 12:59:13,433 Parameters: current_time = 0.0060000200028298 yt : [INFO ] 2019-06-10 12:59:13,434 Parameters: domain_dimensions = [32 32 32] yt : [INFO ] 2019-06-10 12:59:13,435 Parameters: domain_left_edge = [0. 0. 0.] yt : [INFO ] 2019-06-10 12:59:13,437 Parameters: domain_right_edge = [1. 1. 1.] yt : [INFO ] 2019-06-10 12:59:13,439 Parameters: cosmological_simulation = 0.0 Parsing Hierarchy : 100%|██████████| 173/173 [00:00\u0026lt;00:00, 5931.42it/s] yt : [INFO ] 2019-06-10 12:59:13,487 Gathering a field list (this may take a moment.)  max_vals = dd.max([\u0026quot;density\u0026quot;, \u0026quot;temperature\u0026quot;, \u0026quot;velocity_magnitude\u0026quot;]) print(max_vals)  (7.73426503924e-24 g/cm**3, 24826104.0 K, 86290042.8768639 cm/s)  I want to highlight something here which sometimes slips by \u0026ndash; if you were to access the array hanging off an object like dd, like this:\ndd[\u0026quot;density\u0026quot;]  YTArray([4.92775113e-31, 4.94005233e-31, 4.93824694e-31, ..., 1.12879234e-25, 1.59561490e-25, 1.09824903e-24]) g/cm**3  The entire array is loaded into memory. This is done through a different chunk style, \u0026quot;all\u0026quot;, which pre-allocates an array and then loads the whole thing into memory in a single go. I should note that there are a few reasons that this needs to always be provided in the same order as the \u0026quot;io\u0026quot; chunking style, which has presented some fun struggles in refactoring that I may mention later on.\nBut above, we aren\u0026rsquo;t accessing dd[\u0026quot;density\u0026quot;].max() and instead are accessing dd.max(\u0026quot;density\u0026quot;) (along with two other fields, temperature and the magnitude of velocity.) This operation, inspired by the numpy / xarray syntax, iterates over chunks and computes the running max.\nThere are a few other fun operations that I mentioned last time like argmax and whatnot, but for now let\u0026rsquo;s just look at max. While the syntactic sugar for calling dd.max() is reasonably recent, the underpinning functionality dates back about a decade and has, from the start, been MPI-parallel. It hasn\u0026rsquo;t always been elegant, but it\u0026rsquo;s been parallel and memory-conservative.\nSo how does .max() (and, its older form, .quantities[\u0026quot;MaximumValue\u0026quot;]()) work? Let\u0026rsquo;s take a look at the source.\ndd.max??  Signature: dd.max(field, axis=None) Source: def max(self, field, axis=None): r\u0026quot;\u0026quot;\u0026quot;Compute the maximum of a field, optionally along an axis. This will, in a parallel-aware fashion, compute the maximum of the given field. Supplying an axis will result in a return value of a YTProjection, with method 'mip' for maximum intensity. If the max has already been requested, it will use the cached extrema value. Parameters ---------- field : string or tuple field name The field to maximize. axis : string, optional If supplied, the axis to project the maximum along. Returns ------- Either a scalar or a YTProjection. Examples -------- \u0026gt;\u0026gt;\u0026gt; max_temp = reg.max(\u0026quot;temperature\u0026quot;) \u0026gt;\u0026gt;\u0026gt; max_temp_proj = reg.max(\u0026quot;temperature\u0026quot;, axis=\u0026quot;x\u0026quot;) \u0026quot;\u0026quot;\u0026quot; if axis is None: rv = () fields = ensure_list(field) for f in fields: rv += (self._compute_extrema(f)[\u001b[0;36m1],) if len(fields) == \u001b[0;36m1: return rv[\u001b[0;36m0] else: return rv elif axis in self.ds.coordinates.axis_name: r = self.ds.proj(field, axis, data_source=self, method=\u0026quot;mip\u0026quot;) return r else: raise NotImplementedError(\u0026quot;Unknown axis %s\u0026quot; % axis) File: ~/yt/yt/yt/data_objects/data_containers.py Type: method  (One fun bit here is that if you supply an axis and it\u0026rsquo;s a spatial axis, this will project along the axis.)\nLooks like it calls ._compute_extrema so let\u0026rsquo;s take a look there:\ndd._compute_extrema??  Signature: dd._compute_extrema(field) Docstring: \u0026lt;no docstring\u0026gt; Source: def _compute_extrema(self, field): if self._extrema_cache is None: self._extrema_cache = {} if field not in self._extrema_cache: # Note we still need to call extrema for each field, as of right # now mi, ma = self.quantities.extrema(field) self._extrema_cache[field] = (mi, ma) return self._extrema_cache[field] File: ~/yt/yt/yt/data_objects/data_containers.py Type: method  (Fun fact: until I saw the source code right now, I was prepared to say that it computed all the extrema in a single go. Glad there\u0026rsquo;s a backspace key. I should probably file an issue.)\nThis calls self.quantities.extrema for each field, since it\u0026rsquo;s nearly just as cheap to do both min and max in a single pass, and sometimes folks\u0026rsquo;ll want both.\nSo we\u0026rsquo;re starting to see the underpinnings here \u0026ndash; .quantities is where lots of the fun things happen. What is it?\ndd.quantities.extrema??  Signature: dd.quantities.extrema(fields, non_zero=False) Type: Extrema String form: \u0026lt;yt.data_objects.derived_quantities.Extrema object at 0x7db454d7a2e8\u0026gt; File: ~/yt/yt/yt/data_objects/derived_quantities.py Source: class Extrema(DerivedQuantity): r\u0026quot;\u0026quot;\u0026quot; Calculates the min and max value of a field or list of fields. Returns a YTArray for each field requested. If one, a single YTArray is returned, if many, a list of YTArrays in order of field list is returned. The first element of each YTArray is the minimum of the field and the second is the maximum of the field. Parameters ---------- fields The field or list of fields over which the extrema are to be calculated. non_zero : bool If True, only positive values are considered in the calculation. Default: False Examples -------- \u0026gt;\u0026gt;\u0026gt; ds = load(\u0026quot;IsolatedGalaxy/galaxy0030/galaxy0030\u0026quot;) \u0026gt;\u0026gt;\u0026gt; ad = ds.all_data() \u0026gt;\u0026gt;\u0026gt; print ad.quantities.extrema([(\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;), ... (\u0026quot;gas\u0026quot;, \u0026quot;temperature\u0026quot;)]) \u0026quot;\u0026quot;\u0026quot; def count_values(self, fields, non_zero): self.num_vals = len(fields) * \u001b[0;36m2 def __call__(self, fields, non_zero = False): fields = ensure_list(fields) rv = super(Extrema, self).__call__(fields, non_zero) if len(rv) == \u001b[0;36m1: rv = rv[\u001b[0;36m0] return rv def process_chunk(self, data, fields, non_zero): vals = [] for field in fields: field = data._determine_fields(field)[\u001b[0;36m0] fd = data[field] if non_zero: fd = fd[fd \u0026gt; \u001b[0;36m0.0] if fd.size \u0026gt; \u001b[0;36m0: vals += [fd.min(), fd.max()] else: vals += [array_like_field(data, HUGE, field), array_like_field(data, -HUGE, field)] return vals def reduce_intermediate(self, values): # The values get turned into arrays here. return [self.data_source.ds.arr([mis.min(), mas.max()]) for mis, mas in zip(values[::\u001b[0;36m2], values[\u001b[0;36m1::\u001b[0;36m2])] Call docstring: Calculate results for the derived quantity  Ah, this is starting to make sense!\nAll the DerivedQuantity objects\nWhat all do we have?\ndd.quantities.keys()  dict_keys(['WeightedAverageQuantity', 'TotalQuantity', 'TotalMass', 'CenterOfMass', 'BulkVelocity', 'WeightedVariance', 'AngularMomentumVector', 'Extrema', 'SampleAtMaxFieldValues', 'MaxLocation', 'SampleAtMinFieldValues', 'MinLocation', 'SpinParameter'])  Looking at these, there\u0026rsquo;s likely a common theme that is pretty obvious \u0026ndash; they\u0026rsquo;re all pretty easily parallelizable things! Sure, there might need to be some reductions at the end, but these are all pretty straightforward combinations of fields and parameters.\nThe way the base class works is interesting, and we can use that to break down what is going on here in a way that demonstrates how this relies on chunking:\nyt.data_objects.derived_quantities.DerivedQuantity??  Init signature: yt.data_objects.derived_quantities.DerivedQuantity(data_source) Docstring: \u0026lt;no docstring\u0026gt; Source: class DerivedQuantity(ParallelAnalysisInterface): num_vals = -\u001b[0;36m1 def __init__(self, data_source): self.data_source = data_source def count_values(self, *args, **kwargs): return def __call__(self, *args, **kwargs): \u0026quot;\u0026quot;\u0026quot;Calculate results for the derived quantity\u0026quot;\u0026quot;\u0026quot; # create the index if it doesn't exist yet self.data_source.ds.index self.count_values(*args, **kwargs) chunks = self.data_source.chunks([], chunking_style=\u0026quot;io\u0026quot;) storage = {} for sto, ds in parallel_objects(chunks, -\u001b[0;36m1, storage = storage): sto.result = self.process_chunk(ds, *args, **kwargs) # Now storage will have everything, and will be done via pickling, so # the units will be preserved. (Credit to Nathan for this # idea/implementation.) values = [ [] for i in range(self.num_vals) ] for key in sorted(storage): for i in range(self.num_vals): values[i].append(storage[key][i]) # These will be YTArrays values = [self.data_source.ds.arr(values[i]) for i in range(self.num_vals)] values = self.reduce_intermediate(values) return values def process_chunk(self, data, *args, **kwargs): raise NotImplementedError def reduce_intermediate(self, values): raise NotImplementedError File: ~/yt/yt/yt/data_objects/derived_quantities.py Type: RegisteredDerivedQuantity Subclasses: WeightedAverageQuantity, TotalQuantity, CenterOfMass, BulkVelocity, WeightedVariance, AngularMomentumVector, Extrema, SampleAtMaxFieldValues, SpinParameter  The key thing I want to highlight here is that this is rather simple in concept; the chunks are iterated over in parallel (via the parallel_objects routine, which parcels them out to different processors), processed, and then the reduction happens through reduce_intermediate.\nThere are a few things to note here \u0026ndash; this is actually units-aware, which means that even if you\u0026rsquo;ve got (for some reason) cm for a quantity on one processor and km on another, it will correctly convert them. The other is that the set up is such that only the process_chunk and reduce_intermediate operations need to be implemented, along with setting some properties.\nBut, we\u0026rsquo;re getting a bit far away from the topic at hand, which is why how chunking is set up can cause some issues with exposing data to dask. And so I want to return to the notion of the \u0026quot;io\u0026quot; chunking and how this works for differently indexed datasets.\nFine- and Coarse-grained Indexing What yt does during the selection of data is key to how it thinks about the processings of that data. The way that data can be provided to yt takes several forms:\n Regularly structured grids and grid based data, where there may be overlapping regions (typically with one \u0026ldquo;authoritative source of truth\u0026rdquo; as in adaptive mesh refinement) Irregular grids, where the distance between points may vary along each spatial axis Unstructured mesh, where the data arrives in hexahedra, tetrahedra, etc, and there is typically a well-defined form for evaluating field values internal to each polyhedra Discrete, or particle-based datasets, where each point is sampled at some location that we don\u0026rsquo;t know a priori \u0026ndash; for instance, N-body simulations Octree or block-structured data, which can in some cases be thought of as a special-case of grid based data but that follows a more regular form  Several of these have a common trait that comes in quite handy for yt \u0026ndash; namely, that the index of the data occupies considerably less memory than the data itself.\nGrid Indexing For instance, when dealing with a grid of data, typically that grid can be defined by a set of properties such as:\n \u0026ldquo;origin\u0026rdquo; corner of the grid (\u0026ldquo;left edge\u0026rdquo;) \u0026ldquo;terminal\u0026rdquo; corner of the grid (\u0026ldquo;right edge\u0026rdquo;) dimensions along each axis if irregular, the cell-spacing along each axis  There are of course a handful of other attributes that might be useful (and which we can sometimes deduce) but these are the basics. If we imagine that each of these requires 64-bits per axis per value, a standard (regular) grid requires 576 bits, or 72 bytes. If we were storing the actual value locations, each would require 3 64-bit numbers \u0026ndash; which means that as soon as we were storing 3 of them, we would\n(Of course, one probably doesn\u0026rsquo;t need to store dimensions as 64 bits, and there are also probably some other ways to reduce the info necessary, but as straw-person arguments go, this isn\u0026rsquo;t so bad.)\nWhat we can get to with this is that for grid and other regular datasets, it\u0026rsquo;s reasonably cheap to index the data. So when we create a data object, for instance:\nsp = ds.sphere(\u0026quot;center\u0026quot;, (100.0, \u0026quot;kpc\u0026quot;))  yt can determine without touching the disk how many grid cells intersect it, and thus it can pre-allocate arrays of the correct size and fill them in progressively, in whatever fashion it deems best for IO purposes.\nThis isn\u0026rsquo;t without cost \u0026ndash; computing the intersections can be quite costly, and so we do some things to cache those. (The cost/benefit of caching often bites us when we are dealing with large unigrid datasets, though.) This was all designed to prevent having to call a big np.concatenate at some point in the operation when chunking based on \u0026quot;all\u0026quot;, but it\u0026rsquo;s not always obvious to me that the balance was correctly struck here.\nWhen an object is created, no selection is conducted until a field is requested. At some point in the call stack once a field is asked for, the function index._identify_base_chunk is called. This is where things are different for particles, but we\u0026rsquo;ll get to that later.\nParticle Indexing When dealing with particles, our indexing requirements are very different. Here, the cost of storing the index values is very high \u0026ndash; but, we also don\u0026rsquo;t want to have to perform too much IO. So we\u0026rsquo;re stuck minimizing how much IO we do, while also minimizing the amount of information we store in-memory once we \u0026ldquo;index\u0026rdquo; a dataset.\nIn yt-4.0, we accomplish this through the use of bitmap indices, which I described a little bit in the first post. The basic idea of this is that each \u0026ldquo;file\u0026rdquo; (which can be subsets of a single file, and is better thought of as an IO block of some type) is assigned some unique ID. All the files are iterated over and for each discrete point included in that file, an index into a space-filling curve is generated. We use a resonably coarse space filling curve for the first iteration \u0026ndash; say, a level 2 curve \u0026ndash; and that allows ambiguities. This is essentially a binning operation.\n(Incidentally, we often use Morton Z-Ordering because it\u0026rsquo;s just easier to explain. We might get better compression if we used Hilbert since consecutive values may be more likely to be identical.)\nAt the end of the first iteration, we have a key-value store of bitarrays, where the key is the file ID and the value is a set of 1\u0026rsquo;s and 0\u0026rsquo;s, where a 1 indicates that a particle is found in a given region identified by the space-filling curve index corresponding with that 1\u0026rsquo;s index in the array. So, for instance, if we had a level 3 index, we would have a set of bitarrays that looked like:\n001 000 101 010 011 011 ...  So, if we read from left-to-right, the first file has particles that live in (zero-indexed) indices 2, 6 and 8. The second file has particles in indices 1, 4, 5, 7 and 8.\nIf we know that our selector only intersects areas touched by index 2, then we only have to read from the first file.\nThis would work great if we had particles that were distributed pretty homogeneously on large scales, but in many cases, we don\u0026rsquo;t. Sometimes when particles are written to disk they are sorted on some high-order index and then written out in that order. What yt does is perform a secondary, as-needed indexing based on where there are \u0026ldquo;collisions\u0026rdquo; \u0026ndash; i.e., ambiguities. A set of logical operations is performed across all the bitarrays to identify where multiple files overlap; following this, a second round of indexing is conducted at a much higher spatial order.\nIn doing this, we are able to pinpoint with reasonably high precision the file or files that need to be read to get data from a given selector, and minimize very precisely the amount of over-reading that is done.\nUnfortunately, this doesn\u0026rsquo;t give us the ability to explicitly allocate arrays of the correct size. (And, the memory overhead of regaining that ability would be quite high.) But as we saw above, yt doesn\u0026rsquo;t want to do big concatenation operations! So it does the thing I really wish it didn\u0026rsquo;t, which is \u0026hellip; it reads all the position data in IO chunks, figures out how big it is (which only requires a running tally, not a set of allocated arrays), then allocates and fills that single big array.\nThis isn\u0026rsquo;t really that efficient, and it arises from the case where the indexing is comparatively cheap.\nBut all of this arises out of the design decision that we need to optimize for the case that we want a single big array, rather than a bunch of small arrays \u0026ndash; i.e., for the case of:\nds.r[:][\u0026quot;density\u0026quot;].max()  as opposed to\nds.r[:].max(\u0026quot;density\u0026quot;)  \u0026hellip;didn\u0026rsquo;t you say you\u0026rsquo;d be talking about Dask? Well, this is where dask comes in! And, it\u0026rsquo;s also why interfacing to dask is a bit tricky \u0026ndash; because we do a lot of work ahead of time before allocating any arrays, and then we get rid of the information generated during that work.\nIn an ideal world, what we would be able to do is to export a data object (such as a sphere or cylinder or rectangular prism) and a field-type (so we knew if it was a vector, or particles, or nodal/zonal data) as a dask array. For instance, if instead of returning an array (specifically, a YTArray or unyt_array) when we accessed sp[\u0026quot;density\u0026quot;], it returned a DaskArray, we would open up a number of new and interesting techniques.\nBut to do that, we need to be able to know in advance the chunk sizes, and more to the point, we need to be able to specify a function that returns each chunk uniquely.\nNext Entry: Iterables and IO Turns out, I thought I\u0026rsquo;d be done with this entry a lot sooner than I was!\nIn the next blog post, which hopefully will take less than the 8 days this one did, I\u0026rsquo;ll talk about why this is (currently) hard, how to fix that, and what we\u0026rsquo;re doing to fix it.\n","date":1560189573,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560189890,"objectID":"3be15376e926cc94641a4a597673f5ed","permalink":"https://matthewturk.github.io/post/refactoring-yt-frontends-part2/","publishdate":"2019-06-10T12:59:33-05:00","relpermalink":"/post/refactoring-yt-frontends-part2/","section":"post","summary":"SIDE NOTE: I intended for this blog post to be a bit shorter than it turned out, and for it to cover some things it \u0026hellip; didn\u0026rsquo;t! So it looks like there\u0026rsquo;ll be a part three in the series.\nOperations on Data Objects In my previous post, I walked through a few aspects of how the chunking system in yt works, mostly focusing on the \u0026quot;io\u0026quot; style of chunking, where the order in which data arrives is not important.","tags":[],"title":"Refactoring yt Frontends - Part 2","type":"post"},{"authors":[],"categories":null,"content":"","date":1559684783,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560367010,"objectID":"211427e115ea8ae0e1122db98bd34924","permalink":"https://matthewturk.github.io/talk/2019-06-04-odia-yt/","publishdate":"2019-06-04T14:46:23-07:00","relpermalink":"/talk/2019-06-04-odia-yt/","section":"talk","summary":"What have your experiences been with digital infrastructure development?","tags":[],"title":"yt: Hard Questions","type":"talk"},{"authors":[],"categories":[],"content":" In the still-in-development version of yt (4.0), the way that particles are handled has been redesigned from the ground up.\nThe current version of yt (3.x) utilizes an octree-based approach for meshing the particles, although not for indexing them \u0026ndash; which presents some problems when doing subsets of particles, as well as when doing visualizations that rely on an implicit meshing. The main result is that, in general, particle visualizations in yt 3.x aren\u0026rsquo;t that great, and are underresolved.\nIn yt 4.0, the particle system has been reimplemented to use EWAH bitmap indices (for more info, see Daniel Lemire\u0026rsquo;s EWAHBoolArray repository) to track which \u0026ldquo;regions\u0026rdquo; of files correspond to particular spatial regions, as designated by indices in a space-filling curve. Things are now orders of magnitude faster to load, to subset, and to visualize \u0026ndash; and the memory overhead is so much lower!\nThis work was led by Nathan Goldbaum and Meagan Lang, with crucial contributions from the rest of the yt community, including feedback and bugfixes from Bili Dong and Cameron Hummels.\nRecently, I\u0026rsquo;ve been exploring using a different array backend in yt, right now focusing on dask. While yt does lots of MPI-parallel operations, much of what we do with these has to be hand-programmed \u0026ndash; so when you implement a new DerivedQuantity (i.e., stuff like calling min on a data object) you have to jump through a few hoops related to intermediate values and the like. Plus, dask seems to be everywhere, and so if we exported to dask arrays or somehow interoperated better with it, we\u0026rsquo;d be able to interoperate with lots of the rest of the ecosystem more easily.\nUnfortunately, there\u0026rsquo;s a bit of an impedance mismatch which \u0026hellip; has made this more difficult than I\u0026rsquo;d like.\nReading Data Before getting too much further, though, I\u0026rsquo;m going to go through a bit about how yt thinks about \u0026ldquo;chunking\u0026rdquo; data.\nThe fundamental thing that yt does is index data. (Well, that, and take a while to compile all the Cython code.) Processing of the data is all layered on top of that \u0026ndash; including some pretty cool semantics-of-data and units, visualization, etc. The main thing is that if you do a subset, it knows where to go to grab that subset of data, and if you want to do something that touches everything, it\u0026rsquo;ll do its best to reduce the number of times data is loaded off disk in service of that.\nWe do this with a \u0026ldquo;chunking\u0026rdquo; system, which is implemented differently if your data is discrete (i.e., particles), mesh-based, and so on.\nSo to show what the problem is, I\u0026rsquo;m going to load up a dataset from the FIRE project.\nimport yt ds = yt.load(\u0026quot;data/FIRE_M12i_ref11/snapshot_600.hdf5\u0026quot;)  yt : [INFO ] 2019-06-02 16:02:22,303 Calculating time from 1.000e+00 to be 4.355e+17 seconds yt : [INFO ] 2019-06-02 16:02:22,304 Assuming length units are in kpc/h (comoving) yt : [INFO ] 2019-06-02 16:02:22,337 Parameters: current_time = 4.3545571088051386e+17 s yt : [INFO ] 2019-06-02 16:02:22,338 Parameters: domain_dimensions = [1 1 1] yt : [INFO ] 2019-06-02 16:02:22,339 Parameters: domain_left_edge = [0. 0. 0.] yt : [INFO ] 2019-06-02 16:02:22,341 Parameters: domain_right_edge = [60000. 60000. 60000.] yt : [INFO ] 2019-06-02 16:02:22,342 Parameters: cosmological_simulation = 1 yt : [INFO ] 2019-06-02 16:02:22,343 Parameters: current_redshift = 0.0 yt : [INFO ] 2019-06-02 16:02:22,344 Parameters: omega_lambda = 0.728 yt : [INFO ] 2019-06-02 16:02:22,344 Parameters: omega_matter = 0.272 yt : [INFO ] 2019-06-02 16:02:22,345 Parameters: omega_radiation = 0.0 yt : [INFO ] 2019-06-02 16:02:22,347 Parameters: hubble_constant = 0.702  At this point yt has done a tiny little bit of reading of the data \u0026ndash; just enough to figure out some of the metadata. It hasn\u0026rsquo;t indexed anything yet or read any of the actual data fields off of disk.\nNow let\u0026rsquo;s make a plot of the gas density, integrated over the z axis of the simulation. Keep in mind that in doing this, it will have to read all the gas particles and smooth them onto a buffer. The first time this gets run, an index is generated and then stored to disk. More on that in a moment.\nI\u0026rsquo;m going to use ds.r[:] here for \u0026ldquo;dataset region, but the whole thing\u0026rdquo; and then I call integrate on it and specify the field to integrate. Then, I plot it.\np=ds.r[:].integrate(\u0026quot;density\u0026quot;, axis=\u0026quot;z\u0026quot;).plot((\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;))  yt : [INFO ] 2019-06-02 16:02:22,484 Allocating for 4.787e+06 particles Loading particle index: 100%|██████████| 10/10 [00:00\u0026lt;00:00, 817.25it/s] yt : [INFO ] 2019-06-02 16:02:23,623 xlim = 0.000000 60000.000000 yt : [INFO ] 2019-06-02 16:02:23,623 ylim = 0.000000 60000.000000 yt : [INFO ] 2019-06-02 16:02:23,633 Making a fixed resolution buffer of (('gas', 'density')) 800 by 800  (All that empty space is because there are only gas particles in the middle of the dataset!)\nThe first time any data needs to be read from a particle dataset, yt will construct an in-memory index of the data on disk; by default, it will store this in a sidecar file, so the next time that the dataset is read it does not need to be generated again.\nThe way the bitmap indices work is really fun, but that deserves its own blog post. It suffices to say that the indexing helps to figure out both which files to read, and which subsets of those files to read, since we don\u0026rsquo;t assume that the particles are sorted in any way. (Mostly because each code tends to sort the particles in its own way!)\nNow, for projecting over the whole domain, it\u0026rsquo;s not that big a deal to read everything, since we have to anyway, but if we did a subset it could dramatically reduce the IO necessary, and it also keeps much less data resident in memory than the old implementation.\nContinuing on, let\u0026rsquo;s say that we now want to center at a different location. We\u0026rsquo;d figure out the most dense point, and then set our center.\nc = ds.r[:].argmax((\u0026quot;gas\u0026quot;, \u0026quot;density\u0026quot;))  (One thing this next set of code highlights is that, in general, how we handle centers in yt is a bit clumsy at times. Writing this blog post led me to filing an issue which may or may not get any traction or support.)\np.set_origin(\u0026quot;center-window\u0026quot;) p.set_center((c[0], c[1])) p.zoom(25) p.set_zlim((\u0026quot;gas\u0026quot;,\u0026quot;density\u0026quot;), 1e-6, 1e-3)  yt : [INFO ] 2019-06-02 16:02:25,607 xlim = -713.911179 59286.088821 yt : [INFO ] 2019-06-02 16:02:25,611 ylim = 1049.283652 61049.283652 yt : [INFO ] 2019-06-02 16:02:25,619 Making a fixed resolution buffer of (('gas', 'density')) 800 by 800  So, we can visualize now, and it\u0026rsquo;s faster than it was before, and we also get much better results. Great. So why am I belaboring this point?\nIt\u0026rsquo;s because in the background, yt is queryin a data object to see which items to read off disk, then it is reading those items off disk. In this particular instance, it is doing what we call \u0026ldquo;io\u0026rdquo; chunking \u0026ndash; this means to use whatever type of hinting is best to get the most efficient ordering it knows how. Among other things, yt will try to minimize the number of times it opens a file, it seeks in a file, and it tries to keep the memory allocation count as low as possible.\n(I\u0026rsquo;ll write more on this last point later \u0026ndash; much of what yt does to index in yt-3.x and yt-4.0 is designed to keep the number of allocated arrays in the IO routines as low as possible, and to avoid any expensive concatenation or subselection operations. It turns out, this is \u0026hellip; not as big a deal as thought when this was made a design principle. And in general, it leads to a lot more floating point operations than we would like, and sometimes more stuff in memory, too.)\nAnd, so, uh, \u0026ldquo;chunking\u0026rdquo; is\u0026hellip;? We can figure out how yt chunks this data by, well, asking it to do it manually! Every data object presents a chunks interface which is a generator that modifies its internal state and then yields itself. For instance:\ndd = ds.all_data() for chunk in dd.chunks([], \u0026quot;io\u0026quot;): print(chunk[\u0026quot;particle_ones\u0026quot;].size)  1048576 885527 753678 524288 317696 262144 262144 262144 262144 208609  I mentioned that this generator yields itself; this is true. But the internal state is modified to store where we are in the iteration, along with things like the parameters for derived fields and the like. The source for this looks like this:\nfrom yt.data_objects.data_containers import YTSelectionContainer YTSelectionContainer.chunks??  Signature: YTSelectionContainer.chunks(self, fields, chunking_style, **kwargs) Docstring: \u0026lt;no docstring\u0026gt; Source: def chunks(self, fields, chunking_style, **kwargs): # This is an iterator that will yield the necessary chunks. self.get_data() # Ensure we have built ourselves if fields is None: fields = [] # chunk_ind can be supplied in the keyword arguments. If it's a # scalar, that'll be the only chunk that gets returned; if it's a list, # those are the ones that will be. chunk_ind = kwargs.pop(\u0026quot;chunk_ind\u0026quot;, None) if chunk_ind is not None: chunk_ind = ensure_list(chunk_ind) for ci, chunk in enumerate(self.index._chunk(self, chunking_style, **kwargs)): if chunk_ind is not None and ci not in chunk_ind: continue with self._chunked_read(chunk): self.get_data(fields) # NOTE: we yield before releasing the context yield self File: ~/yt/yt/yt/data_objects/data_containers.py Type: function  Note that this relies on the index object providing the _chunk routine, which interprets the type of chunking. Also, _chunked_read is a context manager which looks like this:\nYTSelectionContainer._chunked_read??  Signature: YTSelectionContainer._chunked_read(self, chunk) Docstring: \u0026lt;no docstring\u0026gt; Source: @contextmanager def _chunked_read(self, chunk): # There are several items that need to be swapped out # field_data, size, shape obj_field_data = [] if hasattr(chunk, 'objs'): for obj in chunk.objs: obj_field_data.append(obj.field_data) obj.field_data = YTFieldData() old_field_data, self.field_data = self.field_data, YTFieldData() old_chunk, self._current_chunk = self._current_chunk, chunk old_locked, self._locked = self._locked, False yield self.field_data = old_field_data self._current_chunk = old_chunk self._locked = old_locked if hasattr(chunk, 'objs'): for obj in chunk.objs: obj.field_data = obj_field_data.pop(0) File: ~/yt/yt/yt/data_objects/data_containers.py Type: function  This is a bit clunky, but it stores the old state (because, believe it or not, sometimes we have multiple levels of chunking simultaneously, especially for things like spatial derivatives) and then it makes a fresh state, and then it resets it after the context manager concludes.\nSo the end result here is that we have a mechanism that divides the dataset up into the chunks it needs (YTDataChunk objects), and then iterates over them. What does this look like for our particle dataset? Well, we can find out, evidently, by looking at the _current_chunk attribute on the object yielded by chunks.\nI\u0026rsquo;ve changed what we print out here just a little bit, because I want to keep the output a bit more human readable, but this is what it looks like:\ndd = ds.all_data() for chunk in dd.chunks([], \u0026quot;io\u0026quot;): print(\u0026quot;\\nExamining chunk...\u0026quot;) for obj in chunk._current_chunk.objs: print(\u0026quot; Examining obj...\u0026quot;,) for data_file in obj.data_files: print(\u0026quot; {}: {}-{}\u0026quot;.format(data_file.filename, data_file.start, data_file.end))  Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 0-262144 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 262144-524288 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 524288-786432 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 786432-1048576 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1048576-1310720 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1310720-1572864 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1572864-1835008 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 1835008-2097152 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 2097152-2359296 Examining chunk... Examining obj... /home/matthewturk/data/FIRE_M12i_ref11/snapshot_600.hdf5: 2359296-2567905  A few notes here. Each chunk is able to have multiple \u0026ldquo;objects\u0026rdquo; associated with it (which in grid frontends usually means multiple grid objects) but here, we have only one entry in the obj list associated with each. Each obj then only has one item in data_files, which is not really a data file, but instead a subset of a data file specified by its start and end indices.\nIf you\u0026rsquo;re thinking this is a bit clumsy, I would agree with you.\nDask Stuff The issue that I wrote about at the start of this blog post shows up when we start looking at how these chunks are generated. In principle, this does not map that badly to how dask expect chunks to be emitted.\n(At this point I need to admit that while I\u0026rsquo;ve worked with dask, it\u0026rsquo;s entirely possible that I am going to misrepresent its capabilities. Any errors are my own, and if I find out I am mistaken about any of this, I will happily update this blog post!)\nIt\u0026rsquo;s possible to create a dask array through the dask.array.Array constructor; this is described in the array design docs. Since yt uses unyt for attaching units we will need to do some additional work, but let\u0026rsquo;s imagine that we are simply happy dealing with unit-less (and, I suppose, unyt-less) arrays for now.\nTo generate these arrays most efficiently, we need to be able to specify their size, how to obtain them, and maybe a couple other things. But for our purposes, those are the two most important things.\nUnfortunately, as you might be able to tell, this is not information that is super easily exposed without iterating over the dataset. Sure, if we iterated and read everything, of course we can show the appropriate info. And, I posted a little bit about how one might do this on issue 1891, but there\u0026rsquo;s a key thing going on in that code \u0026ndash; yt has already read all the data from disk.\nSo, this isn\u0026rsquo;t ideal.\nChunks are not persistent This all comes about because chunks are not persistent, and more specifically, chunks are always create on-demand. Each different data object will have its own set of chunks, and these will map differently. So, for instance, we might end up selecting all the same sets of objects, but they will have different sizes (and even each different field might be a different size).\nsp1 = ds.sphere(c, (1, \u0026quot;Mpc\u0026quot;)) sp2 = ds.r[ (20.0, \u0026quot;Mpc\u0026quot;) : (40.0, \u0026quot;Mpc\u0026quot;), (25.0, \u0026quot;Mpc\u0026quot;) : (45.0, \u0026quot;Mpc\u0026quot;), (55.0, \u0026quot;Mpc\u0026quot;) : (65.0, \u0026quot;Mpc\u0026quot;) ] print(\u0026quot;sp1 len == {}\\nsp2 len == {}\u0026quot;.format( len(list(sp1.chunks([], \u0026quot;io\u0026quot;))), len(list(sp2.chunks([], \u0026quot;io\u0026quot;))) )) print(\u0026quot;sp1 =\u0026gt; \u0026quot;, \u0026quot; \u0026quot;.join(str(chunk[\u0026quot;particle_ones\u0026quot;].size) for chunk in sp1.chunks([], \u0026quot;io\u0026quot;))) print(\u0026quot;sp2 =\u0026gt; \u0026quot;, \u0026quot; \u0026quot;.join(str(chunk[\u0026quot;particle_ones\u0026quot;].size) for chunk in sp2.chunks([], \u0026quot;io\u0026quot;)))  sp1 len == 10 sp2 len == 10 sp1 =\u0026gt; 388571 306586 341808 205880 50260 2 1 2 3 0 sp2 =\u0026gt; 12 3673 480 29 146 200 77 419 3697 400  The trickiest part of this is that in these cases, we don\u0026rsquo;t know how big each one is going to be! For other types of indexing, it\u0026rsquo;s slightly different \u0026ndash; the indexing system for grids and octrees and meshes can figure out in advance (without reading data from disk) the precise number of values that will be read. But for particles we don\u0026rsquo;t necessarily know.\nUnfortunately, even if we did, the way that the YTDataChunk objects are the result of creating, then yield-ing, rather than returning a list of objects with known sizes makes it harder to expose this to dask. In particular, because we can\u0026rsquo;t (inexpensively) fast-forward the generator or rewind it or even access it elementwise makes it tricky to interface. One can expose unknown chunk sizes to dask, but it seems like we could do better.\nSo what can be done? Well, let me first note that a lot of this is a result of trying to be clever! Back when the chunking system was being implemented, it seemed like simple generator expressions were the right way to do it. And, a bunch of layers have been added on top of those generator expressions that make it harder to simply strip that component out.\nBut recently, Britton Smith and I have been digging into some of the particle frontends, and we think we might have a solution that would both simplify a lot of this logic and make it a lot easier to expose the arrays to different array backends \u0026ndash; specifically dask.\nFor more on that, wait for part two!\n","date":1559340372,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559509871,"objectID":"e2409a44c1f2fb08fb0b73f5ab5788e4","permalink":"https://matthewturk.github.io/post/refactoring-yt-frontends-part1/","publishdate":"2019-05-31T17:06:12-05:00","relpermalink":"/post/refactoring-yt-frontends-part1/","section":"post","summary":"The first post in a deep dive into yt frontends, chunking, and why and how they might be refactored.","tags":[],"title":"Refactoring yt Frontends - Part 1","type":"post"},{"authors":null,"categories":null,"content":"","date":1557177039,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"2fd5b0f658881a3f851b03da682bc48d","permalink":"https://matthewturk.github.io/project/yt/","publishdate":"2019-05-06T16:10:39-05:00","relpermalink":"/project/yt/","section":"project","summary":"yt is an open-source python package for analyzing and visualizing volumetric data.","tags":[],"title":"yt","type":"project"},{"authors":[],"categories":null,"content":"","date":1556736856,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"5f15fff70af789e7da9d4a78640dfa59","permalink":"https://matthewturk.github.io/talk/2019-05-01-crops-dependencies/","publishdate":"2019-05-01T13:54:16-05:00","relpermalink":"/talk/2019-05-01-crops-dependencies/","section":"talk","summary":"This is a brief overview of how we can think about dependencies in the Crops in Silico project, and how we can use that to organize our work and collaboration.","tags":[],"title":"Crops-in-Silico Collaboration and Dependencies","type":"talk"},{"authors":["Adam Brinckman","Kyle Chard","Niall Gaffney","Mihael Hategan","Matthew B Jones","Kacper Kowalik","Sivakumar Kulasekaran","Bertram Ludäscher","Bryce D Mecum","Jarek Nabrzyski","Victoria Stodden","Ian J Taylor","Matthew J Turk","Kandace Turner"],"categories":null,"content":"","date":1556668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"4d610cc35353e551e4dd5786294bba6d","permalink":"https://matthewturk.github.io/publication/brinckman-2019-oa/","publishdate":"2019-05-30T20:07:09.390766Z","relpermalink":"/publication/brinckman-2019-oa/","section":"publication","summary":"The act of sharing scientific knowledge is rapidly evolving away from traditional articles and presentations to the delivery of executable objects that integrate the data and computational details (e.g., scripts and workflows) upon which the findings rely. This envisioned coupling of data and process is essential to advancing science but faces technical and institutional barriers. The Whole Tale project aims to address these barriers by connecting computational, data-intensive research efforts with the larger research process---transforming the knowledge discovery and dissemination process into one where data products are united with research articles to create ``living publications'' or tales. The Whole Tale focuses on the full spectrum of science, empowering users in the long tail of science, and power users with demands for access to big data and compute resources. We report here on the design, architecture, and implementation of the Whole Tale environment.","tags":["Living publications; Reproducibility; Provenance; Data sharing; Code sharing;Authorship"],"title":"Computing environments for reproducibility: Capturing the ``Whole Tale''","type":"publication"},{"authors":[],"categories":null,"content":"","date":1556082000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"8e090a510726760f69cb7803cc7fcc0c","permalink":"https://matthewturk.github.io/talk/2019-04-24-ddd-update/","publishdate":"2019-05-01T13:52:48-05:00","relpermalink":"/talk/2019-04-24-ddd-update/","section":"talk","summary":"My 'update' talk at the Moore Data Driven Discovery Investigator Symposium in April, 2019.","tags":[],"title":"DDD Update","type":"talk"},{"authors":[],"categories":null,"content":"","date":1554058436,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"3e44c17136c4c894688a3bcd35473a56","permalink":"https://matthewturk.github.io/talk/2019-03-31-troubleshooting-data-storytelling/","publishdate":"2019-05-01T13:53:56-05:00","relpermalink":"/talk/2019-03-31-troubleshooting-data-storytelling/","section":"talk","summary":"","tags":[],"title":"Troubleshooting Data Storytelling","type":"talk"},{"authors":[],"categories":null,"content":"","date":1553893200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557177314,"objectID":"b4dbdf946084cccc2be51a72048b1513","permalink":"https://matthewturk.github.io/talk/2019-03-29-cirss-open-source-teen-years/","publishdate":"2019-04-30T17:34:14-05:00","relpermalink":"/talk/2019-03-29-cirss-open-source-teen-years/","section":"talk","summary":"In this talk, I will reflect on experiences I have had navigating the landscape of open source scholarly software as projects age, and the way that shapes interaction in an ecosystem.  I will also report some recent developments with the open source project yt, and how they both interact with the shifting needs and desires of community members and how they address the needs and desires of potential future community members.  Many exciting buzzwords such as 'Rust' and 'WebAssembly' will be used.","tags":[],"title":"CIRSS Seminar: Open Source in the Teen Years","type":"talk"},{"authors":null,"categories":null,"content":"","date":1549052179,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"4600419641aba970b03879da5587bed8","permalink":"https://matthewturk.github.io/project/crops-in-silico/","publishdate":"2019-02-01T15:16:19-05:00","relpermalink":"/project/crops-in-silico/","section":"project","summary":"Crops in Silico is an integrative and multi-scale modeling platform to combine modeling efforts toward the generation of virtual crops, open and accessible to the global community.","tags":[],"title":"Crops in Silico","type":"project"},{"authors":null,"categories":null,"content":"","date":1548879389,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559249134,"objectID":"670e9b7050687e20c063667daf003870","permalink":"https://matthewturk.github.io/project/whole-tale/","publishdate":"2019-01-30T15:16:29-05:00","relpermalink":"/project/whole-tale/","section":"project","summary":"Whole Tale is an initiative to build a scalable, open source, web-based, multi-user platform for reproducible research.","tags":[],"title":"Whole Tale","type":"project"},{"authors":null,"categories":null,"content":"This course covered topics that could broadly be described as \u0026ldquo;advanced,\u0026rdquo; including new platforms and tools for visualizing data, and how to present data in different, more thoughtful ways. It was structured differently than the other data viz courses, and designed for more interaction with a smaller group of students.\n","date":1546383114,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"81e4db4ee0b05df2af767d7e3ba9a362","permalink":"https://matthewturk.github.io/courses/is590adv-spr2019/","publishdate":"2019-01-01T17:51:54-05:00","relpermalink":"/courses/is590adv-spr2019/","section":"courses","summary":"Seminar on advanced or in-depth topics in data visualization","tags":[],"title":"IS590ADV - Spring 2019","type":"courses"},{"authors":null,"categories":null,"content":"This course, offered in Fall of 2018, included more javascript than previous iterations and also utilized bqplot to a greater extent.\n","date":1533163907,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"edece407444789734693d605192694b5","permalink":"https://matthewturk.github.io/courses/is590dv-fall2018/","publishdate":"2018-08-01T17:51:47-05:00","relpermalink":"/courses/is590dv-fall2018/","section":"courses","summary":"Data Viz from Fall 2018","tags":[],"title":"IS590DV - Fall 2018","type":"courses"},{"authors":["Nathan J Goldbaum","John A ZuHone","Matthew J Turk","Kacper Kowalik","Anna L Rosen"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"abc200f366401cf32068cf880c72c3bb","permalink":"https://matthewturk.github.io/publication/goldbaum-2018-ws/","publishdate":"2019-05-30T20:07:09.372097Z","relpermalink":"/publication/goldbaum-2018-ws/","section":"publication","summary":"Software that processes real-world data or that models a physical system must have some way of managing units. While simple approaches like the understood convention that all data are in a unit system (such as the MKS SI unit system) do work in practice, they are fraught with possible sources of error both by developers and users of the software. In this paper we present unyt, a Python library based on NumPy and SymPy for handling data that has units. It is designed both to aid quick interactive calculations and to be tightly integrated into a larger Python application or library. We compare unyt with two other Python libraries for handling units, Pint and astropy.units, and find that unyt is faster, has higher test coverage, and has fewer lines of code.","tags":["Authorship"],"title":"unyt: Handle, manipulate, and convert data with units in Python","type":"publication"},{"authors":null,"categories":null,"content":"This was an experimental course designed to convey the basics of \u0026ldquo;conversational computation\u0026rdquo; to astronomy undergraduates.\n","date":1514847094,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"a66798ccee3cefb7f8c172b6f85651f8","permalink":"https://matthewturk.github.io/courses/astr496-spr2018/","publishdate":"2018-01-01T17:51:34-05:00","relpermalink":"/courses/astr496-spr2018/","section":"courses","summary":"Introduction to Computational Astrophysics","tags":[],"title":"ASTR496 - Spring 2018","type":"courses"},{"authors":null,"categories":null,"content":"Starting in Spring 2018, I transitioned courses to using Github Pages and RevealJS. The repository includes built slide decks and rendered Jupyter notebooks.\n","date":1514847088,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"106a69791a1c8e84dba3b3f48bdfd470","permalink":"https://matthewturk.github.io/courses/is590dv-spr2018/","publishdate":"2018-01-01T17:51:28-05:00","relpermalink":"/courses/is590dv-spr2018/","section":"courses","summary":"Data Viz from Spring 2018","tags":[],"title":"IS590DV - Spring 2018","type":"courses"},{"authors":["Hsi-Yu Schive","John A ZuHone","Nathan J Goldbaum","Matthew J Turk","Massimo Gaspari","Chin-Yu Cheng"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"3b5789604fd9a3ad84d80b2d4e03c5fe","permalink":"https://matthewturk.github.io/publication/schive-2017-ep/","publishdate":"2019-05-30T20:07:09.39481Z","relpermalink":"/publication/schive-2017-ep/","section":"publication","summary":"We present GAMER-2, a GPU-accelerated adaptive mesh refinement (AMR) code for astrophysics. It provides a rich set of features, including adaptive time-stepping, several hydrodynamic schemes, magnetohydrodynamics, self-gravity, particles, star formation, chemistry and radiative processes with GRACKLE, data analysis with yt, and memory pool for efficient object allocation. GAMER-2 is fully bitwise reproducible. For the performance optimization, it adopts hybrid OpenMP/MPI/GPU parallelization and utilizes overlapping CPU computation, GPU computation, and CPU-GPU communication. Load balancing is achieved using a Hilbert space-filling curve on a level-by-level basis without the need to duplicate the entire AMR hierarchy on each MPI process. To provide convincing demonstrations of the accuracy and performance of GAMER-2, we directly compare with Enzo on isolated disk galaxy simulations and with FLASH on galaxy cluster merger simulations. We show that the physical results obtained by different codes are in very good agreement, and GAMER-2 outperforms Enzo and FLASH by nearly one and two orders of magnitude, respectively, on the Blue Waters supercomputers using $1-256$ nodes. More importantly, GAMER-2 exhibits similar or even better parallel scalability compared to the other two codes. We also demonstrate good weak and strong scaling using up to 4096 GPUs and 65,536 CPU cores, and achieve a uniform resolution as high as $10,240^3$ cells. Furthermore, GAMER-2 can be adopted as an AMR+GPUs framework and has been extensively used for the wave dark matter ($ψ$DM) simulations. GAMER-2 is open source (available at https://github.com/gamer-project/gamer) and new contributions are welcome.","tags":["Authorship;DXL Member Papers"],"title":"GAMER-2: a GPU-accelerated adaptive mesh refinement code -- accuracy, performance, and scalability","type":"publication"},{"authors":null,"categories":null,"content":"This was the second semester that I taught Data Viz, and I was still largely using Google Slides. The link to the repository includes the lecture PDFs and notebooks used.\n","date":1501627863,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"2882197390cd2fb552718a0c7b83241b","permalink":"https://matthewturk.github.io/courses/lis590dv-fall2017/","publishdate":"2017-08-01T17:51:03-05:00","relpermalink":"/courses/lis590dv-fall2017/","section":"courses","summary":"Data Viz from Fall 2017","tags":[],"title":"LIS590DV - Fall 2017","type":"courses"},{"authors":["Amy Marshall-Colon","Stephen P Long","Douglas K Allen","Gabrielle Allen","Daniel A Beard","Bedrich Benes","Susanne von Caemmerer","A J Christensen","Donna J Cox","John C Hart","Peter M Hirst","Kavya Kannan","Daniel S Katz","Jonathan P Lynch","Andrew J Millar","Balaji Panneerselvam","Nathan D Price","Przemyslaw Prusinkiewicz","David Raila","Rachel G Shekar","Stuti Shrivastava","Diwakar Shukla","Venkatraman Srinivasan","Mark Stitt","Matthew J Turk","Eberhard O Voit","Yu Wang","Xinyou Yin","Xin-Guang Zhu"],"categories":null,"content":"","date":1493596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"bb0989fb716998e413388479a9bd5b8e","permalink":"https://matthewturk.github.io/publication/marshall-colon-2017-fb/","publishdate":"2019-05-30T20:07:09.377714Z","relpermalink":"/publication/marshall-colon-2017-fb/","section":"publication","summary":"Multi-scale models can facilitate whole plant simulations by linking gene networks, protein synthesis, metabolic pathways, physiology, and growth. Whole plant models can be further integrated with ecosystem, weather, and climate models to predict how various interactions respond to environmental perturbations. These models have the potential to fill in missing mechanistic details and generate new hypotheses to prioritize directed engineering efforts. Outcomes will potentially accelerate improvement of crop yield, sustainability, and increase future food security. It is time for a paradigm shift in plant modeling, from largely isolated efforts to a connected community that takes advantage of advances in high performance computing and mechanistic understanding of plant processes. Tools for guiding future crop breeding and engineering, understanding the implications of discoveries at the molecular level for whole plant behavior, and improved prediction of plant and ecosystem responses to the environment are urgently needed. The purpose of this perspective is to introduce Crops in silico (cropsinsilico.org), an integrative and multi-scale modeling platform, as one solution that combines isolated modeling efforts toward the generation of virtual crops, which is open and accessible to the entire plant biology community. The major challenges involved both in the development and deployment of a shared, multi-scale modeling platform, which are summarized in this prospectus, were recently identified during the first Crops in silico Symposium and Workshop.","tags":["computational framework; crop yield; integration; model; multiscale;Authorship"],"title":"Crops In Silico: Generating Virtual Crops Using an Integrative and Multi-scale Modeling Platform","type":"publication"},{"authors":["Britton D Smith","Greg L Bryan","Simon C O Glover","Nathan J Goldbaum","Matthew J Turk","John Regan","John H Wise","Hsi-Yu Schive","Tom Abel","Andrew Emerick","Brian W O'Shea","Peter Anninos","Cameron B Hummels","Sadegh Khochfar"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"01c869b2c02e627571168f1e7ea367d2","permalink":"https://matthewturk.github.io/publication/smith-2017-za/","publishdate":"2019-05-30T20:07:09.393935Z","relpermalink":"/publication/smith-2017-za/","section":"publication","summary":"We present the grackle chemistry and cooling library for astrophysical simulations and models. grackle provides a treatment of non-equilibrium primordial chemistry and cooling for H, D and He species, including H2 formation on dust grains; tabulated primordial and metal cooling; multiple ultraviolet background models; and support for radiation transfer and arbitrary heat sources. The library has an easily implementable interface for simulation codes written in c, c++ and fortran as well as a python interface with added convenience functions for semi-analytical models. As an open-source project, grackle provides a community resource for accessing and disseminating astrochemical data and numerical methods. We present the full details of the core functionality, the simulation and python interfaces, testing infrastructure, performance and range of applicability. grackle is a fully open-source project and new contributions are welcome.","tags":["Authorship;DXL Member Papers"],"title":"grackle: a chemistry and cooling library for astrophysics","type":"publication"},{"authors":null,"categories":null,"content":"This was the first time I taught Data Viz, and the course repository includes the original Google Slides PDFs, links to the presentations, and the notebooks used in the course.\n","date":1483246801,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559268143,"objectID":"a8ae1d547cdc224350917d8d2bd536a0","permalink":"https://matthewturk.github.io/courses/lis590dv-spr2017/","publishdate":"2017-01-01T00:00:01-05:00","relpermalink":"/courses/lis590dv-spr2017/","section":"courses","summary":"Data Viz from Spring 2017","tags":[],"title":"LIS590DV - Spring 2017","type":"courses"},{"authors":["Harshil M Kamdar","Matthew J Turk","Robert J Brunner"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d2de67e520898676ea8c2ae39f8eaf22","permalink":"https://matthewturk.github.io/publication/kamdar-2016-ae/","publishdate":"2019-05-30T20:07:09.37875Z","relpermalink":"/publication/kamdar-2016-ae/","section":"publication","summary":"We present a new exploratory framework to model galaxy formation and evolution in a hierarchical Universe by using machine learning (ML). Our motivations are two-fold: (1) presenting a new, promising technique to study galaxy formation, and (2) quantitatively analysing the extent of the influence of dark matter halo properties on galaxies in the backdrop of semi-analytical models (SAMs). We use the influential Millennium Simulation and the corresponding Munich SAM to train and test various sophisticated ML algorithms (k-Nearest Neighbors, decision trees, random forests, and extremely randomized trees). By using only essential dark matter halo physical properties for haloes of M  1012 M⊙ and a partial merger tree, our model predicts the hot gas mass, cold gas mass, bulge mass, total stellar mass, black hole mass and cooling radius at z = 0 for each central galaxy in a dark matter halo for the Millennium run. Our results provide a unique and powerful phenomenological framework to explore the galaxy--halo connection that is built upon SAMs and demonstrably place ML as a promising and a computationally efficient tool to study small-scale structure formation.","tags":["Authorship"],"title":"Machine learning and cosmological simulations -- I. Semi-analytical models","type":"publication"},{"authors":["Harshil M Kamdar","Matthew J Turk","Robert J Brunner"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"0e563eede49bd354c26f1fd53f232568","permalink":"https://matthewturk.github.io/publication/kamdar-2016-mu/","publishdate":"2019-05-30T20:07:09.376485Z","relpermalink":"/publication/kamdar-2016-mu/","section":"publication","summary":"We extend a machine learning (ML) framework presented previously to model galaxy formation and evolution in a hierarchical universe using N-body+ hydrodynamical simulations. In this work, we show that ML is a promising technique to study galaxy formation …","tags":["Authorship"],"title":"Machine learning and cosmological simulations--II. Hydrodynamical simulations","type":"publication"},{"authors":["Desika Narayanan","Matthew Turk","Robert Feldmann","Thomas Robitaille","Philip Hopkins","Robert Thompson","Christopher Hayward","David Ball","Claude-André Faucher-Giguère","Dušan Kereš"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"f598bdf0862f455746a898a59f677c1a","permalink":"https://matthewturk.github.io/publication/narayanan-2015-gq/","publishdate":"2019-05-30T20:07:09.392775Z","relpermalink":"/publication/narayanan-2015-gq/","section":"publication","summary":"Submillimetre-bright galaxies at high redshift are the most luminous, heavily star-forming galaxies in the Universe and are characterized by prodigious emission in the far-infrared, with a flux of at least five millijanskys at a wavelength of 850 micrometres. They reside in haloes with masses about 10(13) times that of the Sun, have low gas fractions compared to main-sequence disks at a comparable redshift, trace complex environments and are not easily observable at optical wavelengths. Their physical origin remains unclear. Simulations have been able to form galaxies with the requisite luminosities, but have otherwise been unable to simultaneously match the stellar masses, star formation rates, gas fractions and environments. Here we report a cosmological hydrodynamic galaxy formation simulation that is able to form a submillimetre galaxy that simultaneously satisfies the broad range of observed physical constraints. We find that groups of galaxies residing in massive dark matter haloes have increasing rates of star formation that peak at collective rates of about 500-1,000 solar masses per year at redshifts of two to three, by which time the interstellar medium is sufficiently enriched with metals that the region may be observed as a submillimetre-selected system. The intense star formation rates are fuelled in part by the infall of a reservoir gas supply enabled by stellar feedback at earlier times, not through major mergers. With a lifetime of nearly a billion years, our simulations show that the submillimetre-bright phase of high-redshift galaxies is prolonged and associated with significant mass buildup in early-Universe proto-clusters, and that many submillimetre-bright galaxies are composed of numerous unresolved components (for which there is some observational evidence).","tags":["Authorship;DXL Member Papers"],"title":"The formation of submillimetre-bright galaxies from gas infall over a billion years","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"96c4421b91ab99587be7a001045f86a2","permalink":"https://matthewturk.github.io/publication/turk-2015-yw/","publishdate":"2019-05-30T20:07:09.373042Z","relpermalink":"/publication/turk-2015-yw/","section":"publication","summary":"Gmail. It serves as the center of my digital life. When I receive a new email with a Google Docs document linked in it, there's a preview of that document--- when I send one to someone else, Gmail first checks to see if the person receiving it has permission to view it and lets me know …","tags":["Authorship"],"title":"Vertical Integration","type":"publication"},{"authors":["Samuel W Skillman","Michael S Warren","Matthew J Turk","Risa H Wechsler","Daniel E Holz","P M Sutter"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"c4612415f473e20e2680d86b0e53ec81","permalink":"https://matthewturk.github.io/publication/skillman-2014-vt/","publishdate":"2019-05-30T20:07:09.380327Z","relpermalink":"/publication/skillman-2014-vt/","section":"publication","summary":"The Dark Sky Simulations are an ongoing series of cosmological N-body simulations designed to provide a quantitative and accessible model of the evolution of the large-scale Universe. Such models are essential for many aspects of the study of dark matter and dark energy, since we lack a sufficiently accurate analytic model of non-linear gravitational clustering. In July 2014, we made available to the general community our early data release, consisting of over 55 Terabytes of simulation data products, including our largest simulation to date, which used $1.07 times 10^12~(10240^3)$ particles in a volume $8h^-1mathrmGpc$ across. Our simulations were performed with 2HOT, a purely tree-based adaptive N-body method, running on 200,000 processors of the Titan supercomputer, with data analysis enabled by yt. We provide an overview of the derived halo catalogs, mass function, power spectra and light cone data. We show self-consistency in the mass function and mass power spectrum at the 1% level over a range of more than 1000 in particle mass. We also present a novel method to distribute and access very large datasets, based on an abstraction of the World Wide Web (WWW) as a file system, remote memory-mapped file access semantics, and a space-filling curve index. This method has been implemented for our data release, and provides a means to not only query stored results such as halo catalogs, but also to design and deploy new analysis techniques on large distributed datasets.","tags":["Authorship"],"title":"Dark Sky Simulations: Early Data Release","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"a1f14428b663d7f3b82f91bbfe0a9609","permalink":"https://matthewturk.github.io/publication/turk-2014-ak/","publishdate":"2019-05-30T20:07:09.364588Z","relpermalink":"/publication/turk-2014-ak/","section":"publication","summary":"Designing Software for Collaboration In computational science, collaboration in different scientific communities often takes different forms---examining the results of data generation or acquisition, combining dispa- rate pieces of software in a computational pipeline, and even enhancing …","tags":["Authorship"],"title":"Fostering Collaborative Computational Science","type":"publication"},{"authors":["Benjamin Holtzman","Jason Candler","Matthew Turk","Daniel Peter"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ce1db4fcedcc809c4ff9dcee5ab6f2ff","permalink":"https://matthewturk.github.io/publication/holtzman-2014-wb/","publishdate":"2019-05-30T20:07:09.373678Z","relpermalink":"/publication/holtzman-2014-wb/","section":"publication","summary":"We construct a representation of earthquakes and global seismic waves through sound and animated images. The seismic wave field is the ensemble of elastic waves that propagate through the planet after an earthquake, emanating from the rupture on the fault. The sounds are made by time compression (i.e. speeding up) of seismic data with minimal additional processing. The animated images are renderings of numerical simulations of seismic wave propagation in the globe. Synchronized sounds and images reveal complex patterns and illustrate numerous aspects of the seismic wave field. These movies represent phenomena occurring far from the time and length scales normally accessible to us, creating a profound experience for the observer. The multi-sensory perception of these complex phenomena may also bring new insights to researchers.","tags":["Authorship"],"title":"Seismic Sound Lab: Sights, Sounds and Perception of the Earth as an Acoustic Space","type":"publication"},{"authors":["T Kwasnitschka","K C Yu","M Turk"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"36670fa758f1f2a192e24ec7cefa9258","permalink":"https://matthewturk.github.io/publication/kwasnitschka-2014-ya/","publishdate":"2019-05-30T20:07:09.361875Z","relpermalink":"/publication/kwasnitschka-2014-ya/","section":"publication","summary":"What topics belong inside a planetarium and what is beyond the scope of our mission? Are we limited to astronomy?","tags":["Authorship"],"title":"The Ground beath our Feet: Earth Science in the Planetarium","type":"publication"},{"authors":["Ji-Hoon Kim","Mark R Krumholz","John H Wise","Matthew J Turk","Nathan J Goldbaum","Tom Abel"],"categories":null,"content":"","date":1383264000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"7dc7f1a8d9fa48cd9148edec88aee4a6","permalink":"https://matthewturk.github.io/publication/kim-2013-ac/","publishdate":"2019-05-30T20:07:09.371064Z","relpermalink":"/publication/kim-2013-ac/","section":"publication","summary":"We investigate the spatially resolved star formation relation using a galactic disk formed in a comprehensive high-resolution (3.8 pc) simulation. Our new implementation of stellar feedback includes ionizing radiation as well as supernova explosions, and we handle ionizing radiation by solving the radiative transfer equation rather than by a subgrid model. Photoheating by stellar radiation stabilizes gas against Jeans fragmentation, reducing the star formation rate (SFR). Because we have self-consistently calculated the location of ionized gas, we are able to make simulated, spatially resolved observations of star formation tracers, such as H$α$ emission. We can also observe how stellar feedback manifests itself in the correlation between ionized and molecular gas. Applying our techniques to the disk in a galactic halo of 2.3 $times$ 1011 M ☉, we find that the correlation between SFR density (estimated from mock H$α$ emission) and H2 density shows large scatter, especially at high resolutions of ≲75 pc that are comparable to the size of giant molecular clouds (GMCs). This is because an aperture of GMC size captures only particular stages of GMC evolution and because H$α$ traces hot gas around star-forming regions and is displaced from the H2 peaks themselves. By examining the evolving environment around star clusters, we speculate that the breakdown of the traditional star formation laws of the Kennicutt-Schmidt type at small scales is further aided by a combination of stars drifting from their birthplaces and molecular clouds being dispersed via stellar feedback.","tags":["Authorship"],"title":"DWARF GALAXIES WITH IONIZING RADIATION FEEDBACK. II. SPATIALLY RESOLVED STAR FORMATION RELATION","type":"publication"},{"authors":["Ji-Hoon Kim","Mark R Krumholz","John H Wise","Matthew J Turk","Nathan J Goldbaum","Tom Abel"],"categories":null,"content":"","date":1377993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d89d1130804c0344ac02ec8f41dc7d61","permalink":"https://matthewturk.github.io/publication/kim-2013-jy/","publishdate":"2019-05-30T20:07:09.379476Z","relpermalink":"/publication/kim-2013-jy/","section":"publication","summary":"We describe a new method for simulating ionizing radiation and supernova feedback in the analogs of low-redshift galactic disks. In this method, which we call star-forming molecular cloud (SFMC) particles, we use a ray-tracing technique to solve the radiative transfer equation for ultraviolet photons emitted by thousands of distinct particles on the fly. Joined with high numerical resolution of 3.8 pc, the realistic description of stellar feedback helps to self-regulate star formation. This new feedback scheme also enables us to study the escape of ionizing photons from star-forming clumps and from a galaxy, and to examine the evolving environment of star-forming gas clumps. By simulating a galactic disk in a halo of 2.3 $times$ 1011 M ☉, we find that the average escape fraction from all radiating sources on the spiral arms (excluding the central 2.5 kpc) fluctuates between 0.08% and 5.9% during a ∼20 Myr period with a mean value of 1.1%. The flux of escaped photons from these sources is not strongly beamed, but manifests a large opening angle of more than 60° from the galactic pole. Further, we investigate the escape fraction per SFMC particle, f esc(i), and how it evolves as the particle ages. We discover that the average escape fraction f esc is dominated by a small number of SFMC particles with high f esc(i). On average, the escape fraction from an SFMC particle rises from 0.27% at its birth to 2.1% at the end of a particle lifetime, 6 Myr. This is because SFMC particles drift away from the dense gas clumps in which they were born, and because the gas around the star-forming clumps is dispersed by ionizing radiation and supernova feedback. The framework established in this study brings deeper insight into the physics of photon escape fraction from an individual star-forming clump and from a galactic disk.","tags":["Authorship"],"title":"DWARF GALAXIES WITH IONIZING RADIATION FEEDBACK. I. ESCAPE OF IONIZING PHOTONS","type":"publication"},{"authors":["The Enzo Collaboration","Greg L Bryan","Michael L Norman","Brian W O'Shea","Tom Abel","John H Wise","Matthew J Turk","Daniel R Reynolds","David C Collins","Peng Wang","Samuel W Skillman","Britton Smith","Robert P Harkness","James Bordner","Ji-Hoon Kim","Michael Kuhlen","Hao Xu","Nathan Goldbaum","Cameron Hummels","Alexei G Kritsuk","Elizabeth Tasker","Stephen Skory","Christine M Simpson","Oliver Hahn","Jeffrey S Oishi","Geoffrey C So","Fen Zhao","Renyue Cen","Yuan Li"],"categories":null,"content":"","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"8517285c643a11800825d1dc3fe6aa6b","permalink":"https://matthewturk.github.io/publication/the-enzo-collaboration-2013-es/","publishdate":"2019-05-30T20:07:09.391823Z","relpermalink":"/publication/the-enzo-collaboration-2013-es/","section":"publication","summary":"This paper describes the open-source code Enzo, which uses block-structured adaptive mesh refinement to provide high spatial and temporal resolution for modeling astrophysical fluid flows. The code is Cartesian, can be run in 1, 2, and 3 dimensions, and supports a wide variety of physics including hydrodynamics, ideal and non-ideal magnetohydrodynamics, N-body dynamics (and, more broadly, self-gravity of fluids and particles), primordial gas chemistry, optically-thin radiative cooling of primordial and metal-enriched plasmas (as well as some optically-thick cooling models), radiation transport, cosmological expansion, and models for star formation and feedback in a cosmological context. In addition to explaining the algorithms implemented, we present solutions for a wide range of test problems, demonstrate the code's parallel performance, and discuss the Enzo collaboration's code development methodology.","tags":["Authorship"],"title":"Enzo: An Adaptive Mesh Refinement Code for Astrophysics","type":"publication"},{"authors":["M J Turk"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"c8a33d90212b796c8d03b80111c09aa0","permalink":"https://matthewturk.github.io/publication/turk-2013-ks/","publishdate":"2019-05-30T20:07:09.374781Z","relpermalink":"/publication/turk-2013-ks/","section":"publication","summary":"As scientists' needs for computational techniques and tools grow, they cease to be supportable by software developed in isolation. In many cases, these needs are being met by communities of practice, where software is developed by domain scientists to reach …","tags":["Authorship"],"title":"How to scale a code in the human dimension","type":"publication"},{"authors":["Matthew J Turk"],"categories":null,"content":"","date":1356998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"2e693ceb78643251252efd3fdb648e1b","permalink":"https://matthewturk.github.io/publication/turk-2013-jj/","publishdate":"2019-05-30T20:07:09.375168Z","relpermalink":"/publication/turk-2013-jj/","section":"publication","summary":"As scientists' needs for computational techniques and tools grow, they cease to be supportable by software developed in isolation. In many cases, these needs are being met by communities of practice, where software is developed by domain scientists to reach …","tags":["XSEDE proceedings","community","open source;Authorship"],"title":"Scaling a Code in the Human Dimension","type":"publication"},{"authors":["John H Wise","Tom Abel","Matthew J Turk","Michael L Norman","Britton D Smith"],"categories":null,"content":"","date":1346457600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"52da72f3fa7de9f835f555ba2e7cb3a4","permalink":"https://matthewturk.github.io/publication/wise-2012-kl/","publishdate":"2019-05-30T20:07:09.362758Z","relpermalink":"/publication/wise-2012-kl/","section":"publication","summary":"Here we present three adaptive mesh refinement radiation hydrodynamics simulations that illustrate the impact of momentum transfer from ionising radiation to the absorbing gas on star formation in high-redshift dwarf galaxies. Momentum transfer is calculated by solving the radiative transfer equation with a ray tracing algorithm that is adaptive in spatial and angular coordinates. We find that momentum input partially affects star formation by increasing the turbulent support to a three-dimensional rms velocity equal to the circular velocity of early haloes. Compared to a calculation that neglects radiation pressure, the star formation rate is decreased by a factor of five to 1.8 ? 10?2 M? yr?1 in a dwarf galaxy with a dark matter and stellar mass of 2.0 ? 108 M? and 4.5 ? 105 M?, respectively, when radiation pressure is included. Its mean metallicity of 10?2.1 Z? is consistent with the observed dwarf galaxy luminosity-metallicity relation. In addition to photo-heating in H II regions, radiation pressure further drives dense gas from star forming regions, so supernovae feedback occurs in a warmer and more diffuse medium, launching metal-rich outflows.","tags":["Authorship"],"title":"The imprint of pop III stars on the first galaxies","type":"publication"},{"authors":["Matthew J Turk","Jeffrey S Oishi","Tom Abel","Greg L Bryan"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"3cbbedf0bd49e08ae8ed40e5800104c8","permalink":"https://matthewturk.github.io/publication/turk-2012-ac/","publishdate":"2019-05-30T20:07:09.382091Z","relpermalink":"/publication/turk-2012-ac/","section":"publication","summary":"We study the buildup of magnetic fields during the formation of Population III star-forming regions by conducting cosmological simulations from realistic initial conditions and varying the Jeans resolution. To investigate this in detail, we start simulations from identical initial conditions, mandating 16, 32, and 64 zones per Jeans length, and study the variation in their magnetic field amplification. We find that, while compression results in some amplification, turbulent velocity fluctuations driven by the collapse can further amplify an initially weak seed field via dynamo action, provided there is sufficient numerical resolution to capture vortical motions (we find this requirement to be 64 zones per Jeans length, slightly larger than but consistent with previous work run with more idealized collapse scenarios). We explore saturation of amplification of the magnetic field, which could potentially become dynamically important in subsequent, fully resolved calculations. We have also identified a relatively surprising phenomenon that is purely hydrodynamic: the higher-resolved simulations possess substantially different characteristics, including higher infall velocity, increased temperatures inside 1000 AU, and decreased molecular hydrogen content in the innermost region. Furthermore, we find that disk formation is suppressed in higher-resolution calculations, at least at the times that we can follow the calculation. We discuss the effect this may have on the buildup of disks over the accretion history of the first clump to form as well as the potential for gravitational instabilities to develop and induce fragmentation.","tags":["Authorship"],"title":"MAGNETIC FIELDS IN POPULATION III STAR FORMATION","type":"publication"},{"authors":["John H Wise","Tom Abel","Matthew J Turk","Michael L Norman","Britton D Smith"],"categories":null,"content":"","date":1325376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"aa120229066e2ea9fc4e86004a779463","permalink":"https://matthewturk.github.io/publication/wise-2012-dn/","publishdate":"2019-05-30T20:07:09.382982Z","relpermalink":"/publication/wise-2012-dn/","section":"publication","summary":"Massive stars provide feedback that shapes the interstellar medium of galaxies at all redshifts and their resulting stellar populations. Here we present three adaptive mesh refinement radiation hydrodynamics simulations that illustrate the impact of momentum …","tags":["Authorship"],"title":"The birth of a galaxy--II. The role of radiation pressure","type":"publication"},{"authors":["John H Wise","Matthew J Turk","Michael L Norman","Tom Abel"],"categories":null,"content":"","date":1322697600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"2e513dfacd2ecb326d361c9ba89cffb2","permalink":"https://matthewturk.github.io/publication/wise-2011-ph/","publishdate":"2019-05-30T20:07:09.386569Z","relpermalink":"/publication/wise-2011-ph/","section":"publication","summary":"By definition, Population III stars are metal-free, and their protostellar collapse is driven by molecular hydrogen cooling in the gas phase, leading to large characteristic masses. Population II stars with lower characteristic masses form when the star-forming gas reaches a critical metallicity of 10--6-10--3.5 Z ☉. We present an adaptive mesh refinement radiation hydrodynamics simulation that follows the transition from Population III to Population II star formation. The maximum spatial resolution of 1 comoving parsec allows for individual molecular clouds to be well resolved and their stellar associations to be studied in detail. We model stellar radiative feedback with adaptive ray tracing. A top-heavy initial mass function for the Population III stars is considered, resulting in a plausible distribution of pair-instability supernovae and associated metal enrichment. We find that the gas fraction recovers from 5% to nearly the cosmic fraction in halos with merger histories rich in halos above 107 M ☉. A single pair-instability supernova is sufficient to enrich the host halo to a metallicity floor of 10--3 Z ☉ and to transition to Population II star formation. This provides a natural explanation for the observed floor on damped Ly$α$ systems metallicities reported in the literature, which is of this order. We find that stellar metallicities do not necessarily trace stellar ages, as mergers of halos with established stellar populations can create superpositions of t--Z evolutionary tracks. A bimodal metallicity distribution is created after a starburst occurs when the halo can cool efficiently through atomic line cooling.","tags":["Authorship"],"title":"THE BIRTH OF A GALAXY: PRIMORDIAL METAL ENRICHMENT AND STELLAR POPULATIONS","type":"publication"},{"authors":["M J Turk","B D Smith"],"categories":null,"content":"","date":1293840000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"b3764bc9a12880080515cba2186c14bc","permalink":"https://matthewturk.github.io/publication/turk-2011-ck/","publishdate":"2019-05-30T20:07:09.366733Z","relpermalink":"/publication/turk-2011-ck/","section":"publication","summary":"The usage of the high-level scripting language Python has enabled new mechanisms for data interrogation, discovery and visualization of scientific data. We present yt, an open source, community-developed astrophysical analysis and visualization toolkit for data …","tags":["Authorship"],"title":"High-Performance Astrophysical Simulations and Analysis with Python","type":"publication"},{"authors":["Matthew J Turk","Paul Clark","S C O Glover","T H Greif","Tom Abel","Ralf Klessen","Volker Bromm"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"972f8cfd264acbc97e71613f0de8f131","permalink":"https://matthewturk.github.io/publication/turk-2010-mg/","publishdate":"2019-05-30T20:07:09.381233Z","relpermalink":"/publication/turk-2010-mg/","section":"publication","summary":"The transformation of atomic hydrogen to molecular hydrogen through three-body reactions is a crucial stage in the collapse of primordial, metal-free halos, where the first generation of stars (Population III stars) in the universe is formed. However, in the published literature, the rate coefficient for this reaction is uncertain by nearly an order of magnitude. We report on the results of both adaptive mesh refinement and smoothed particle hydrodynamics simulations of the collapse of metal-free halos as a function of the value of this rate coefficient. For each simulation method, we have simulated a single halo three times, using three different values of the rate coefficient. We find that while variation between halo realizations may be greater than that caused by the three-body rate coefficient being used, both the accretion physics onto Population III protostars as well as the long-term stability of the disk and any potential fragmentation may depend strongly on this rate coefficient.","tags":["Authorship"],"title":"EFFECTS OF VARYING THE THREE-BODY MOLECULAR HYDROGEN FORMATION RATE IN PRIMORDIAL STAR FORMATION","type":"publication"},{"authors":["Matthew J Turk","Michael L Norman","Tom Abel"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ba4d0d7e4ffe9e250803df6633aca598","permalink":"https://matthewturk.github.io/publication/turk-2010-vh/","publishdate":"2019-05-30T20:07:09.389768Z","relpermalink":"/publication/turk-2010-vh/","section":"publication","summary":"We report on simulations of the formation of the first stars in the","tags":["cosmology: theory; galaxies: formation; H II regions; stars:; formation; Astrophysics - Cosmology and Nongalactic Astrophysics;Authorship"],"title":"High-entropy Polar Regions Around the First Protostars","type":"publication"},{"authors":["Matthew J Turk","Britton D Smith","Jeffrey S Oishi","Stephen Skory","Samuel W Skillman","Tom Abel","Michael L Norman"],"categories":null,"content":"","date":1291161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"880368f1545659f8c60cd5a61c8a39e5","permalink":"https://matthewturk.github.io/publication/turk-2010-hk/","publishdate":"2019-05-30T20:07:09.388606Z","relpermalink":"/publication/turk-2010-hk/","section":"publication","summary":"The analysis of complex multiphysics astrophysical simulations presents a unique and rapidly growing set of challenges: reproducibility, parallelization, and vast increases in data size and complexity chief among them. In order to meet these challenges, and in order to open up new avenues for collaboration between users of multiple simulation platforms, we present yt (available at http://yt.enzotools.org/) an open source, community-developed astrophysical analysis and visualization toolkit. Analysis and visualization with yt are oriented around physically relevant quantities rather than quantities native to astrophysical simulation codes. While originally designed for handling Enzo's structure adaptive mesh refinement data, yt has been extended to work with several different simulation methods and simulation codes including Orion, RAMSES, and FLASH. We report on its methods for reading, handling, and visualizing data, including projections, multivariate volume rendering, multi-dimensional histograms, halo finding, light cone generation, and topologically connected isocontour identification. Furthermore, we discuss the underlying algorithms yt uses for processing and visualizing data, and its mechanisms for parallelization of analysis tasks.","tags":["Authorship"],"title":"yt: A MULTI-CODE ANALYSIS TOOLKIT FOR ASTROPHYSICAL SIMULATION DATA","type":"publication"},{"authors":["S Skory","M J Turk","M L Norman","A L Coil"],"categories":null,"content":"","date":1262304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"477b00158b1a9bd5ba4c5e21832541b4","permalink":"https://matthewturk.github.io/publication/skory-2010-il/","publishdate":"2019-05-30T20:07:09.377133Z","relpermalink":"/publication/skory-2010-il/","section":"publication","summary":"Modern N-body cosmological simulations contain billions ($10^ 9$) of dark matter particles. These simulations require hundreds to thousands of gigabytes of memory, and employ hundreds to tens of thousands of processing cores on many compute nodes. In order to …","tags":["Authorship"],"title":"Parallel hop: A scalable halo finder for massive cosmological data sets","type":"publication"},{"authors":["Matthew J Turk","Tom Abel","Brian O'Shea"],"categories":null,"content":"","date":1246406400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"ecfd80b5ead375d5b6a7dee166994961","permalink":"https://matthewturk.github.io/publication/turk-2009-fl/","publishdate":"2019-05-30T20:07:09.385796Z","relpermalink":"/publication/turk-2009-fl/","section":"publication","summary":"Previous high-resolution cosmological simulations predicted that the first stars to appear in the early universe were very massive and formed in isolation. Here, we discuss a cosmological simulation in which the central 50 M(o) (where M(o) is the mass of the Sun) clump breaks up into two cores having a mass ratio of two to one, with one fragment collapsing to densities of 10(-8) grams per cubic centimeter. The second fragment, at a distance of approximately 800 astronomical units, is also optically thick to its own cooling radiation from molecular hydrogen lines but is still able to cool via collision-induced emission. The two dense peaks will continue to accrete from the surrounding cold gas reservoir over a period of approximately 10(5) years and will likely form a binary star system.","tags":["Authorship"],"title":"The formation of Population III binaries from cosmological initial conditions","type":"publication"},{"authors":["Britton D Smith","Matthew J Turk","Steinn Sigurdsson","Brian W O'Shea","Michael L Norman"],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"b1dcadcb3cbb4b53f68bf4f702bd8de8","permalink":"https://matthewturk.github.io/publication/smith-2009-dp/","publishdate":"2019-05-30T20:07:09.384512Z","relpermalink":"/publication/smith-2009-dp/","section":"publication","summary":"Simulations of the formation of Population III (Pop III) stars suggest that they were much more massive than the Pop II and Pop I stars observed today. This is due to the collapse dynamics of metal-free gas, which is regulated by the radiative cooling of molecular hydrogen. We study how the collapse of gas clouds is altered by the addition of metals to the star-forming environment by performing a series of simulations of pre-enriched star formation at various metallicities. To make a clean comparison with metal-free star formation, we use initial conditions identical to a Pop III star formation simulation, with low ionization and no external radiation other than the cosmic microwave background (CMB). For metallicities below the critical metallicity, Z cr, collapse proceeds similar to the metal-free case, and only massive objects form. For metallicities well above Z cr, efficient cooling rapidly lowers the gas temperature to the temperature of the CMB. The gas is unable to radiatively cool below the CMB temperature, and becomes thermally stable. For high metallicities, Z ≳ 10--2.5 Z ☉, this occurs early in the evolution of the gas cloud, when the density is still relatively low. The resulting cloud cores show little or no fragmentation, and would most likely form massive stars. If the metallicity is not vastly above Z cr, the cloud cools efficiently but does not reach the CMB temperature, and fragmentation into multiple objects occurs. We conclude that there were three distinct modes of star formation at high redshift (z ≳ 4): a ``primordial'' mode, producing massive stars (10s to 100s of M ☉) at very low metallicities (Z ≲ 10--3.75 Z ☉); a CMB-regulated mode, producing moderate mass (10s of M ☉) stars at high metallicities (Z ≳ 10--2.5 Z ☉ at redshift z∼ 15-20); and a low-mass (a few M ☉) mode existing between these two metallicities. As the universe ages and the CMB temperature decreases, the range of the low-mass mode extends to higher metallicities, eventually becoming the only mode of star formation.","tags":["Authorship"],"title":"THREE MODES OF METAL-ENRICHED STAR FORMATION IN THE EARLY UNIVERSE","type":"publication"},{"authors":["John H Wise","Matthew J Turk","Tom Abel"],"categories":null,"content":"","date":1228089600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"6e687ba97e6cd867bc232f39e56f3687","permalink":"https://matthewturk.github.io/publication/wise-2008-gl/","publishdate":"2019-05-30T20:07:09.383653Z","relpermalink":"/publication/wise-2008-gl/","section":"publication","summary":"Numerous cosmological hydrodynamic studies have addressed the formation of galaxies. Here we choose to study the first stages of galaxy formation, including nonequilibrium atomic primordial gas cooling, gravity, and hydrodynamics. Using initial conditions appropriate for the concordance cosmological model of structure formation, we perform two adaptive mesh refinement simulations of 108 M☉ galaxies at high redshift. The calculations resolve the Jeans length at all times with more than 16 cells and capture over 14 orders of magnitude in length scales. In both cases, the dense, 105 solar mass, one parsec central regions are found to contract rapidly and have turbulent Mach numbers up to 4. Despite the ever decreasing Jeans length of the isothermal gas, we only find one site of fragmentation during the collapse. However, rotational secular bar instabilities transport angular momentum outward in the central parsec as the gas continues to collapse and lead to multiple nested unstable fragments with decreasing masses down to sub-Jupiter mass scales. Although these numerical experiments neglect star formation and feedback, they clearly highlight the physics of turbulence in gravitationally collapsing gas. The angular momentum segregation seen in our calculations plays an important role in theories that form supermassive black holes from gaseous collapse.","tags":["Authorship"],"title":"Resolving the Formation of Protogalaxies. II. Central Gravitational Collapse","type":"publication"},{"authors":["Matthew J Turk","Tom Abel","Brian W O'Shea"],"categories":null,"content":"","date":1204329600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"41c188ffd7b199c76187c2e05e873a44","permalink":"https://matthewturk.github.io/publication/turk-2008-nk/","publishdate":"2019-05-30T20:07:09.363676Z","relpermalink":"/publication/turk-2008-nk/","section":"publication","summary":"Modeling the formation of the first stars in the universe is a well?posed problem and ideally suited for computational investigation.We have conducted high?resolution numerical studies of the formation of primordial stars. Beginning with primordial initial conditions appropriate for a ?CDM model, we used the Eulerian adaptive mesh refinement code (Enzo) to achieve unprecedented numerical resolution, resolving cosmological scales as well as sub?stellar scales simultaneously. Building on the work of Abel, Bryan and Norman (2002), we followed the evolution of the first collapsing cloud until molecular hydrogen is optically thick to cooling radiation. In addition, the calculations account for the process of collision?induced emission (CIE) and add approximations to the optical depth in both molecular hydrogen roto?vibrational cooling and CIE. Also considered are the effects of chemical heating/cooling from the formation/destruction of molecular hydrogen. We present the results of these simulations, showing the formation of a 10 Jupiter?mass protostellar core bounded by a strongly aspherical accretion shock. Accretion rates are found to be as high as one solar mass per year.","tags":["Authorship"],"title":"Towards Forming a Primordial Protostar in a Cosmological AMR Simulation","type":"publication"},{"authors":["M Turk"],"categories":null,"content":"","date":1199145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559246695,"objectID":"d444bc751f807abdbc31db0e30dd812f","permalink":"https://matthewturk.github.io/publication/turk-2008-bl/","publishdate":"2019-05-30T20:07:09.374317Z","relpermalink":"/publication/turk-2008-bl/","section":"publication","summary":"The study the origins of cosmic structure requires large-scale computer simulations beginning with well-constrained, observationally-determined, initial conditions. We use Adaptive Mesh Refinement to conduct multi-resolution simulations spanning twelve orders …","tags":["Authorship"],"title":"Analysis and visualization of multi-scale astrophysical simulations using python and numpy","type":"publication"}]